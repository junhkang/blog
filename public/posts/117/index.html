<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>RLHF(Reinforcement Learning with Human Feedback): 인간 피드백으로 더 나은 AI 만들기 | Jun Kang&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content=":::::::::::: {.main role=&ldquo;main&rdquo;}
::::::::::: area-main
:::::::::: area-view
:::::: article-header
::::: inner-article-header
:::: box-meta
RLHF(Reinforcement Learning with Human Feedback): 인간 피드백으로 더 나은 AI 만들기
::: box-info
LLM
2024-11-24 12:11:48
:::
::::
:::::
::::::

::::: article-view
::: contents_style
대규모 언어 모델(LLM)이 다양한 작업에서 뛰어난 성능을 발휘하지만, 인간이
기대하는 정교한 응답이나 윤리적 기준에 부합하는 출력을 생성하는 데에는
한계가 있다. 이를 해결하기 위해 등장한 기술이 RLHF(Reinforcement
Learning with Human Feedback)이다. RLHF는 인간 피드백을 활용하여 모델의
행동을 조정하고, 인간과 더 자연스럽고 신뢰성 있는 상호작용을 가능하게
한다.

RLHF의 기본 개념

RLHF는 강화 학습(Reinforcement Learning)과 인간의 평가를 결합한 학습
방식이다. 기존 LLM은 대규모 텍스트 데이터로 사전 학습을 거치지만, 이러한
학습 방식만으로는 모델이 항상 사용자 기대에 부합하는 답변을 생성하지
못한다. RLHF는 인간의 피드백을 통해 모델의 출력을 평가하고, 이를 강화
학습의 보상 신호로 사용하여 모델의 출력을 개선한다.

RLHF의 작동 방식
\">
<meta name="author" content="Jun Kang">
<link rel="canonical" href="http://localhost:1313/posts/117/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/117/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Jun Kang&#39;s Blog (Alt + H)">Jun Kang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      RLHF(Reinforcement Learning with Human Feedback): 인간 피드백으로 더 나은 AI 만들기
    </h1>
    <div class="post-meta"><span title='2024-11-24 12:11:48 +0000 UTC'>November 24, 2024</span>&nbsp;·&nbsp;Jun Kang

</div>
  </header> 
  <div class="post-content"><p>:::::::::::: {.main role=&ldquo;main&rdquo;}
::::::::::: area-main
:::::::::: area-view
:::::: article-header
::::: inner-article-header
:::: box-meta</p>
<h2 id="rlhfreinforcement-learning-with-human-feedback-인간-피드백으로-더-나은-ai-만들기" class="title-article">RLHF(Reinforcement Learning with Human Feedback): 인간 피드백으로 더 나은 AI 만들기<a hidden class="anchor" aria-hidden="true" href="#rlhfreinforcement-learning-with-human-feedback-인간-피드백으로-더-나은-ai-만들기">#</a></h2>
<p>::: box-info
LLM</p>
<p>2024-11-24 12:11:48
:::
::::
:::::
::::::</p>
<hr>
<p>::::: article-view
::: contents_style
대규모 언어 모델(LLM)이 다양한 작업에서 뛰어난 성능을 발휘하지만, 인간이
기대하는 정교한 응답이나 윤리적 기준에 부합하는 출력을 생성하는 데에는
한계가 있다. 이를 해결하기 위해 등장한 기술이 RLHF(Reinforcement
Learning with Human Feedback)이다. RLHF는 인간 피드백을 활용하여 모델의
행동을 조정하고, 인간과 더 자연스럽고 신뢰성 있는 상호작용을 가능하게
한다.<br>
<br>
RLHF의 기본 개념<br>
<br>
RLHF는 강화 학습(Reinforcement Learning)과 인간의 평가를 결합한 학습
방식이다. 기존 LLM은 대규모 텍스트 데이터로 사전 학습을 거치지만, 이러한
학습 방식만으로는 모델이 항상 사용자 기대에 부합하는 답변을 생성하지
못한다. RLHF는 인간의 피드백을 통해 모델의 출력을 평가하고, 이를 강화
학습의 보상 신호로 사용하여 모델의 출력을 개선한다.<br>
<br>
RLHF의 작동 방식<br>
\</p>
<ol>
<li>초기 언어 모델 준비<br>
대규모 데이터를 활용해 사전 학습된 LLM(예: GPT)을 준비한다.\</li>
<li>피드백 데이터 수집<br>
인간 평가자를 통해 모델이 생성한 출력에 대한 선호도를 수집한다. 예를
들어, 여러 출력 중에서 가장 적절한 응답을 선택하거나, 출력의 품질을
점수화한다.\</li>
<li>보상 모델 학습(Reward Model)<br>
수집된 피드백 데이터를 사용하여 보상 모델을 학습한다. 보상 모델은 입력과
출력 쌍에 대해 품질 점수를 예측하는 역할을 한다.\</li>
<li>강화 학습 단계<br>
보상 모델에서 제공하는 품질 점수를 기반으로, 기존 언어 모델을 강화 학습
알고리즘(예: Proximal Policy Optimization, PPO)을 사용해
Fine-Tuning한다.\</li>
<li>반복 및 개선<br>
모델이 개선된 출력을 생성할 수 있도록, 피드백 수집과 강화 학습 단계를
반복한다.<br>
<br>
RLHF의 주요 장점<br>
\</li>
<li>인간 중심의 모델 조정<br>
인간이 기대하는 결과에 맞춰 모델의 출력을 조정할 수 있어, 더 신뢰할 수
있고 유용한 AI를 개발할 수 있다.\</li>
<li>윤리적 기준 강화<br>
민감한 주제나 윤리적 논란이 있는 상황에서, 인간 피드백을 활용하여 모델이
불쾌하거나 위험한 출력을 생성하지 않도록 제어할 수 있다.\</li>
<li>특정 작업에 대한 최적화<br>
RLHF는 특정 도메인이나 작업에 대한 성능을 향상시키는 데 효과적이다.\</li>
<li>유연한 피드백 반영<br>
새로운 피드백을 지속적으로 수집하여, 모델의 성능을 점진적으로 개선할 수
있다.<br>
<br>
RLHF의 응용 사례<br>
\</li>
<li>챗봇 및 대화형 AI<br>
RLHF는 사용자의 기대에 부합하는 더 자연스러운 대화를 생성하는 데
활용된다. 예를 들어, ChatGPT는 RLHF를 사용해 응답 품질을 지속적으로
개선한다.\</li>
<li>고객 서비스<br>
고객의 피드백을 반영하여, 특정 산업이나 고객 요구에 맞춘 응답을 제공하는
고객 지원 AI를 개발할 수 있다.\</li>
<li>콘텐츠 생성<br>
사용자가 선호하는 스타일이나 톤에 맞춰 글을 작성하거나 이미지를 생성하는
AI 모델을 개선할 수 있다.\</li>
<li>윤리적 AI 개발<br>
민감한 주제에서 모델이 윤리적으로 올바른 응답을 생성하도록 인간 피드백을
활용해 학습시킨다.<br>
<br>
RLHF의 한계와 도전 과제<br>
\</li>
<li>피드백의 품질<br>
인간 평가자의 피드백이 항상 일관되거나 정확하지 않을 수 있다. 낮은
품질의 피드백은 모델 학습에 부정적인 영향을 미칠 수 있다.\</li>
<li>비용과 시간<br>
인간 피드백을 수집하는 과정은 비용이 많이 들고 시간이 오래 걸린다.\</li>
<li>모델의 복잡성 증가<br>
RLHF는 보상 모델과 강화 학습 단계를 추가하기 때문에, 모델 학습 과정이 더
복잡하고 자원 소모가 크다.\</li>
<li>편향 문제<br>
인간 피드백이 편향될 경우, 모델이 이러한 편향을 학습하여 비합리적인
출력을 생성할 가능성이 있다.<br>
<br>
RLHF의 미래<br>
<br>
RLHF는 LLM의 품질을 높이고, 인간과 더 나은 상호작용을 가능하게 하는
중요한 기술로 자리 잡고 있다. 앞으로는 피드백 수집의 자동화, 보상 모델의
정밀화, 그리고 비용 효율적인 학습 방법의 개발이 RLHF의 핵심 연구 방향이
될 것이다.<br>
<br>
또한, RLHF는 AI 윤리와 신뢰성 확보를 위한 강력한 도구로, 다양한 산업
분야에서 더욱 널리 활용될 것으로 기대된다. 예를 들어, 의료, 법률, 교육
등 민감한 영역에서 RLHF는 AI의 실용성과 안전성을 동시에 확보할 수 있는
기술로 주목받고 있다.<br>
<br>
RLHF는 인간과 AI의 협력을 강화하고, 더욱 인간 중심적인 AI를 구현하는 데
중요한 역할을 할 것이다.
:::</li>
</ol>
<p>\</p>
<p>::: tags
#오블완 #티스토리챌린지
:::
:::::
::::::::::
:::::::::::
::::::::::::</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Jun Kang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
