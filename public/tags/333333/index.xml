<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>333333 on Jun Kang&#39;s Blog</title>
    <link>http://localhost:1313/tags/333333/</link>
    <description>Recent content in 333333 on Jun Kang&#39;s Blog</description>
    <generator>Hugo -- 0.143.1</generator>
    <language>ko</language>
    <atom:link href="http://localhost:1313/tags/333333/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[AWS] Elastic IP (탄력적 IP)의 개념 및 적용</title>
      <link>http://localhost:1313/posts/18/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/18/</guid>
      <description>&lt;p&gt;EIP(Elastic Ip Address)란 인터넷으로 접속이 가능한 공인 IP를 할당하여, 인스턴스에 탈부착할 수 있는 서비스이다. 인스턴스 혹은 네트워크 인터페이스에 연결이 가능하며 삭제 전까지 해당 IP를 유지할 수 있다.EC2 인스턴스 생성 시 공인 IP 사용 설정을 Enable로 변경 (default는 Disable)할 경우 인스턴스 자체에 공인 IP를 할당받을 수 있는데 왜 굳이 Elastic IP를 사용하는 것일까?![](/images/posts/18/스크린샷 2023-10-09 오후 1.56.51.png)인스턴스가 stop 후 재시작될 경우 공인 IP가 변경되는 경우가 발생한다. 인스턴스 자체의 공인 IP가 변경될 경우 큰 문제로 이어질 수 있어EIP를 인스턴스에 연결함으로써 인스턴스의 공인IP를 고정시켜 준다.## 2. Elastic IP 개념 및 특징- 탄력적 IP 주소는 정적이며 시간이 지남에 따라 변경되지 않는다.- 탄력적 IP 주소는 특정 리전에서만 사용할 수 있으며 다른 리전으로 이전할 수 없다.- IPv4 주소의 Amazon 풀 또는 AWS 계정으로 가져온 사용자 지정 IPv4 주소 풀에서 탄력적 IP 주소를 할당할 수 있다. (3-3. EIP 옵션 설정 시 선택 가능)- 탄력적 IP 주소를 사용하려면 먼저 계정에 주소를 할당한 후 인스턴스 또는 네트워크 인터페이스와 연결해야 한다.(3-6. 인스턴스 할당)- 탄력적 IP 주소는 리소스에서 연결 해제했다가 다른 리소스와 다시 연결할 수 있다. 예기치 않은 동작을 방지하려면 변경하기 전에 기존 연결에 이름이 지정된 리소스에 대한 모든 활성 연결이 닫혀 있는지 확인해야 한다. 탄력적 IP 주소를 다른 리소스에 연결한 후 새로 연결된 리소스에 대한 연결을 다시 열 수 있다.- 연결 해제한 Elastic IP 주소는 명시적으로 릴리스(삭제)할 때까지 계정에 할당되어 있기 때문에 실행 중인 인스턴스와 연결되지 않은 탄력적 IP 주소에 대해서는 소액의 시간당 요금이 부과된다 (2. 요금 항목 참고).- 탄력적 IP 주소를 이전에 퍼블릭 IPv4 주소가 있던 인스턴스와 연결하면 인스턴스의 퍼블릭 DNS 호스트 이름이 탄력적 IP 주소에 맞게 변경된다.- Amazon은 퍼블릭 DNS 호스트 이름을 인스턴스 네트워크 외부에서는 인스턴스의 퍼블릭 IPv4 주소 또는 탄력적 IP 주소로 변환하고, 인스턴스 네트워크 내부에서는 인스턴스의 프라이빗 IPv4 주소로 변환한다.- AWS 계정으로 가져온 IP 주소 풀에서 탄력적 IP 주소를 할당하는 경우 해당 IP 주소는 탄력적 IP 주소 한도에 포함되지 않는다.- 탄력적 IP 주소는 특정 네트워크 경계 그룹에서만 사용할 수 있다.퍼블릭(IPv4) 인터넷 주소는 흔치 않은 퍼블릭 리소스 이기 때문에 지역당 5개로 제한되며, 인스턴스 장애 시 주소를 다른 인스턴스로 다시 매핑하는 기능이 필요할때는 EIP를 주로 사용하고, 다른 모든 노드 간 통신은 DNS 호스트명을 사용하는 것을 권장한다. (사용개수 제한은 AWS에 별도 문의하여 최대 사용량을 증가시킬 수 있다.)## 3. 요금- 실행 중인 인스턴스와 연결된 각 추가 IP 주소에 대해 시간당 0.005 USD(비례 할당으로 계산)- 실행 중인 인스턴스와 연결되지 않은 각 탄력적 IP 주소에 대해 시간당 0.005 USD(비례 할당으로 계산)- 매달 처음 100개의 재매핑에 대해 탄력적 IP 주소 재 매핑당 0.00 USD- 매달 100개 이후의 재매핑에 대해 탄력적 IP 주소 재 매핑당 0.10 USD상세 요금은 다음 공식 링크에서 확인 가능하다.&lt;a href=&#34;https://aws.amazon.com/ec2/pricing/on-demand/&#34;&gt;https://aws.amazon.com/ec2/pricing/on-demand/&lt;/a&gt;EC2 온디맨드 인스턴스 요금 – Amazon Web Servicesaws.amazon.com4. 적용#### 4-1. EC2 메뉴에서 Elastic IPs 선택![](/images/posts/18/스크린샷 2023-10-09 오후 2.00.56.png)#### 4-2. Allocate Elastic IP address 선택![](/images/posts/18/스크린샷 2023-10-09 오후 2.02.17.png)### 4-3. 네트워크 경계그룹, IP주소 풀 선택 후 &amp;ldquo;Allocate&amp;quot;선택#### 4-3-1. 네트워크 경계 그룹AWS가 Public 주소를 알리는 가용영역, , Local Zone 또는 Wavelength Zone의 집합이다. Local Zone 및 Wavelength Zone은 AWS 네트워크와 해당 영역의 리소스에 액세스 하는 고객 간의 지연 시간 또는 물리적 거리를 최소화하기 위해 리전의 AZ와 다른 네트워크 경계 그룹을 가질 수 있다.주의 : EIP와 연결될 AWS 리소스는 동일한 네트워크 경계 그룹에 할당되어야 한다.#### 4-3-2. Public IPv4 address pool- [Amazon의 IP 주소 풀(Amazon&amp;rsquo;s pool of IPv4 addresses)] - 특별한 경우가 아니라면 해당 옵션을 선택하여 공인 IP를 할당받으면 된다. IPv4 주소를 Amazon의 IPv4 주소 풀에서 할당하려는 경우이다.- AWS 계정으로 가져오는 퍼블릭 IPv4 주소 - AWS 계정으로 가져온 IP 주소 풀에서 IPv4 주소를 할당하려는 경우. IP 주소 풀이 없는 경우에는 이 옵션을 사용할 수 없다.- [고객 소유 IPv4 주소 풀(Customer owned pool of IPv4 addresses)] - AWS Outpost를 통해 사용할 온프레미스 네트워크에서 생성된 풀에서 IPv4 주소를 할당하려는 경우. AWS Outposts가 없는 경우 이 옵션이 비활성화된다.![](/images/posts/18/스크린샷 2023-10-09 오후 2.03.46.png)### 4-4. EIP 생성이 완료되면 리스트에서 확인이 가능하며, 인스턴스 할당을 위해서 해당 항목을 클릭하여 상세 페이지로 들아간다.![](/images/posts/18/스크린샷 2023-10-09 오후 2.04.45.png)### 4-5. EIP 상세에서 우측 상단 &amp;ldquo;Associate Elastic IP address&amp;quot;를 선택한다.&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/posts/18/img_1.png&#34;&gt;### 4-6. AWS 인스턴스와 연결하고 싶다면 인스턴스를 선택, 특정 사설(private) ip와 연결하고 싶다면 IP주소를 입력하면 된다.&amp;ldquo;Allow this Elastic IP address to be reassociated&amp;rdquo; 옵션을 선택하면 해당 EIP를 할당 후 다른 인스턴스로 변경이 가능하다.&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/posts/18/img_2.png&#34;&gt;참고&lt;a href=&#34;https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html&#34;&gt;https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html&lt;/a&gt;#AWS #eip #Elastic IPs&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Elastic Search] Elastic Search란? Elastic Search의 개념 및 장단점</title>
      <link>http://localhost:1313/posts/50/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/50/</guid>
      <description>&lt;p&gt;Apache Lucene에 구축되어 배포된 검색 및 분석 엔진이다. 현재 검색엔진을 넘어 보안, 로그분석, Full-text 분석 등 다양한 영역에서 사용되며, Kibana, Logstash, Beats들과 함께 사용된다. 오픈 소스 프로젝트로 활발히 개발되고 있으며, 유닉스, 자바의 기초지식 필요하다. Apache Lucene의 한계를 보완하기 위한 새로운 검색엔진 프로젝트로 시작되었고 Logstash, Kibana와 함께 사용되어 ELK Stack (ES, Logstash, Kibana)라고 불렸으나 2013년 Logstash, Kibana 프로젝트 정식 흡수되었다.## 2. Elastic Stack이란?ES, Logstash, Kibana를 묶은 ELK 서비스이다. 5.0.0 버전부터 Beats를 포함하며 Elastic Stack 이란 이름으로 서비스가 제공되고 있다. 서버로부터 모든 유형의 데이터를 가져와 실시간 검색, 분석, 시각화를 도와주는 Elastic 오픈 소스 서비스 제품이다.&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/posts/50/img.png&#34;&gt;### 2-1. Elastic Search- 아파치 루씬(Apache Lucene) 기반의 Full Text로 검색이 가능한 오픈 소스 분석 엔진- 주로 Rest API로 처리- 대량 데이터를 거의 실시간으로 신속하게 저장, 검색, 분석 가능### 2-2. Logstash- 플러그인을 통해 데이터 집계와 보관, 서버 데이터 처리 담당- ES와 상관없이 독자적으로도 사용 가능- 파이프라인으로 데이터를 수집, 필터를 통해 변환 후 Elastic Search로 전송- 입력 - Beats, CloudWatch, Eventlog 등 다양한 입력 지원, 데이터 수집- 필터 - 형식, 복잡성에 상관없이 데이터를 동적으로 변환- 출력 - ES, Email, ECS, Kafka 등 원하는 저장소에 데이터 전송### 2-3. Kibana- 데이터 시각화 도구- 검색 및 aggregation 집계기능을 통해 ES로부터 문서, 집계 결과등을 가져와 웹도구로 시각화- DIscover, Visualize, Dashboard 3개의 기본 메뉴와 다양한 App 들로 구성- 플러그인을 통해 App 설치 가능### 2-4. Beats- 경량 에이전트로 설치- 데이터를 Logstash 또는 ES로 전송- Logstash 보다 경량화 된 서비스- Filebeat, Metricbeat, Packetbeat, Winlogbeat, Heartbeat 등- Libbeat을 통해 직접 구축 가능## 3. Elastic Search의 특징과 장단점### 3-1. 장점#### → 3-1. 실시간분석- 하둡 시스템과 달리 ES 클러스터가 실행되고 있는 동안에는 계속해서 데이터가 입력 (인덱싱) 되고, 동시에 실시간에 가까운 속도로 색인된 데이터의 집계, 검색이 가능#### → 3-2. Full Text 검색 엔진- Lucene은 기본적으로 역파일 색인 구조로 데이터를 저장하며 이를 사용하는 ES도 동일 방식&amp;gt; 역색인 - 일반적인 색인의 목적은 문서의 위치에 대한 index를 생성하여 그 문서에 빠르게 접근하기 위함이지만, 역색인은 문서 내의 문자와 같은(혹은 유사한) 내용들에 대한 매핑 정보를 색인하는 것이다.- 내부적으로는 역파일 색인이라도 사용자 관점에서는 JSON 형식으로 전달- 쿼리문 또는 쿼리에 결과도 모두 JSON 형태로 반환- key-value 형식이 아닌 문서기반으로 되어있어 복잡한 정보를 포함해도 그대로 저장이 가능하여 직관적- 여러 계층 구조의 문서로 저장이 가능하며, 계층 구조로 된 문서도 한 번의 쿼리로 조회 가능#### → 3-3. RESTFul API- Rest API를 기본으로 지원하여 모든 데이터의 조회, 입력, 삭제를 HTTP 프로토콜을 통해 처리 가능#### → 3-4. multitenancy- ES의 데이터들은 index라는 논리 집합 단위로 구성되며 서로 다른 저장소에 분산-저장된다.- 서로 다른 인덱스들을 별도 커넥션 없이 하나의 쿼리로 묶어서 검색, 하나의 출력 결과를 도출한다. (서로 상이한 인덱스일지라도 검색할 필드명만 같으면 여러 인덱스를 동시에 조회 가능)#### → 3-5. 확장성- 분산 구성이 가능, 분산환경에서 데이터는 shard 단위로 분리- 플러그인을 사용한 기능 확장 가능- AWS, MS Azure 같은 클라우드 서비스, Hadoop 플랫폼들과도 연동 가능#### 3-6. 다양한 알고리즘 제공- 점수 기반의 다양한 정확도 알고리즘, 실시간 분석 등의 구현 가능### 3-2. 단점- 색인된 데이터는 내부 commit, flush 등의 프로세스를 거치기에 1초정도 뒤에 검색이 가능- 클러스터의 성능향상을 위해 비용소모가 큰 트랜잭션 롤백이 지원되지 않는다.- 업데이트 요청시 기존 문서를 삭제 후 신규 문서를 재생성하기에 업데이트 비용이 크다.## 4. Elastic Search 구성&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/posts/50/img_1.png&#34;&gt;#### Index- 데이터 저장공간- 하나의 물리 노드에 여러 개의 논리 인덱스 생성- 하나의 인덱스가 여러 노드에 분산 저장#### Shard- 색인된 문서는 하나의 인덱스와 그 내부의 여러 개의 파티션(샤드)으로 나뉘어 구성#### Type- 인덱스의 논리적 구조- 6.1부터 인덱스당 하나의 타입만 설정 가능#### Document- 인덱스가 저장되는 최소단위- JSON 포멧으로 저장- RDBMS의 ROW와 동일#### Field- 문서를 구성하기 위한 속성- 하나의 필드는 다수의 데이터 타입 정의 가능- RDBMS의 컬럼과 동일#### Mapping- 문서의 필드, 필드 속성을 정의하고 색인 방법을 정의하는 프로세스## 5. Elastic Search와 RDBMS의 관계익숙한 관계형 데이터베이스와의 유사 기능 관계를 통해 이해해 보면&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/posts/50/img_2.png&#34;&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/posts/50/img_3.png&#34;&gt;참고&lt;a href=&#34;https://jaemunbro.medium.com/elastic-search-%EA%B8%B0%EC%B4%88-%EC%8A%A4%ED%84%B0%EB%94%94-ff01870094f0&#34;&gt;https://jaemunbro.medium.com/elastic-search-%EA%B8%B0%EC%B4%88-%EC%8A%A4%ED%84%B0%EB%94%94-ff01870094f0&lt;/a&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/indices.html&#34;&gt;https://www.elastic.co/guide/en/elasticsearch/reference/current/indices.html&lt;/a&gt;&lt;a href=&#34;https://velog.io/@hanblueblue/Elastic-Search-1&#34;&gt;https://velog.io/@hanblueblue/Elastic-Search-1&lt;/a&gt;#Elastic Stack #Elastic search&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Java] 가상 스레드 (Virtual Threads)란? 자바 21의 가상스레드 (Virtual Thread) 도입</title>
      <link>http://localhost:1313/posts/37/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/37/</guid>
      <description>&lt;p&gt;2023.09.20 릴리즈 된 자바 21에 추가된 가상 스레드(Virtual Threads)라는 기능을 살펴보자.가상 스레드는 경량 스레드로, 높은 처리량의 동시 어플리케이션을 작성, 유지 및 관찰하는 작업 공수를 크게 줄인다.  OS스레드를 그대로 사용하지 않고 JVM 자체적으로 스케쥴링을 통해 사용할 수 있는 경량 스레드이며, 하나의 프로세스가 수십 - 수백만 스레드를 동시에 실행할수 있도록 설계되었다.## 2. 자바의 전통적인 스레드자바 개발자들은 근 30년동안 동시성 서버 어플리케이션의 처리를 위해 스레드에 의존해왔다. 모든 메서드의 구문들은 스레드 내부에서 실행되며, 1개의 요청을 1개의 스레드가 처리한다. 대표적으로 스프링은 멀티스레드 구조이기에, 여러 스레드의 실행이 동시에 발생하며 동시 요청이 많아질수록 스레드의 수 역시 증가한다. 각각의 스레드는 지역 변수를 저장하고 메소드 호출을하는 스택을 제공하며, 문제가 생겼을 때의 Context도 제공하는데, 예를들어 Exception은 동일 스레드 내에서의 메소드에 의해 throw/catch 된다. 그렇기 때문에 개발자는 스레드의 Stack trace로 문제를 추적할 수 있는 것이고, 그 외 Debugger (스레드의 메소드 내에서 구문을 차례로 훑어본다), Profiler(JFR) (여러 스레드의 행동을 시각화하여 스레드의 성능을 이해할 수 있도록 도와준다.)도 모두 스레드 기반으로 되어있다.### 2-1. 전통적인 스레드의 한계점- 기존 JDK의 스레드는 OS 스레드를 Wrapping한 것으로 사용가능한 스레드의 수가 하드웨어 수준보다 훨씬 적게 제한되어있다.- OS 스레드는 생성, 유지 비용이 높고 갯수가 제한적이라 요청량에 비례하여 무한정 늘릴수 없다.- 어플리케이션 코드가 플렛폼 스레드를 사용하면 실제로는 OS스레드를 사용하는 것이며, 이 스레드는 비용이 비싸기 때문에 스레드 풀을 사용하여 접근하는 방식으로 사용한다.&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/posts/37/img.png&#34;&gt;- Spring 같은 어플리케이션의 기본 처리방식은 Thread-per-request이다.&amp;gt; [Thread-per-request]서버 어플리케이션은 일반적으로 서로 독립적인 유저의 동시 요청들을 처리하기에, 어플리케이션이 전체 요청기간 동안 스레드를 전담하여 요청을 처리해야한다. 이러한 Thread-per-request 스타일은 플랫폼의 동시성 단위가 곧 어플리케이션의 동시성 단위이기 때문에 이해하기 쉽고, 개발 및 디버그, 프로파일링 하기 쉽다.- Thread-per-request 방식은 요청을 처리하는 스레드에서 I/O 작업시 Blocking이 발생한다.- Blocking 발생시 스레드는 I/O 작업 종료시까지 대기해야하기에 많은 요청을 처리해야 하는 상황이라면 Blocking으로 발생하는 낭비를 줄여야한다.&amp;gt; [Reactive Programming]- Blocking 방식으로 발생하는 낭비를 줄이기 위해 발전하게된 처리량을 높이기 위한 방법, 비동기 방식 프로그래밍이다.- Non-blocking 방식으로 변경하면서 Blocking을 대기하는데 소요된 자원을 다른 요청에서 사용할 수 있다.- 기존 자바 프로그래밍은 스레드를 기반으로하기에 라이브러리들이 모두 Reactive Programming 방식에 맞게 새로 작성되어야하는 문제가 있다.## 3. 가상 스레드의 작동방식가상 스레드는 OS를 Wrapping한 구조가 아니기에 스레드 풀 없이 사용 가능하고, JVM 자체적으로 OS스레드와 연결하는 스케쥴링을 처리하기에 기존 스타일로 코드를 작성하더라도 내부의 가상 스레드가 효율적인 방법으로 스케줄링 해준다. (가상 스레드를 사용하면 Non-blocking에 대한 처리를 JVM단에서 처리해준다.)&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/posts/37/img_1.png&#34;&gt;## 4. 목표공식 문서에 따르면 가상 스레드의 목표와 목표가 아닌 것 (Goals / Non-Goals)을 확인할 수 있다.목표 (Goals)- 기존의 Thread-per-request (요청당 처리) 방식으로 작성된 서버 어플리케이션을 near-optimal(최적화) 하드웨어 사용으로 확장 가능해야한다.- java.lang.Thread API를 사용하는 기존 자바 코드를 최소한의 수정으로 가상 스레드를 채택 가능해야 한다.- 기존 JDK 툴들을 사용하여 가상 스레드의 쉬운 트러블 슈팅, 디버깅 및 프로파일링이 가능해야 한다.목표가 아닌 것 (Non-Goals)- 기존 thread의 사용을 제거하는 것이나 기존 어플리케이션이 가상 스레드를 사용하기 위해 은밀하게 마이그레이션 하는 것이 아니다.- 자바의 기본 동시성 모델을 바꾸는 것이 아니다.- 자바 언어나 자바 라이브러리에 새로운 데이터 병렬구조를 제공하려는 것이 목표가 아니다. Stream API는 큰 데이터를 병렬로 처리하는데 여전히 선호되는 방법이다.가상 스레드는 자바의 기본 동시성 모델을 바꾸거나, 새로운 데이터 흐름의 병렬 구조를 제시하는 것이 아닌 기존 자바 코드를 최소한으로 수정하는 선에서 동시성을 제어하는 어플리케이션이 기존 Thread-per-request 방식 외에 가상 스레드 풀 없이 Reactive Programming이 추구하는 Non-blocking의 효율적인 자원사용을 지원하는데 목표를 두고 있다.참고&lt;a href=&#34;https://mangkyu.tistory.com/309&#34;&gt;https://mangkyu.tistory.com/309&lt;/a&gt;&lt;a href=&#34;https://findstar.pe.kr/2023/04/17/java-virtual-threads-1/&#34;&gt;https://findstar.pe.kr/2023/04/17/java-virtual-threads-1/&lt;/a&gt;&lt;a href=&#34;https://openjdk.org/jeps/444&#34;&gt;https://openjdk.org/jeps/444&lt;/a&gt;#Java #가상 스레드&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Java] 클래스 로딩 과정(Java Class Loading Process)이란?</title>
      <link>http://localhost:1313/posts/35/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/35/</guid>
      <description>&lt;p&gt;자바 클래스 로딩 과정 (Java Class Loading Process)은 클래스 로더가 클래스 파일을 찾아 동적으로 JVM의 메모리 영역인 Runtime Data Areas에 올려놓는 과정을 말한다.자바에서 객체가 어떻게 형성/관리 되는지 이해하려면. java 파일의 소스코드가 어떻게 JVM 위에 로딩되는지 아는 것이 중요하고, 클래스 로딩할 때 발생하는 문제 (&amp;lsquo;java.lang.ClassNotFoundException&amp;rsquo;과 같은 에러)를 쉽게 해결하고, 코드상 동적으로 클래스 로딩하는 구문 이해하는데 필요하다.## 2. 클래스 로더 3단계 과정클래스 로더는 다음 3단계 과정을 거쳐 클래스 파일을 로딩한다.&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/posts/35/img.png&#34;&gt;&amp;gt; 1. Loading 클래스 파일을 읽어 바이너리 코드로 만들고 JVM의 메모리에 로드2. Linking 클래스파일 사용을 위한 검증코드 내부의 러퍼런스를 연결3. InitializationStatic 변수들의 초기화 및 값 할당## 3. 클래스 로드 시점Loading 시점에서 JVM은 실행 시점에 모든 클래스를 메모리에 올려놓지 않고 필요한 클래스를 동적으로 메모리에 적재하여 효율적으로 관리한다.&amp;gt; 1. 인스턴스 생성시2. Static 변수 할당3. Static 메서드 호출4. Static final 상수 호출 (Static 변수, 메소드 호출과 다르게 Outer 클래스가 로딩되지 않는다, JVM의 Method Area에 Constant Pool에 따로 저장되어 관리되기 때문이다.)## 4. 클래스 로더의 종류&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/posts/35/img_1.png&#34;&gt;JVM을 실행했을 때 각 클래스 로더들은 자신이 호출할 수 있는 클래스들을 호출하여 JVM에 로딩하게 된다.&amp;gt; 1. 부트스트랩 클래스 로더 $JAVA_HOME/jre/lib/rt.jar 에서 rt.jar에 있는 JVM을 실행시키기 위한 최소한의 핵심 클래스들을 로딩한다.-verbose:class JVM 옵션을 주고 자바 애플리케이션을 실행시키면 rt.jar에 있는 파일 로딩되는 것을 확인할 수 있다.Java 9 이후로는 rt.jar 등이 없어짐에 따라 로딩할 수 있는 클래스의 범위가 축소되어 정확하게 ClassLoader 내 최상위 클래스들만 로드한다.2. 확장 클래스로더$JAVA_HOME/jre/lib/ext 경로의 자바 확장 클래스들을 로딩한다3. 애플리케이션  클래스 로더자바 프로그램 실행 시$CLASSPATH에 설정된 경로의 클래스들을 로딩하게 된다. 이 시점에 개발된. class파일들이 로딩된다.클래스 로더들은 계층 구조를 가지도록 생성이 가능하고 각 부모 클래스 로더에서 자식클래스 로더를 가지는 형태로 클래스 로더를 만들 수 있다.## 5. 클래스 로더의 작동원칙위임 원칙 (Delegate Load Request)&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/posts/35/img_2.png&#34;&gt;System Loader가 A라는 클래스를 로딩할 때 그 요청은 부모로더들로 거슬러 올라가 부트스트랩 로더에 도착한 후 그 밑으로 로딩 요청을 수행한다. 최상위 클래스 로더에 요청을 위임한 후, 파일을 찾으며 자식 클래스 로더에게 넘기며, 클래스로더 중 하나라도 파일 찾는 데 성공하면 자식 로더에게 넘겨준다.가시성 제약 조건(Have Visibility Constraint)부모 로더에서 찾지 못한 클래스는 자식 로더로 찾지 못하고, 자식로더가 찾지 못한 것은 부모로더에 위임해서 찾을 수 있다언로드 불가 (Cannot unload classes)클래스 로더로 로딩된 클래스들은 JVM 상에서 없앨 수 없다유일성 원칙 (Uniqueness Principle)하위 클래스 로더가 상위 클래스 로더에서 로드한 클래스를 다시 로드하지 않아야 한다.상위 클래스로만 책임을 위임하기에 고유한 클래스를 보장할 수 있게 해주는 원칙이다.참고&lt;a href=&#34;https://co-no.tistory.com/103&#34;&gt;https://co-no.tistory.com/103&lt;/a&gt;&lt;a href=&#34;https://engkimbs.tistory.com/entry/Java-Java-%ED%81%B4%EB%9E%98%EC%8A%A4-%EB%A1%9C%EB%94%A9-%EA%B3%BC%EC%A0%95Java-Class-Loading-Process&#34;&gt;https://engkimbs.tistory.com/entry/Java-Java-%ED%81%B4%EB%9E%98%EC%8A%A4-%EB%A1%9C%EB%94%A9-%EA%B3%BC%EC%A0%95Java-Class-Loading-Process&lt;/a&gt;#Java #jvm #클래스 로더&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Linux] 스왑 메모리(Swap Memory)의 개념과 적용방법</title>
      <link>http://localhost:1313/posts/34/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/34/</guid>
      <description>&lt;h3 id=&#34;1-1-swap-메모리-확인swapon--sfree--himagesposts34스크린샷-2023-10-23-오후-32549png-shared--하나의-프로세스에서-다른-프로세스의-데이터에-효율적으로-접근하기-위해-사용하는-메모리buffcache--버퍼와-캐시를-위해-사용하는-메모리-커널이-성능향상을-위해-캐시-영역으로-사용되는-메모리buff--프로세스가-사용하는-메모리-영역이-아닌-시스템-성능향상을-위해-커널이-사용하고-있는-영역cache--캐시영역의-메모리-io-작업을-더-빠르게-하기-위해-커널에서-사용-total---buffcache---free--사용중인-메모리-total---used---buffcache--실제-사용가능한-메모리-1-2-swap-메모리-추가sudo-dd-ifdevzero-ofswapfile-bs1024-count200000-bs--포멧-단위-bs--1m-로도-사용-가능count--블록수-1kb를-200000번-devzero로-초기화하기에-총-200mb의-공간을-swap-파일로-포맷한-것으로-메모리-크기를-의미한다-count2000000---2gb-swap-메모리imagesposts34스크린샷-2023-10-23-오후-32638pngimagesposts34스크린샷-2023-10-23-오후-32753png-1-3-mkswap으로-swap-파일로-포멧sudo-mkswap-swapfilesudo-chmod-600-swapfileimagesposts34스크린샷-2023-10-23-오후-32823pngimagesposts34스크린샷-2023-10-23-오후-32859png-1-4-swap-메모리-활성화---단일-swap-메모리-onsudo-swapon-swapfile---전체-swap-메모리-onswapon--aswapon-swapfile-swapon-failed-device-or-resource-busy에러가-뜬다면-sudo-swapoff-swapfile로-swap-비활성화-후-다시-시도해-주면-된다imagesposts34스크린샷-2023-10-23-오후-33511pngswap-메모리가-활성된-후에-다시-1-1-swap-메모리-확인을-해보면-설정값만큼의-swap-메모리가-활성화된-것을-확인할-수-있다-1-5-시스템-재시작-시에도-swap-메모리-활성화sudo-vi-etcfstab해당-파일을-열어-맨-아랫줄에sawpfile-swap-swap-default-0-0를-추가하면-된다-1-6-swap-메모리-비활성화---단일-swap-메모리-offsudo-swapoff-swapfile---전체-swap-메모리-offswapoff--a-1-7-swap-메모리-삭제sudo-rm--r-swapfileswap-out---swap-in-되면서-실제-물리-메모리로-이동한다-다소-시간이-걸리는-작업이다-1-8-swap-영역-초기화swap-파티션-초기화sudo-mkswap-devswap-partitionswap-파일-초기화sudo-truncate--s-0-pathtoswapfilesudo-chmod-600-pathtoswapfilesudo-mkswap-pathtoswapfileswap-초기화-시-swap-영역의-데이터가-모두-삭제된다-swap-메모리-초기화-시에는해당-파티션-해당-파일을-원마운트하고-비워야-한다-1-9-swap-영역-다시-활성화swap-파티션-활성화sudo-swapon-devswap-partitionswap-파일-활성화sudo-swapon-pathtoswapfile-2-swap-메모리란주-메모리ram가-모두-사용되어-추가-메모리가-필요할-때-디스크-공간을-활용하여-부족한-메모리를-대체할-수-있는-공간이다-운영-체제는-일부-데이터를-ram에서-디스크의-swap-영역하드디스크의-특정-파티션-혹은-swap-파일으로-옮겨-메모리-부족상태를-해결한다-하드디스크를-사용하는-것이-아니라-속도-측면에선-아주-떨어지지만-시스템-안정성과-성능-유지에-큰-역할을-한다-실제-메모리보다-큰-프로그램을-실행하거나-동시에-더-많은-프로세스를-실행하는-데-사용된다-예를-들어-ec2-프리티어의-경우-t2micro-ram-은-1g-뿐이지만-임시로-swap-메모리를-설정하면-여러-개-프로세스-띄울-수-있게-된다그렇다면-ram이-낮은-인스턴스를-사용하고-swap메모리를-사용하는-것이-비용적으로-효율적일까swap-메모리는-데이터-전송속도가-느리기에-swap-메모리-초기화-시에는해당-파티션-해당-파일을-원마운트하고-비워야-한다-영역으로-데이터를-옮기는-작업이-발생할-때-성능이-저하된다-빈번한-swap이-발생할-경우-성능향상을-위해서-ram을-추가해야-한다참고httpsreakwontistorycom96httpsreakwontistorycom96httpsjw910911tistorycom122httpsjw910911tistorycom122httpsserverfaultcomquestions688627swapon-failed-device-or-resource-busy-on-mounted-diskhttpsserverfaultcomquestions688627swapon-failed-device-or-resource-busy-on-mounted-disklinux-swap-memory&#34;&gt;1-1. Swap 메모리 확인&lt;code&gt;swapon -sfree -h&lt;/code&gt;![](/images/posts/34/스크린샷 2023-10-23 오후 3.25.49.png)&amp;gt; shared = 하나의 프로세스에서 다른 프로세스의 데이터에 효율적으로 접근하기 위해 사용하는 메모리buff/cache = 버퍼와 캐시를 위해 사용하는 메모리, 커널이 성능향상을 위해 캐시 영역으로 사용되는 메모리buff = 프로세스가 사용하는 메모리 영역이 아닌 시스템 성능향상을 위해 커널이 사용하고 있는 영역cache = 캐시영역의 메모리, I/O 작업을 더 빠르게 하기 위해 커널에서 사용* total - buff/cache - free = 사용중인 메모리* total - used - buff/cache = 실제 사용가능한 메모리### 1-2. Swap 메모리 추가&lt;code&gt;sudo dd if=/dev/zero of=/swapfile bs=1024 count=200000&lt;/code&gt;&amp;gt; bs = 포멧 단위 (bs = 1M 로도 사용 가능)count = 블록수, 1kb를 200000번 /dev/zero로 초기화하기에 총 200MB의 공간을 Swap 파일로 포맷한 것으로 메모리 크기를 의미한다. (count=2000000 -&amp;gt; 2GB swap 메모리)![](/images/posts/34/스크린샷 2023-10-23 오후 3.26.38.png)![](/images/posts/34/스크린샷 2023-10-23 오후 3.27.53.png)### 1-3. mkswap으로 Swap 파일로 포멧&lt;code&gt;sudo mkswap swapfilesudo chmod 600 /swapfile&lt;/code&gt;![](/images/posts/34/스크린샷 2023-10-23 오후 3.28.23.png)![](/images/posts/34/스크린샷 2023-10-23 오후 3.28.59.png)### 1-4. Swap 메모리 활성화&lt;code&gt;-- 단일 Swap 메모리 onsudo swapon swapfile-- 전체 Swap 메모리 onswapon -a&lt;/code&gt;swapon: /swapfile: swapon failed: Device or resource busy에러가 뜬다면 sudo swapoff /swapfile로 Swap 비활성화 후 다시 시도해 주면 된다.![](/images/posts/34/스크린샷 2023-10-23 오후 3.35.11.png)Swap 메모리가 활성된 후에 다시 &amp;ldquo;1-1. Swap 메모리 확인&amp;quot;을 해보면, 설정값만큼의 Swap 메모리가 활성화된 것을 확인할 수 있다.### 1-5. 시스템 재시작 시에도 Swap 메모리 활성화&lt;code&gt;sudo vi /etc/fstab&lt;/code&gt;해당 파일을 열어 맨 아랫줄에&lt;code&gt;/sawpfile swap swap default 0 0&lt;/code&gt;를 추가하면 된다.### 1-6. Swap 메모리 비활성화&lt;code&gt;-- 단일 Swap 메모리 offsudo swapoff swapfile-- 전체 Swap 메모리 offswapoff -a&lt;/code&gt;### 1-7. Swap 메모리 삭제&lt;code&gt;sudo rm -r swapfile&lt;/code&gt;swap out -&amp;gt; swap in 되면서 실제 물리 메모리로 이동한다. (다소 시간이 걸리는 작업이다.)### 1-8. Swap 영역 초기화Swap 파티션 초기화&lt;code&gt;sudo mkswap /dev/{swap partition}&lt;/code&gt;Swap 파일 초기화&lt;code&gt;sudo truncate -s 0 /path/to/swapfilesudo chmod 600 /path/to/swapfilesudo mkswap /path/to/swapfile&lt;/code&gt;Swap 초기화 시, Swap 영역의 데이터가 모두 삭제된다. Swap 메모리 초기화 시에는해당 파티션, 해당 파일을 원마운트하고 비워야 한다.## 1-9. Swap 영역 다시 활성화Swap 파티션 활성화&lt;code&gt;sudo swapon /dev/{swap partition}&lt;/code&gt;Swap 파일 활성화&lt;code&gt;sudo swapon /path/to/swapfile&lt;/code&gt;## 2. Swap 메모리란?주 메모리(RAM)가 모두 사용되어 추가 메모리가 필요할 때 디스크 공간을 활용하여 부족한 메모리를 대체할 수 있는 공간이다. 운영 체제는 일부 데이터를 RAM에서 디스크의 Swap 영역(하드디스크의 특정 파티션 혹은 Swap 파일)으로 옮겨 메모리 부족상태를 해결한다. 하드디스크를 사용하는 것이 아니라 속도 측면에선 아주 떨어지지만 시스템 안정성과 성능 유지에 큰 역할을 한다. 실제 메모리보다 큰 프로그램을 실행하거나 동시에 더 많은 프로세스를 실행하는 데 사용된다. 예를 들어 ec2 프리티어의 경우 t2.micro RAM 은 1G 뿐이지만 임시로 swap 메모리를 설정하면, 여러 개 프로세스 띄울 수 있게 된다.그렇다면 RAM이 낮은 인스턴스를 사용하고 Swap메모리를 사용하는 것이 비용적으로 효율적일까?Swap 메모리는 데이터 전송속도가 느리기에 Swap 메모리 초기화 시에는해당 파티션, 해당 파일을 원마운트하고 비워야 한다. 영역으로 데이터를 옮기는 작업이 발생할 때 성능이 저하된다. 빈번한 Swap이 발생할 경우 성능향상을 위해서 RAM을 추가해야 한다.참고&lt;a href=&#34;https://reakwon.tistory.com/96&#34;&gt;https://reakwon.tistory.com/96&lt;/a&gt;&lt;a href=&#34;https://jw910911.tistory.com/122&#34;&gt;https://jw910911.tistory.com/122&lt;/a&gt;&lt;a href=&#34;https://serverfault.com/questions/688627/swapon-failed-device-or-resource-busy-on-mounted-disk&#34;&gt;https://serverfault.com/questions/688627/swapon-failed-device-or-resource-busy-on-mounted-disk&lt;/a&gt;#Linux #swap memory&lt;/h3&gt;</description>
    </item>
    <item>
      <title>[LLM] Google Cloud Discovery Engine 데이터 스토어 업로드 포맷</title>
      <link>http://localhost:1313/posts/121/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/121/</guid>
      <description></description>
    </item>
    <item>
      <title>[PostgreSQL] BRIN 인덱스의 원리 및 특징</title>
      <link>http://localhost:1313/posts/11/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/11/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;▪ Block range index의 약자▪ Page 검색에 도움 되는 메타 데이터를 뽑아서 인덱스를 구성 (ex, 특정컬럼의 최대/최솟값)▪ 특정 컬럼이 물리 주소의 일정한 상관관계를 가지는 매우 큰 테이블을 다루기 위해 설계 (타임시쿼스한 대용량 데이터 조회에 유용)Block range는 테이블 내에서 근접한 물리주소를 가진 page 그룹을 의미한다. 각 Block range 에 대해 일부 요약 정보가 인덱스로 저장된다. 예를 들어 상점의 판매 주문을 저장하는 테이블에는 각 주문이 배치된 날짜 열이 있을 수 있으며 대부분의 경우 이전 주문시점에 맞게 순차적으로 주문정보가 들어갈 것이고, ZIP 코드 열을 저장하는 테이블에는 도시에 대한 모든 코드가 자연스럽게 그룹화되어 있을 것이다.BRIN 인덱스는 정기적인 비트맵 인덱스 검색을 통해 쿼리를 결과를 확인하고, 인덱스에 의해 저장된 요약 정보가 쿼리 조건과 일치하면, 범위 내 모든 페이지의 모든 튜플을 반환한다. 쿼리 실행기는 반환된 튜플을 다시 검사하고, 쿼리 조건과 일치하지 않는 튜플을 폐기한다. (결과가 일치하지 않아 폐기된 인덱스는 손실된다.) BRIN 인덱스는 매우 작기 때문에 인덱스를 스캔하면 순차적 스캔에 비해 오버헤드가 거의 발생하지 않지만, 일치하는 튜플이 없는 것으로 알려진 테이블의 많은 부분을 스캔하는 것은 피할 수 있다.BRIN 인덱스가 저장할 특정 데이터는 인덱스의 각 열에 대해 선택된 연산자 유형에 따라서도 달라진다. 예를 들어 선형 정렬 순서를 갖는 데이터 유형은 각 블록 범위 내에서 최솟값과 최댓값을 저장할 수 있고, 기하학적 유형은 블록 범위의 모든 객체에 대한 경계 정보를 저장할 수도 있다.## 2. BRIN 인덱스 관리Brin 인덱스가 생성될시, 모든 존재하는 heap page를 스캔하고, 각 block range마다 요약 인덱스 tuple을 생성하고 마지막으로 불완전한 block range를 생성한다. 새로운 page가 데이터로 가득 차면, 이미 요약된 block range가 새 튜플의 데이터로 요약 정보가 업데이트된다. 마지막 요약 범위에 속하지 않는 새 페이지가 생성되면 요약 튜플을 자동으로 획득하지 않고 나중에 요약 실행이 호출될 때까지 해당 튜플은 요약되지 않은 상태로 남아 해당 범위에 대한 초기 요약을 만든다. 이 과정을 직접 실행하는 몇 가지 방법이 있다. 테이블을 auto vacuum 하여 요약되지 않은 page ranges를 요약한다. 만약 auto summarize 파라미터가 on이라면(default 아님), autovacuum이 데이터베이스에 실행될 때마다 summarization이 실행된다.&lt;code&gt;--요약 안된 전체 범위 요약brin_summarize_new_values(regclass)--주어진 page만 요약 (요약안됐을 경우에만)brin_summarize_range(regclass, bigint)&lt;/code&gt;을 통해 ranges에 summarization 실행 가능하다. 반대로  다음을 통해 요약을 해제 하는것도 가능하다.&lt;code&gt; brin_desummarize_range(regclass, bigint)&lt;/code&gt;tuple의 기존값이 변경되어 인덱스 tuple이 더 이상 좋은 결과를 나타내지 못할 때 유용하다.## 3. BRIN VS B-TREE&amp;gt; ▪ BRIN 인덱스는 B-TREE 인덱스보다 쿼리 성능이 좋다.▪ BRIN 인덱스는, B-TREE에서 사용하는 용량의 1%만 사용한다.▪ BRIN이 특정 블록 범위만 다루다 보니, 검색 범위를 이탈할 경우 해당하는 블록 범위 전체를 검사한다.▪ BRIN은 lossy index이므로, 데이터의 hash 값을 저장하는 컬럼에 BRIN을 써도 데이터가 포함된 블록을 정확히 반환하지 못한다.▪ 인덱스 생성 속도가 BRIN이 더 빠르다.## 4. 연산자이름인덱싱 된 데이터 유형인덱싱 가능한 연산자abstime_minmax_opsabstime= &amp;gt;int8_minmax_opsbigint= &amp;gt;bit_minmax_opsbit= &amp;gt;varbit_minmax_opsbit varying= &amp;gt;box_inclusion_opsbox&amp;raquo; ~= @&amp;gt; &amp;gt; |&amp;amp;&amp;gt;bytea_minmax_opsbytea= &amp;gt;bpchar_minmax_opscharacter= &amp;gt;char_minmax_ops&amp;quot;char&amp;quot;= &amp;gt;date_minmax_opsdate= &amp;gt;float8_minmax_opsdouble precision= &amp;gt;inet_minmax_opsinet= &amp;gt;network_inclusion_opsinet&amp;amp;&amp;amp; &amp;raquo;= &amp;gt;int4_minmax_opsinteger= &amp;gt;interval_minmax_opsinterval= &amp;gt;macaddr_minmax_opsmacaddr= &amp;gt;name_minmax_opsname= &amp;gt;numeric_minmax_opsnumeric= &amp;gt;pg_lsn_minmax_opspg_lsn= &amp;gt;oid_minmax_opsoid= &amp;gt;range_inclusion_opsany range type&amp;raquo; @&amp;gt;  &amp;gt;=float4_minmax_opsreal= &amp;gt;reltime_minmax_opsreltime= &amp;gt;int2_minmax_opssmallint= &amp;gt;text_minmax_opstext= &amp;gt;tid_minmax_opstid= &amp;gt;timestamp_minmax_opstimestamp without time zone= &amp;gt;timestamptz_minmax_opstimestamp with time zone= &amp;gt;time_minmax_opstime without time zone= &amp;gt;timetz_minmax_opstime with time zone= &amp;gt;uuid_minmax_opsuuid= &amp;gt;참고 :&lt;a href=&#34;https://bajratech.github.io/2016/09/16/Postgres-BRIN-Index/&#34;&gt;https://bajratech.github.io/2016/09/16/Postgres-BRIN-Index/&lt;/a&gt;&lt;a href=&#34;https://www.postgresql.kr/docs/13/brin-intro.html&#34;&gt;https://www.postgresql.kr/docs/13/brin-intro.html&lt;/a&gt;#brin #Index #PostgreSQL&lt;/p&gt;</description>
    </item>
    <item>
      <title>[PostgreSQL] GIN인덱스의 원리 및 특징</title>
      <link>http://localhost:1313/posts/10/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/10/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Generalized Inverted Index의 약자이다. 이전 포스트인 full text search에서 사용하는 인덱스의 유형. 기본 구조는 B-tree와 유사하지만, 저장 형태가 다르다.  저장된 요소 자제에 대한 검색이 아닌 인덱스 컬럼의 값을 split 한 token인 lexeme 배열에 대해서 검색을 한다. array_ops, tsvector_ops, jsonb_ops, jsonb_path_ops 등 의 built-in operators를 통해 접근이 가능하다.## 2. full text search에서의 적용### 2-1. 샘플 테이블 및 데이터 생성&lt;code&gt;create table ts(doc text, doc_tsv tsvector);insert into ts(doc) values(&#39;Can a sheet slitter slit sheets?&#39;),(&#39;How many sheets could a sheet slitter slit?&#39;),(&#39;I slit a sheet, a sheet I slit.&#39;),(&#39;Upon a slitted sheet I sit.&#39;),(&#39;Whoever slit the sheets is a good sheet slitter.&#39;),(&#39;I am a sheet slitter.&#39;),(&#39;I slit sheets.&#39;),(&#39;I am the sleekest sheet slitter that ever slit sheets.&#39;),(&#39;She slits the sheet she sits on.&#39;);update ts set doc_tsv = to_tsvector(doc);create index on ts using gin(doc_tsv);select doc from ts where doc_tsv @@ to_tsquery(&#39;many &amp;amp; slitter&#39;);&lt;/code&gt;### 2-2. 조회 결과 및 플랜 확인&lt;code&gt;                             QUERY PLAN---------------------------------------------------------------------Bitmap Heap Scan on tsRecheck Cond: (doc_tsv @@ to_tsquery(&#39;many &amp;amp; slitter&#39;::text))-&amp;gt;  Bitmap Index Scan on ts_doc_tsv_idxIndex Cond: (doc_tsv @@ to_tsquery(&#39;many &amp;amp; slitter&#39;::text))(4 rows)``                     doc---------------------------------------------How many sheets could a sheet slitter slit?(1 row)&lt;/code&gt;### 2-3. 작동 방식&amp;gt; ▪ 먼저 쿼리에서 검색에 사용할 lexeme인 &amp;lsquo;many&amp;rsquo;와 &amp;lsquo;slitter&amp;rsquo;를 추출한다. ▪ lexeme B-tree에서 2개의 키를 동시에 찾는다.- mani = (0,2)- slitter = (0,1),(0,2),(1,2),(1,3),(2,2)▪ 마지막으로, 발견된 TID각각에 대해 검색 쿼리에 부합하는지 확인한다.(예제의 쿼리의 경우 and 조건이기에 (0,2)에 해당하는 TID만 리턴하게 된다.)&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/posts/10/img.png&#34;&gt;## 3. 특징▪ GIN의 업데이트는 매우 느리다. document는 보통 많은 lexeme을 포함하고, 1개의 document가 업데이트되거나 추가된다고 해도 인덱스 트리 내에서는 많은 업데이트가 진행된다.▪ 반면에, 몇몇의 document가 동시에 업데이트된다면, 중복되는 Lexeme들이 존재할 것이고, 총 인덱스 업데이트량은 개별 업데이트 시보다 줄어들 것이다.▪ GIN인덱스의 또 하나의 특징은 항상 결과를 bitmap으로 리턴한다는 것이다. (TID 자체로 리턴하지 않는다.) 그렇기 때문에 Limit을 통한 결괏값 제한은 그렇게 효율적이지 않다.▪ full text search, array, json 등의 타입 조회에 효율적이다.참고 : &lt;a href=&#34;https://postgrespro.com/blog/pgsql/4261647&#34;&gt;https://postgrespro.com/blog/pgsql/4261647&lt;/a&gt;#Gin #Index #PostgreSQL&lt;/p&gt;</description>
    </item>
    <item>
      <title>[PostgreSQL] Visibility Map(가시성 맵)의 개념, 원리, 생명주기 및 정보 확인 방법</title>
      <link>http://localhost:1313/posts/79/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/79/</guid>
      <description>&lt;p&gt;Visibility Map은 트랜잭션에서 데이터에 접근할 때 어떤 데이터가 가시적인지(모든 트랜잭션에서 읽을 수 있는지), 안정적인지 (동결된 튜플인지) 판별하는데 도움을 준다. 데이터 접근 시 불필요한 I/O작업을 줄여주고, 데이터베이스가 어떤 페이지를 직접 접근할 수 있는지를 빠르게 판단함으로써 시스템의 효율적을 올려주는 역할을 한다.## 2. Visibility Map(가시성 맵)의 데이터 관리Visibility Map은 데이터를 주요 데이터와는 별도의 파일(fork)에 _vm 접미사를 붙여 관리한다. 예를 들어 예를 들어 employees 테이블이 있다고 하면 테이블의 Visibility Map은 별도의 포크에 저장된다. 이 포크의 이름은 파일 노드 번호에 _vm 접미사를 붙여 구성되며, 예를 들어 파일 노드번호가 12345인 경우 VM 파일은 12345_vm으로 저장된다. 데이터에는 해당 테이블의 page가 모든 트랜잭션에 보이는지, 동결된 튜플만을 포함하는지 등의 정보를 저장한다. 데이터베이스가 employees 테이블을 조회할 때, 가시성 맵을 먼저 확인한다. 만약 쿼리가 접근하려는 pages가 모든 트랜잭션에게 보이는 상태라고 확인되면, 시스템은 데이터에 더 빠르게 접근한다. 불필요한 버전검사나 락을 안 해도 되기에 성능이 향상된다.## 3. Visibility Map(가시성 맵)의 원리Visiblity Map은 힙 pages당 2개의 비트를 별도로 저장한다. 첫 번째 비트가 설정되어 있으면, 해당 페이지가 모두 visible(가시적) 한 상태이고, 이는 vacuum이 필요한 튜플을 포함하지 않는다는 뜻이다. 이는 인덱스 영역의 tuple만을 사용하여 index-only-scan으로 쿼리를 조회할 때도 사용된다. index-only-scan은 해당 포스트에서 확인 가능하다.&lt;a href=&#34;https://junhkang.tistory.com/70&#34;&gt;2024.03.13 - [Postgresql] - [PostgreSQL] Index-Only 스캔과 Covering 인덱스, Index-only스캔의 효율적인 사용&lt;/a&gt;[PostgreSQL] Index-Only 스캔과 Covering 인덱스, Index-only스캔의 효율적인 사용1. Index-Only Scans PostgreSQL의 모든 인덱스는 &amp;ldquo;보조(Secondary)&amp;rdquo; 인덱스이다. 각 인덱스는 테이블의 메인 데이터 영역(테이블의 heap 영역)과 분리되어서 저장된다. 그렇기 때문에 일반적인 인덱스 스캔에junhkang.tistory.com두 번째 bit가 설정되어 있다면 모든 pages의 튜플이 frozen(동결된) 상태라는 뜻이다. 이 상태에선 일반적인 vacuum은 물론 anti-wraparound vacuum도 동작시킬 필요가 없다.anti-wraparound-vacuum - 전체 데이터베이스를 검사하여 트랜잭션 ID가 안전한 범위 내에 있는지 확인하여, 필요에 따라 조정하며 트랜잭션 ID의 오버플로우를 방지한다. 트랜잭션 ID에 대한 상세 내용은 해당 포스트에서 확인 가능하다.&lt;a href=&#34;https://junhkang.tistory.com/67&#34;&gt;2024.03.08 - [Postgresql] - [PostgreSQL] 트랜잭션(Transaction)의 작동원리&lt;/a&gt;[PostgreSQL] 트랜잭션(Transaction)의 작동원리1. 기본 트랜잭션의 개념 및 원리 트랜잭션의 기본 개념과 사용 방법은 다음 포스트에서 확인이 가능하다. 2023.10.10 - [Postgresql] - [PostgreSQL] 트랜잭션(Transaction)의 개념 및 사용 [PostgreSQL] 트랜잭션(Tjunhkang.tistory.comVisiblity Map의 2가지 비트는 최대한 보수적으로 해석된다. 1, 2 번째 비트가 설정되어 있을 경우에는 무조건 참이지만, 비트가 설정되지 않을 경우에는 참 일수도 거짓일 수도 있다.## 4. Visibility Map(가시성 맵)의 생명주기Visiblity Map의 비트는 vacuum에 의해서만 설정된다. 데이터베이스 내의 pages에 vacuum 작업이 수행되면 관련 Visiblity Map의 비트가 설정이 되고, 해당 pages가 모든 트랜잭션에서 완전히 가시적임을 표시하며, 더 이상 vacuum 안 해도 됨을 나타낸다. 그 후에 pages의 데이터가 하나라도 수정(update, insert, delete 등) 될 경우, VM의 비트는 초기화된다. 데이터의 상태가 변경되었기에 vacuum 작업 대상에 포함시켜야 함을 나타낸다.## 5. Visibility Map(가시성 맵) 정보 확인pg_visibility 함수를 사용해서 vm에 저장된 정보를 확인할 수 있다.- pg_visibility_map(relation regclass, blkno bigint, all_visible OUT boolean, all_frozen OUT boolean) returns record- 해당 테이블, 해당 블록의 모든 VM의  visible, frozen 비트 조회- pg_visibility(relation regclass, blkno bigint, all_visible OUT boolean, all_frozen OUT boolean, pd_all_visible OUT boolean) returns record - 해당 테이블, 해당 블록의 모든 VM의  visible, frozen 비트 조회 + PD_ALL_VISIBLE 비트- pg_visibility_map(relation regclass, blkno OUT bigint, all_visible OUT boolean, all_frozen OUT boolean) returns setof record - 해당 테이블의 모든 블록의 VM의  visible, frozen 비트 조회- pg_visibility(relation regclass, blkno OUT bigint, all_visible OUT boolean, all_frozen OUT boolean, pd_all_visible OUT boolean) returns setof record - 해당 테이블의 모든 블록의 VM의  visible, frozen 비트 조회 + PD_ALL_VISIBLE 비트- pg_visibility_map_summary(relation regclass, all_visible OUT bigint, all_frozen OUT bigint) returns record - VM에 연관 있는 테이블의 visible 페이지 수량, frozen 페이지 수량 확인- pg_check_frozen(relation regclass, t_ctid OUT tid) returns setof tid - VM에 frozen으로 마킹되어 있는 pages 중 non-frozen 튜플의 TID, 존재해서는 안 되는 경우로, 뭔가 조회가 된다면 VM에 문제가 있는 것- pg_check_visible(relation regclass, t_ctid OUT tid) returns setof tid - VM에 visible으로 마킹되어 있는 pages 중 non-visible 튜플의 TID, 존재해서는 안 되는 경우로, 뭔가 조회가 된다면 VM에 문제가 있는 것- pg_truncate_visibility_map(relation regclass) returns void - 해당 테이블의 VM을 truncate 한다. VM에 문제가 있는 경우 강제로 재설정이 필요할 때 사용. 해당 테이블의 첫 번째 vacuum이 실행될 때 재생성되며, 그전까지는 모든 VM이 모두 0 값으로 유지참고&lt;a href=&#34;https://www.postgresql.org/docs/16/storage-vm.html&#34;&gt;https://www.postgresql.org/docs/16/storage-vm.html&lt;/a&gt;&lt;a href=&#34;https://www.postgresql.org/docs/16/pgvisibility.html&#34;&gt;https://www.postgresql.org/docs/16/pgvisibility.html&lt;/a&gt;#PostgreSQL #Visibility Map #가시성 맵&lt;/p&gt;</description>
    </item>
    <item>
      <title>[PostgreSQL] 날짜 형태 검증하기 (ERROR: date/time field value out of range)</title>
      <link>http://localhost:1313/posts/38/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/38/</guid>
      <description>&lt;p&gt;데이터 베이스에서 날짜형태로 형 변환을 하는 것은 다음과 같은 방법으로 쉽게 가능하다.&lt;code&gt;-- Unix타임(int)형 변환SELECT to_timestamp(1658792421)-- varchar 타입 변환SELECT to_timestamp(&#39;20231026&#39;,&#39;yyyymmdd&#39;)-- 날짜형을 char로 변환SELECT to_char(to_timestamp(1658792421), &#39;DD-MM-YYYY&#39;)&lt;/code&gt;2. 유효한 날짜형태 검증데이터 정제가 완료되지 않아 조회하려는 데이터에 날짜유형에서 벗어난 데이터 (&amp;lsquo;20231301&amp;rsquo;,202301&amp;rsquo;, &amp;lsquo;20231232&amp;rsquo; 등)가 하나라도 있을 경우 조회 자체가 안된다. 그럴 경우 날짜 규격에 맞지 않는 데이터를 보정 후 연산해야 하는 경우가 있는데 단순 월별 케이스문으로 분리하여 날짜 유형에 어긋나는 경우를 찾을 수도 있지만 row마다 날짜 유형이 다르거나 윤달을 체크할 수 없다.그래서 날짜 형태자체를 변환시도하고 성공 여부에 따라 결과값을 추출하는 함수를 생성해야 한다.&lt;code&gt;CREATE OR REPLACE FUNCTION VALIDATE_DATE(S VARCHAR) RETURNS INT AS$$BEGINIF COALESCE(S, &#39;-&#39;) = &#39;-&#39; THENRETURN -1;END IF;PERFORM S::DATE;RETURN 0;EXCEPTIONWHEN OTHERS THENRETURN 1;END;$$ LANGUAGE PLPGSQL;&lt;/code&gt;이제 VALIDATE_DATE() 함수를 실행시키면, 날짜유형, 윤달에 상관없이 해당 데이터가 유효한 날짜라면 0, 유효하지 않은 데이터라면 1, null이라면 -1을 리턴하게 된다.다음과 같이 유효한 날짜 유형의 데이터만 조회하거나&lt;code&gt;SELECT date_column from tableWHERE VALIDATE_DATE(date_column) = 0;&lt;/code&gt;날짜 유형에 어긋나는 데이터들을 일괄 null 업데이트하여 처리할 수 있다.&lt;code&gt;UPDATE table SET date_column = NULLWHERE VALIDATE_DATE(date_column) = 1&lt;/code&gt;#PostgreSQL&lt;/p&gt;</description>
    </item>
    <item>
      <title>[PostgreSQL] 대량 데이터 인서트 시 성능 개선 및 주의 사항</title>
      <link>http://localhost:1313/posts/65/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/65/</guid>
      <description>&lt;p&gt;최초 서비스 배포나 데이터 마이그레이션을 할 때 대량의 데이터를 한 번에 인서트 하는 경우가 있다. PostgreSQL 공식문서에서는 대량 인서트 시에 효율적으로 진행할 수 있는 방법을 제시해 준다. (대량 데이터를 인서트 할 때 효율적인 설정이지 데이터베이스 조회나 업데이트 등실제 운영 시에 사용할 방법은 아니다.)## 1. Autocommit 옵션 해제대량의 인서트 실행 시, Autocommit 옵션을 해제하고 한 트랜잭션에서 작업 후에 커밋을 진행해야 한다(일반적으로 SQL를 실행 시에 자동으로 시작 시 BEGIN, 끝날 때 COMMIT으로 트랜잭션 처리가 되지만, 확실히 되고 있는지 확인필요하다.). 대량 데이터 인서트의 각각을 별도로 commit 한다면, PostgreSQL은 인서트 되는 각 열에 대해 너무 많은 작업을 수행하게 된다. 또한 모든 인서트를 한 트랜잭션에 처리할 경우에는 한 INSERT가 실패할 경우 그 시점까지 인서트 된 모든 작업이 취소되기에 실패 작업에 대한 부분 보완 및 무결성을 고려하지 않아도 된다.## 2. COPY, PREPARE 사용INSERT를 여러 번 실행시키기보다 COPY 커맨드 한 번으로 해결하라. COPY 명령어는 많은 ROW를 로드하는데 최적화되어 있다. INSERT 구문보다 덜 유연하지만, 대량 데이터를 로딩하는데 훨씬 적은 오버헤드를 발생시킨다. (COPY는 단일 명령어이기에 실행 시 Autocommit을 따로 비활성화시킬 필요 없다.) COPY를 사용할 수 없는 상황이라면 PREPAPRE 구문을 통해 준비된 INSERT 구문을  EXECUTE를 통해 필요한 만큼 실행시키는 방법도 있다. 이 방법은 INSERT 구분을 반복적으로 사용할 때 드는 파싱과 실행계획의 오버헤드를 줄여준다.COPY를 사용한 대량 데이터 로딩은 INSERT보다 거의 모든 경우에 더 빠르다. (PREPARE 구문을 사용하고 단일 트랜잭션에서 배치를 통해 INSERT를 한다고 해도 COPY를 사용하는 것이 더 빠르기 때문에 가능하다면 COPY를 사용하는 것이 유리하다.) COPY는 해당 테이블의 CREATE TABLE 혹은 TRUNCATE 명령어와 같이 쓸 때 더 빠르다. 이 경우에 에러가 날 경우에 최신으로 로드된 데이터를 포함하고 있는 파일은 무조건 삭제되기 때문에 WAL write가 필요 없다. (WAL의 개념은 다음 포스트를 참고)&lt;a href=&#34;https://junhkang.tistory.com/66&#34;&gt;https://junhkang.tistory.com/66&lt;/a&gt;[PostgreSQL] WAL (Write-Ahead Logging) / 아카이브 모드 백업(Archive mode backup)의 개념 및 장단점1. WAL (Write-Ahead Logging) / 아카이브 모드 백업(Archive mode backup)이란? 아카이브 모드 백업을 이해하기 위해 WAL에 대한 개념을 먼저 살펴보자. WAL은 PostgreSQL에서 데이터의 무결성을 보장하는 표준 방junhkang.tistory.com그러나 wal_level이 minimal로 설정되어 있을 경우에만 유효하다.## 3. 인덱스를 삭제하라아얘 새로운 테이블을 생성하는 상황이라면, 가장 빠른 방법은 테이블을 만들고 COPY를 사용해서 테이블 데이터를 채우고 테이블에 필요한 인덱스를 그 후에 생성하는 것이다. 이미 존재하는 데이터에 인덱스를 생성하는 것이 데이터를 인서트 하면서 인덱스를 생성하는 것보다 더 빠르다.기존 테이블에 데이터를 인서트 하는 상황이라면, 인덱스를 삭제하고, 데이터를 채우고, 인덱스를 다시 생성하는 것이 유리하다. 물론 인덱스가 사리진 기간 동안 해당 데이터에 접근하는 다른 사용자들은 성능 이슈를 겪을 것이고 Unique 인덱스를 drop 하는 경우에는 인덱스가 누락된 동안 무결성에 영향을 줄 수 있기에 작업과정에서의 영향도를 철저히 확인해야 한다.## 4. FK 제약을 삭제하라인덱스와 동일하게, FK는 벌크로 한 번에 체크하는 것이 row단위로 체크하는 것보다 효율적이다. 그래서 FK제약을 작업 전에 삭제하고, 데이터를 넣고, FK제약조건을 다시 생성한다. 인덱스와 동일하게 인서트 속도가 향상되는 반면 제약조건이 없는 사이에 데이터에 대한 무결성이 깨질 수 있다.이미 존재하는 FK 제약조건이 있는 테이블에 데이터를 넣을 때, FK 제약조건을 체크하는 행위가 서버의 pending trigger 이벤트 목록에 추가된다. 수백만건의 데이터를 인서트 하는 경우에 트리거 이벤트 큐의 가용 메모리를 초과하여 적정량 이상의 메모리 스왑이 발생하거나 실행명령이 완전히 실패할 수도 있다. 그래서 많은 양의 데이터를 인서트 할 때는 FK 조건을 삭제하고 다시 설정해야 한다. 제약조건을 일시적으로 해제할 수 없다면, 더 작은 트랜잭션 단위로 분할하는 것이 유일한 방법일 수도 있다.## 5. maintenance_work_mem 증가일시적으로 maintenance_work_mem 설정 값을 늘리는 것은 대량 데이터를 로딩 시 성능을 향상할 수 있다. 이 옵션은 CREATE INDEX, ALTER TABLE ADD FOREIGN KEY 명령어의 속도를 올려준다. COPY 자체의 속도를 올려주진 않기에 CREATE INDEX, ALTER TABLE ADD FOREIGN KEY를 사용할 때만 유효하다.## 6. max_wal_size 증가일시적으로 max_wal_size 설정값을 늘리는 것은 대량 데이터 로딩을 더 빠르게 해 준다. PostgreSQL에서의 대량 데이터 로딩은 일반적인 checkpoint 체크(checkpoint_timeout 옵션에 해당하는) 보다 더 잦은 checkpoint확인을 요한다. checkpoint가 발생할 때마다, 모든 부적절한 pages가 디스크에서 flush 된다. max_wal_Size를 늘림으로써 필요한 checkpoint의 빈도를 낮출 수 있다.## 7. WAL Archival, Streaming Replication을 미사용WAL Archiving이나 Streaming replication을 사용하는 경우 대량 데이터를 로딩할 때, 급격히 증가하는 WAL 데이터를 처리하는 것보다 로드가 완료된 후 새로운 백업을 실행하는 것이 유리하다. 데이터 로딩 중에 WAL 로깅이 증가하는 것을 방지하기 위해 WAL Archiving, streaming replication을 중지한다. (다음 3가지 옵션을 통해 설정가능)- wal_level = minimal- archive_mode = off- max_wal_senders = 0변경 후 서버를 재시작해야 하고, 기존의 기본 백업 정보들을 아카이브 복구나 standby서버로 사용이 불가능하게 된다. 데이터 손실로 이어질 수 있기 때문에 기존 백업 데이터에 대한 확인이 필요하다.해당 옵션을 적용하면 Archiver 시간이나 WAL 샌더가 WAL데이터를 처리하는 시간을 줄여주는 것뿐만 아니라, 이 작업을 하면 실제로 특정 명령어들의 실행 속도를 더 빠르게 해 준다. WAL_LEVEL 이 mininal일 때, 현재 트랜잭션 혹은 상위 트랜잭션에서 테이블 변경 혹은 인덱스 생성/삭제할 때 WAL을 전혀 작성하지 않기 때문이다. (WAL 아카이빙을 하지 않아도, 마지막에 fsync 실행으로 충돌을 더 효율적으로 방지할 수 있다.)## 8. 작업 후 ANALYZE 실행테이블 내의 데이터 분포를 크게 변경하는 경우에는 ANALYZE를 실행시켜야 한다. 대량 데이터를 인서트 할 때도 유효하며, ANALYZE 혹은 VACUUM ANALZYE를 실행시키면 플래너가 테이블의 최신 통계를 가져오는 것을 확인할 수 있다. 정확하지 않은 통계나 수집되지 않은 통계가 있을 때 플래너는 쿼리 실행계획을 비효율적으로 세울 수 있고, 이는  테이블의 성능저하를 유발한다. (Autovacuum 데몬이 실행 중이라면 Analyze를 자동으로 실행하고 있을 것이다.)## 9. pg_dump 확인 시 주의사항pg_dump에 의해 생성된 Dump script는 대량 데이터 인서트시에 위의 가이드라인(1&lt;del&gt;8)의 일부만을 자동으로 적용시킨다.  pg_dump의 덤프를 최대한 빨리 복구하려면, 몇몇 추가 세팅을 수동으로 해야 한다. (이 작업은 dump를 복구하는데 적용되는 거지, 생성할 때 적용되는 것이 아니다.)Default로, pg_dump는 COPY를 사용하며(가이드 2번 적용),  완벽한 schema-and-data 덤프를 생성할 때는 인덱스 및 외부키를 생성하기 전에 데이터를 로드 (가이드 3,4 적용) 하기 때문에 몇몇 가이드라인이 자동으로 적용되고, 유저는 다음 항목들만 정의하면 된다.- maintenance_work_mem, max_wal_size를 적절한 값으로 설정- WAL archiving, streaming replication을 사용 중이라면, 덤프 복구 중에는 미사용을 고려- archive_mode = off, wal_level = minimal, max_wal_sender = 0을 덤프를 로딩하기 전에 설정하고, 덤프 복구 후에 원래 값으로 되돌리고 기본 백업을 최신으로 실행- pg_dump, pg_restore의 병렬 dump 및 복구 모드를 테스트해서 최적의 동시 job 실행 개수를 적용. (-j 옵션을 사용하여 덤프 및 복원을 적절하게 병렬로 수행하면 직렬보다 더 높은 성능 가능하다.)- 모든 덤프가 단일 트랜잭션으로 복구되도록 설정 (-1 혹은 &amp;ndash;single-transaction 커맨드라인 옵션을 psql 혹은 pg_restore에 날리면 된다.) 다만, 이 모드를 사용하면, 아주 작은 에러라도 날 경우 전체 복구 과정이 롤백되어 몇 시간의 작업을 날릴 수도 있게 된다- DB서버에 여러 개의 CPU를 가용하는 것이 가능하다면 pg_restore의 &amp;ndash;jobs 옵션을 쓸 수 있다. 이를 통해 병렬로 동시에 데이터를 인서트 하고 인덱스도 생성할 수 있다.- 작업 이후 ANALYZE를 실행시킨다data-only 덤프는 여전히 COPY를 사용하지만 인덱스를 삭제하거나 재생성하지 않고 FK에 영향을 주지 않는다. 그래서 data-only 덤프를 로딩할 때, 인덱스나 FK를 삭제한 후 다시 생성하는 방식을 사용할지 말지는 유저의 선택이다. 대량 데이터를 인서트 할 때와 동일하게, max_wal_size를 증가시키는 것은 유리하다, 하지만 data-only 덤프는 maintenance_work_mem을 늘리는 것에 영향을 받지 않는다. 그보다 인덱스와 FK키를 삭제 후 수동으로 다시 생성하는 것이 효율적이다.(* FK를 삭제하는 것 대신에 &amp;ndash;disable-triggers 옵션으로 FK검증트리거 실행을 방지할 수 있지만, 단지 체크를 지연시키는 것으로 무결성에 위배되는 데이터가 들어올 수 있음은 동일하다)## 10. 결론 및 적용 검토실제로 산군의 데이터베이스는 수백만 건의 데이터를 마이그레이션 &amp;amp; 백업하는 작업이 주기적으로 있다. 일부 작업의 경우 타 테이블을 참조하거나 대량 업데이트가 포함되어 있어 해당 테이블의 조회 성능에 직접적인 영향을 주는 경우가 있기에 트랜잭션을 분할하여 배치성으로 작업을 하거나, 특정 테이블을 격리 후 작업 및 동기화를 진행하고 있다. 인서트 시간이 길어지는 만큼 부담이 가는 작업인 만큼, 속도 향상을 위해 다양한 방법을 시도하고 있고, 공식문서에 나온 다음 가이드들을 추가 검토해 보게 되었다.1&lt;/del&gt;8 가이드 중 직접적으로 적용 가능한 부분이 있을까?- 1. Autocommit 해제 - 작업은 기존에도 사용 중 (데이터 무결성, 작업 내용 검증 및 백업을 위해 트랜잭션 컨트롤은 필수)- 2. COPY, PREPARE - COPY 명령어도 사용 중이지만 한계가 있는 경우가 많고, 배치성으로는 이미 작업 중- 3, 4 인덱스, FK 삭제 후 재설정 - 실제 운영 중인 라이브 테이블에 인덱스, FK를 일시적으로 없애는 것은 효율적이지 못함- 5,6,7,8 PG옵션 추가 - 실제 운영 중인 DB에 설정값 적용 및 재부팅을 시도하는 것은 불가능간단하게 적용가능한 1,2번은 보통 다 사용중일 것이고, 전체적으로 운영 중인 데이터베이스에 실행하기엔 위험부담이 큰 작업들이 대부분이다(특히 5&lt;del&gt;8 옵션을 통해 DB자체를 다운시켜야 하거나 기존 백업을 사용할 수 없는 상황은 적용 불가). 대부분 최초 서비스 배포 혹은 리뉴얼 시에 적용 가능한 방법들로 보인다. 현재 운영 중인 데이터베이스의 특정 테이블을 격리하는 방식으로 3&lt;/del&gt;4번이 부분적용이 가능해 보이기는 하지만, 인덱스를 삭제하고 데이터를 인서트시 코스트가 줄어들더라도 대량의 데이터가 들어간 후의 테이블에 인덱스를 추가하는 시간도 만만치 않을 것이다.운영 중인 데이터베이스에 대량 데이터를 인서트시 인덱스, FK에 대한 조정, 옵션값 변경 후 DB 재실행이 불가능한 상황이라면, 배치성, 트랜잭션 분할을 통해 (데이터베이스 성능과 데이터 무결성) 모니터링을 함께 진행하는 것이 가장 효과적일 듯하다.&lt;a href=&#34;https://www.postgresql.org/docs/current/populate.html&#34;&gt;https://www.postgresql.org/docs/current/populate.html&lt;/a&gt;&lt;a href=&#34;https://postgresql.kr/docs/9.6/wal-intro.html&#34;&gt;https://postgresql.kr/docs/9.6/wal-intro.html&lt;/a&gt;#PostgreSQL #대량 데이터 INSERT #DB성능향상&lt;/p&gt;</description>
    </item>
    <item>
      <title>[PostgreSQL] 문자열내 중복 공백, 단어 제거</title>
      <link>http://localhost:1313/posts/12/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/12/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;특정 문자열에 대해서 중복 공백 제거를 하고 싶다면 postgresql 정규식을 사용해서 가능하다.(공백 외에 단일 문자에 대한 중복제거도 동일한 방법으로 가능하다.)&lt;code&gt;select regexp_replace(name, &#39; +&#39;, &#39; &#39;, &#39;g&#39;) from TABLE; -- &#39;g&#39; 옵션을 제거할 경우 최초 건에 대에서만 변경&lt;/code&gt;## 2. 중복 단어 제거&amp;gt; 컬럼 단위 중복제거는 distinct, group by를 통해 쉽게 가능하지만, 컬럼 내 문자열의 중복 단어 제거의 경우 다음과 같다.(쉼표 기준으로 컬럼을 분리, 중복을 제거한 후 다시 연결)&lt;code&gt;select id, array_to_string(array_agg(distinct token), &#39; &#39;) from (SELECT unnest(string_to_array(COLUMN, &#39; &#39;)) as token, id FROM TABLE) as tmpgroup by id&lt;/code&gt;## 3. 실습### 3-1. 테이블 생성&lt;code&gt;-- 테스트 테이블 생성create table duplicate_test (id serial primary key,name varchar(255) not null);&lt;/code&gt;### 3-2. 테스트 데이터 insert&lt;code&gt;-- 테스트 데이터 입력insert into duplicate_test(name)values(&#39;서울  서울    대구 서울 부산&#39;),(&#39;서울 서울 대구     서울&#39;),(&#39;부산 대구 대구 서울   서울서울 서울 광주&#39;),(&#39;서울 에서 대구   갔다가 부산 거쳐 다시   서울 로&#39;),(&#39;광주광주대구 대구 대   구 서울  &#39;),(&#39;서울 서울 서울 &#39;),(&#39;서울 대구 대   구 서울  &#39;),(&#39;서울 대구 대구 대   구 서울  &#39;),(&#39;서울 대구    대구 대구 부산부산 서울 부산 서울부 산&#39;);&lt;/code&gt;![](/images/posts/12/스크린샷 2023-10-04 오후 7.20.58.png)### 3-3. 중복 공백 제거&lt;code&gt;select regexp_replace(name, &#39; +&#39;, &#39; &#39;, &#39;g&#39;) from DUPLICATE_TEST; -- &#39;g&#39; 옵션을 제거할 경우 최초 건에 대에서만 변경select regexp_replace(name, &#39;단일문자열+&#39;, &#39; &#39;, &#39;g&#39;) from DUPLICATE_TEST; -- 단일문자열에 대한 중복 제거도 동일한 방법으로 가능하다.&lt;/code&gt;![](/images/posts/12/스크린샷 2023-10-04 오후 7.20.03.png)### 3-4. 중복 단어 제거&lt;code&gt;select id, array_to_string(array_agg(distinct token), &#39; &#39;) from (SELECT unnest(string_to_array(name, &#39; &#39;)) as token, id FROM DUPLICATE_TEST) as tmpgroup by id&lt;/code&gt;![](/images/posts/12/스크린샷 2023-10-04 오후 7.20.20.png)#SQL #PostgreSQL&lt;/p&gt;</description>
    </item>
    <item>
      <title>[PostgreSQL] 문자열에서 날짜/시간 변환 및 처리 과정</title>
      <link>http://localhost:1313/posts/88/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/88/</guid>
      <description>&lt;p&gt;PostgreSQL의 날짜형태의 칼럼을 조회할 때, 종종 정확한 날짜 형태를 사용하는 것이 아닌, 문자열, 혹은 숫자 형태로 간편하게 조회하는 경우가 있다. 예를 들어 2024/05/02 이후의 값을 조회할 때 다음 두 가지 조회 방법을 사용할 수 있다.&amp;gt; date_column &amp;gt; &amp;lsquo;20240502&amp;rsquo;date_column &amp;gt; TO_DATE(&amp;lsquo;20240502&amp;rsquo;, &amp;lsquo;YYYYMMDD&amp;rsquo;)예제와 같이 PostgreSQL은 일련의 문자/숫자열을 조건에 맞는 날짜형으로 자동으로 디코딩을 해주는데, 문자열을 인식하는 상세 과정을 순서대로 알아보자.## 2. 문자열에서 날짜/시간으로의 디코딩 과정### 2-1. 문자열을 토큰으로 분리하고 각 토큰을 시간, 시간대, 또는 숫자로 분류한다.예제들에서는 정상적으로 날짜 및 시간이 변환되는지 확인하기 위해 강제로 TIMESTAMP 및 DATE로 형 변환을 하였지만, 날짜 형태의 데이터와 문자열 그대로를 비교하여도 날짜 및 시간 비교가 가능하다.- 숫자 토큰이 &amp;ldquo;:&amp;ldquo;를 포함한다면, 시간 문자열로 인식되며, 하나라도 발견되면 이후의 모든 숫자와 콜론은 시간 문자열의 일부로 취급&lt;code&gt;SELECT &#39;20240202 13:45:30&#39;::TIMESTAMP&lt;/code&gt;![](/images/posts/88/스크린샷 2024-05-02 오후 5.36.09.png)13:45:30에 &amp;ldquo;:&amp;ldquo;를 포함하였으니 해당 토큰 전체를 시간 문자열로 취급하여, 13시 45분 30초로 해석된다. (hh:mm, hh:mm:ss 등의 표준규격에 맞는 경우에만)- 숫자 토큰에 하이픈(-), 슬래시(/) 또는 두개이상의 점(.)이 포함되어 있으면 날짜 문자열 취급&lt;code&gt;SELECT &#39;2024-05-02&#39;::DATE;SELECT &#39;05/02/2024&#39;::DATE;SELECT &#39;02.05.2024&#39;::DATE;&lt;/code&gt;![](/images/posts/88/스크린샷 2024-05-02 오후 5.36.24.png)yyyy mm dd의 표준 규격에 맞는 경우 모두 날짜 형태로 해석된다.- 이미 날짜 토큰이 확인된경우 문자열을 시간대 이름 (ex America/New_York)으로 해석&lt;code&gt;SELECT &#39;2023-12-25 America/New_York&#39;::TIMESTAMP WITH TIME ZONE;&lt;/code&gt;![](/images/posts/88/스크린샷 2024-05-02 오후 5.38.56.png)2023-12-25 America/New_York 은, 앞부분에서 이미 날짜 토큰이 확인되었기에, 뒷부분은 시간대 이름으로 해석된다.- 토큰이 숫자만으로 구성되어 있으면 단일필드이거나 ISO 8601 형식의 날짜로 해석된다.&lt;code&gt;SELECT &#39;19990113&#39;::DATE;&lt;/code&gt;![](/images/posts/88/스크린샷 2024-05-02 오후 5.39.45.png)19990113 = 1999년 1월 13일 또는 (141516 = 14시 15분 16초)- 토큰이 +, -로 시작하면 숫자 시간대 또는 특별필드이다.+0200 : UTC보다 2시간 빠른 시간대-0500 : UTC보다 5시간 늦은 시간대+15 : 현재 날짜로부터 15일 후 날짜-3 : 현재 날짜로부터 3일 전 날짜&lt;code&gt;SELECT &#39;20240202 12:00 +0200&#39;::TIMESTAMP WITH TIME ZONE;&lt;/code&gt;![](/images/posts/88/스크린샷 2024-05-02 오후 5.49.12.png)### 2-2. 토큰이 알파벳 문자열이라면 가능한 문자열 사전을 조회한다.- 토큰이 알려진 시간대 약어 중에 일치하는 게 있는지 확인&lt;code&gt;SELECT &#39;20240202 12:00 PM EST&#39;::TIMESTAMP WITH TIME ZONE;&lt;/code&gt;![](/images/posts/88/스크린샷 2024-05-02 오후 5.41.50.png)&amp;ldquo;12:00 PM EST&amp;rdquo; 에서 EST는 동부표준시(Eastern Standard Time)의 약어로, UTC 오프셋 매핑 딕셔너리에 포함되어 있어 사용 가능- 발견된 문자열이 없으면 내부 테이블을 검색하여 토큰을 특별 문자열 (ex, today, Thursday, January) 혹은 at, on 같은 조사와 매칭시킨다&lt;code&gt;SELECT &#39;Today&#39;::DATE + 1;&lt;/code&gt;![](/images/posts/88/스크린샷 2024-05-02 오후 5.42.17.png)&amp;lsquo;Today&amp;rsquo;::date + 1 은 내일 날짜를 반환한다.- 문자열이 위 두조건에 부합하지 않는다면 에러 발생### 2-3. 토큰이 숫자, 숫자 필드로만 이루어져 있을 때- 6 , 8자리이며 다른 날짜 필드가 발견되기 전이라면 날짜 형태로 해석(YYYYMMDD 혹은 YYMMDD)&lt;code&gt;SELECT &#39;20240201&#39;::DATESELECT &#39;240201&#39;::DATE&lt;/code&gt;20240201, 240201 모두  2024년 02월 01일로 해석된다.- 토큰이 3자리 숫자이고, 이미 연도가 발견되었다면, 그 해의 n번째 일수로 해석&lt;code&gt;SELECT &#39;2024 021&#39;::DATE;&lt;/code&gt;![](/images/posts/88/스크린샷 2024-05-02 오후 5.43.23.png)&amp;lsquo;2024 021&amp;rsquo;::date 은 2024년의 21번째 날로 2024년 01월 21일로 해석된다.- 네 자리 또는 여섯 자리 숫자이고 이미 날짜가 발견되었다면, 시간(HHMM 또는 HHMMSS)으로 해석&lt;code&gt;SELECT &#39;20240502 123422&#39;::TIMESTAMP;&lt;/code&gt;![](/images/posts/88/스크린샷 2024-05-02 오후 5.44.27.png)&amp;lsquo;20240502 123422&amp;rsquo;::timestamp 에서 앞부분 토큰에 날짜는 이미 발견되었기에, 6자리 숫자가 시간으로 해석된다. (2024년 05월 02일의 12:34:22)- 세 자리 이상의 숫자이고 아직 날짜 필드가 발견되지 않았다면, 연도로 해석 (기본적으로 yy-mm-dd 순서이며 서버의 DateStyle 설정에 따라 mm-dd-yy, dd-mm-yy 등으로 변경할 수 있다.&lt;code&gt;SELECT &#39;240522&#39;::TIMESTAMP&lt;/code&gt;![](/images/posts/88/스크린샷 2024-05-02 오후 5.45.27.png)&amp;lsquo;240522&amp;rsquo;::timestamp는 처음 발견된 날짜형 토큰이기에 기본 설정인 yy-mm-dd에 따라 해석된다. (2024년 05월 02일)- 월/일 필드가 범위를 벗어나거나 유효하지 않은 값이라면 오류 발생### 2-4. BC(기원전) 설정- bc 문자열을 통해 기원전 설정이 가능하며, BC(기원전)이 설정되어 있다면, 내부적으로는 연도를 음수로 바꾸고 1을 더한 후 저장한다. (그레고리력에는 연도 0이 없으므로 수치상 1 BC는 연도 0이 됨)&lt;code&gt;SELECT &#39;bc 1200201&#39;::DATE&lt;/code&gt;![](/images/posts/88/스크린샷 2024-05-02 오후 5.46.24.png)- BC가 지정되지 않고 연도 필드가 두 자리 숫자인 경우, 연도를 4자리로 조정한다. 해당 필드가 70보다 작으면 2000을 더하고, 그렇지 않으면 1900을 더한다.- &amp;lsquo;800502&amp;rsquo;::date - 년도필드(80)가 70보다 크기에 1900을 더한 &amp;lsquo;1980년 05월 02일&amp;rsquo;을 의미- &amp;lsquo;240502&amp;rsquo;::date - 년도필드(24)가 70보다 작기에 2000을 더한 &amp;lsquo;2024년 05월 02일&amp;rsquo;을 의미&lt;code&gt;SELECT &#39;800502&#39;::DATE ;&lt;/code&gt;![](/images/posts/88/스크린샷 2024-05-02 오후 5.47.10.png)&lt;code&gt;SELECT &#39;240502&#39;::DATE ;&lt;/code&gt;![](/images/posts/88/스크린샷 2024-05-02 오후 5.47.27.png)참고&lt;a href=&#34;https://www.postgresql.org/docs/16/datetime-input-rules.html&#34;&gt;https://www.postgresql.org/docs/16/datetime-input-rules.html&lt;/a&gt;#시간 #날짜 #PostgreSQL&lt;/p&gt;</description>
    </item>
    <item>
      <title>[PostgreSQL] 시퀀스(Sequence)의 개념과 사용법(생성, 삭제, 조회 등)</title>
      <link>http://localhost:1313/posts/23/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/23/</guid>
      <description>&lt;h3 id=&#34;1-1-생성-삭제-조회---101부터-시작하는-기본-시퀀스-생성create-sequence-serial-start-101---시퀀스-다음값-조회select-nextvalserial---시퀀스-현재값-조회select-currvalserial---시퀀스-삭제drop-sequence-serial---시퀀스로-insert하기insert-into-distributors-values-nextvalserial-nothing---copy-from-후에-시퀀스-시작값-변경하기begincopy-distributors-from-input_fileselect-setvalserial-maxid-from-distributorsend---synopsiscreate--temporary--temp--sequence--if-not-exists--이름-as-자료형--increment--by--증가값--minvalue-최소값--no-minvalue---maxvalue-최대값--no-maxvalue--start--with--시작값---cache-캐시----no--cycle--owned-by--테이블이름칼럼이름--none-------------1-2-사용-중인-시퀀스-확인select-nnspname-as-sequence_schemacrelname-as-sequence_nameuusename-as-ownerfrom-pg_class-cjoin-pg_namespace-n-on-noid--crelnamespacejoin-pg_user-u-on-uusesysid--crelownerwhere-crelkind--sand-uusename--current_user-2-시퀀스-생성시-상세-옵션--temporary-or-temp현재-세션에서만-사용하는-시퀀스를-생성하며-세션이-종료되면-시퀀스는-자동-삭제된다--if-not-exists동일명의-시퀀스가-있다면-알림만-보여주고-작업은-생략한다--as-자료형시퀀스의-자료형을-설정한다-smallint-integer-and-bigint-세-종류로-bigint형이-기본값이다--increment-by-증가값시퀀스-채번식-증가값을-더하여-구한다-양수라면-증가-시퀀스-음수면-감소시퀀스이다-default는-1이다--minvalue--no-minvalue해당-시퀀스의-최솟값을-설정한다-기본-값은-1이며-감소-시퀀스의-경우-해당-자료형의-최솟값이다--maxvalue--no-maxvalue해당-시퀀스의-최댓값을-설정한다-기본값은-해당-자료형의-최댓값이다-감소-시퀀스의-경우--1이다--start-with시퀀스의-시작값을-설정한다-기본값은-증가시퀀스의-경우-최솟값이며-감소-시퀀스는-최댓값이다--cache시퀀스-채번을-빠르게-하기-위해-메모리에서만-처리하는-캐시값이다기본값은-1이며-캐시를-사용하지-않고-매번-디스크를-사용함을-뜻한다--cycle--no-cycle시퀀스가-최댓값최솟값에-도달했을-때-순환하며-다시-시작한다no-cycle의-경우-최솟값최댓값에-도달하면-에러로-처리한다기본-설정은-no-cycle이다--owned-by--owned-by-none해당-칼럼과-시퀀스의-의존관계를-생성한다테이블칼럼이-삭제되면-시퀀스는-자동으로-삭제되며-테이블시퀀스의-소유자가-같아야-한다owned-by-none이-기본-옵션이며-어떠한-의존관계도-없는-상태이다-3-시퀀스의-개념create-sequence는-일련번호-생성기인-sequence를-생성한다-시퀀스를-생성하면-내부적으로-지정한-명칭으로-단일-로우의-특수-테이블을-만들고-그-로우의-값을-초기화한다-시퀀스는-특수-용도의-테이블-이기-때문에-다음과-같은-쿼리를-사용할-수-있지만select--from-seq_nameimagesposts23스크린샷-2023-10-11-오후-12847png이-테이블을-직접-조작하면-안되며-결과에서-last_value-nextval은-실행-시점의-최신-값이다-그-후로는-다른-세션에서-호출되어-바뀌었을-수도-있는-값이다-4-시퀀스의-특징--시퀀스명은-시퀀스--테이블--인덱스--뷰-명과-겹칠-수-없다--시퀀스는-기본적으로-bigint형으로-계산한다--9223372036854775808--9223372036854775807--nextval-setval은-취소가-되지-않는다-때문에-연속되지-않는-일련번호를-처리하는-용도로-사용할-수-없고-락을-통해-구현은-가능하지만-시퀀스를-사용하지-않는-것보다-더-높은-코스트가-소모된다-특히-동시-접속-많은-서비스라면-더-비효율적이다-5-cache-옵션-----------5-1-cache-옵션이란cache-옵션을-사용하는-경우-다중-세션에서-시퀀스가-순차적으로-채번-되지-않는다-각세션-별로-캐시값만큼-생략-후-그다음-세션시작번호를-채번-한다-last_value-캐시-1-즉-시퀀스-캐시는-한-세션-내의-캐시를-의미한다-이를-통해-다른-세션에서-이전-세션에서-미사용-한-일련번호를-사용할-수-없도록-하고-그렇기-때문에-시퀀스가-연속적이지-않는-경우가-종종-발생한다-----------5-2-cache-옵션-사용시-주의-사항캐시를-사용하면-시퀀스가-크다는-의미가-꼭-나중에-생성된-데이터라는-보장이-없다-그렇기에-순차적-시퀀스를-사용해야-하는-경우라면-캐시값을-1로-설정해야-만한다-예를-들어-캐시값은-10으로-설정할-경우--a-세션이-110-시퀀스를-선점--b-세션이-1120-시퀀스를-선점이-상태에서-b세션에서-더-빨리-시퀀스를-호출하더라도-a세션의-110-시퀀스보다-낮은-값을-가질-수-없기-때문이다cache를-사용하더라도-유니크한-시퀀스를-채번함에는-전혀-지장이-없기에-순차적-의미로써-시퀀스를-사용할-것이-아니라면-사용하여도-무관하다참고httpswwwpostgresqlkrdocs10sql-createsequencehtmlhttpswwwpostgresqlkrdocs10sql-createsequencehtmlsequence-postgresql&#34;&gt;1-1. 생성, 삭제, 조회&lt;code&gt;-- 101부터 시작하는 기본 시퀀스 생성CREATE SEQUENCE serial START 101;-- 시퀀스 다음값 조회SELECT nextval(&#39;serial&#39;);-- 시퀀스 현재값 조회select currval(&#39;serial&#39;);-- 시퀀스 삭제DROP SEQUENCE serial;-- 시퀀스로 INSERT하기INSERT INTO distributors VALUES (nextval(&#39;serial&#39;), &#39;nothing&#39;);-- COPY FROM 후에 시퀀스 시작값 변경하기BEGIN;COPY distributors FROM &#39;input_file&#39;;SELECT setval(&#39;serial&#39;, max(id)) FROM distributors;END;-- SynopsisCREATE [ TEMPORARY | TEMP ] SEQUENCE [ IF NOT EXISTS ] 이름[ AS 자료형 ][ INCREMENT [ BY ] 증가값 ][ MINVALUE 최소값 | NO MINVALUE ] [ MAXVALUE 최대값 | NO MAXVALUE ][ START [ WITH ] 시작값 ] [ CACHE 캐시 ] [ [ NO ] CYCLE ][ OWNED BY { 테이블이름.칼럼이름 | NONE } ]&lt;/code&gt;###           1-2. 사용 중인 시퀀스 확인&lt;code&gt;select n.nspname as sequence_schema,c.relname as sequence_name,u.usename as ownerfrom pg_class cjoin pg_namespace n on n.oid = c.relnamespacejoin pg_user u on u.usesysid = c.relownerwhere c.relkind = &#39;S&#39;and u.usename = current_user;&lt;/code&gt;## 2. 시퀀스 생성시 상세 옵션- TEMPORARY or TEMP현재 세션에서만 사용하는 시퀀스를 생성하며, 세션이 종료되면 시퀀스는 자동 삭제된다.- IF NOT EXISTS동일명의 시퀀스가 있다면 알림만 보여주고 작업은 생략한다.- AS 자료형시퀀스의 자료형을 설정한다. smallint, integer, and bigint 세 종류로 bigint형이 기본값이다.- INCREMENT BY 증가값시퀀스 채번식 증가값을 더하여 구한다. 양수라면 증가 시퀀스, 음수면 감소시퀀스이다. default는 1이다.- MINVALUE / NO MINVALUE해당 시퀀스의 최솟값을 설정한다. 기본 값은 1이며, 감소 시퀀스의 경우 해당 자료형의 최솟값이다.- MAXVALUE / NO MAXVALUE해당 시퀀스의 최댓값을 설정한다. 기본값은 해당 자료형의 최댓값이다. 감소 시퀀스의 경우 -1이다.- START WITH시퀀스의 시작값을 설정한다. 기본값은 증가시퀀스의 경우 최솟값이며, 감소 시퀀스는 최댓값이다.- CACHE시퀀스 채번을 빠르게 하기 위해 메모리에서만 처리하는 캐시값이다.기본값은 1이며 캐시를 사용하지 않고 매번 디스크를 사용함을 뜻한다.- CYCLE / NO CYCLE시퀀스가 최댓값/최솟값에 도달했을 때 순환하며 다시 시작한다.NO CYCLE의 경우 최솟값/최댓값에 도달하면 에러로 처리한다.기본 설정은 NO CYCLE이다.- OWNED BY / OWNED BY NONE해당 칼럼과 시퀀스의 의존관계를 생성한다.테이블/칼럼이 삭제되면 시퀀스는 자동으로 삭제되며, 테이블/시퀀스의 소유자가 같아야 한다.OWNED BY NONE이 기본 옵션이며 어떠한 의존관계도 없는 상태이다.## 3. 시퀀스의 개념CREATE SEQUENCE는 일련번호 생성기인 SEQUENCE를 생성한다. 시퀀스를 생성하면, 내부적으로 지정한 명칭으로 단일 로우의 특수 테이블을 만들고, 그 로우의 값을 초기화한다. 시퀀스는 특수 용도의 &amp;ldquo;테이블&amp;rdquo; 이기 때문에 다음과 같은 쿼리를 사용할 수 있지만,&lt;code&gt;SELECT * FROM seq_name;&lt;/code&gt;![](/images/posts/23/스크린샷 2023-10-11 오후 1.28.47.png)이 테이블을 직접 조작하면 안되며, 결과에서 last_value (nextval)은 &amp;ldquo;실행 시점&amp;quot;의 최신 값이다. (그 후로는 다른 세션에서 호출되어 바뀌었을 수도 있는 값이다.)## 4. 시퀀스의 특징- 시퀀스명은 시퀀스 / 테이블 / 인덱스 / 뷰 명과 겹칠 수 없다.- 시퀀스는 기본적으로 bigint형으로 계산한다. (-9223372036854775808 ~ 9223372036854775807)- nextval, setval은 취소가 되지 않는다. 때문에 연속되지 않는 일련번호를 처리하는 용도로 사용할 수 없고, 락을 통해 구현은 가능하지만 시퀀스를 사용하지 않는 것보다 더 높은 코스트가 소모된다. 특히 동시 접속 많은 서비스라면 더 비효율적이다.## 5. CACHE 옵션###           5-1. CACHE 옵션이란?CACHE 옵션을 사용하는 경우 다중 세션에서 시퀀스가 순차적으로 채번 되지 않는다. 각세션 별로 캐시값만큼 생략 후 그다음 세션시작번호를 채번 한다. (last_Value +캐시-1) 즉 시퀀스 캐시는 한 세션 내의 캐시를 의미한다. 이를 통해 다른 세션에서 이전 세션에서 미사용 한 일련번호를 사용할 수 없도록 하고, 그렇기 때문에 시퀀스가 연속적이지 않는 경우가 종종 발생한다.###           5-2. CACHE 옵션 사용시 주의 사항캐시를 사용하면 시퀀스가 크다는 의미가 꼭 나중에 생성된 데이터라는 보장이 없다. 그렇기에 순차적 시퀀스를 사용해야 하는 경우라면 캐시값을 1로 설정해야 만한다. 예를 들어&amp;gt; 캐시값은 10으로 설정할 경우- A 세션이 1&lt;del&gt;10 시퀀스를 선점- B 세션이 11&lt;/del&gt;20 시퀀스를 선점이 상태에서 B세션에서 더 빨리 시퀀스를 호출하더라도 A세션의 1~10 시퀀스보다 낮은 값을 가질 수 없기 때문이다.(CACHE를 사용하더라도 유니크한 시퀀스를 채번함에는 전혀 지장이 없기에, 순차적 의미로써 시퀀스를 사용할 것이 아니라면 사용하여도 무관하다.)참고&lt;a href=&#34;https://www.postgresql.kr/docs/10/sql-createsequence.html&#34;&gt;https://www.postgresql.kr/docs/10/sql-createsequence.html&lt;/a&gt;#Sequence #PostgreSQL&lt;/h3&gt;</description>
    </item>
    <item>
      <title>[PostgreSQL] 외래키(Foreign Keys) 개념, 사용법, 장단점, 적용검토</title>
      <link>http://localhost:1313/posts/64/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/64/</guid>
      <description>&lt;p&gt;Foreign key constraint 외래키 제약은 특정 칼럼 혹은 칼럼들의 값이 다른 테이블의 특정 row와 매칭되어야 하는 제약조건이다. 이를 두 관련 테이블 사이의 참조 무결성 (referential integrity)를 유지한다고 말한다. 그렇게 복잡한 개념은 아니니 바로 사용법을 확인해 보도록 하자## 2. 예제### 2-1. 기본 외래키(Foreign Keys) 생성products 테이블은 물품의 이름, 가격 정보 테이블이고, orders 테이블은 존재하는 물품 각각에 대한 순서 정보가 들어있는 테이블이다. orders, products 테이블의 product_no에 외래키 제약을 적용하는 예제이다.&lt;code&gt;CREATE TABLE products (product_no integer PRIMARY KEY,name text,price numeric);CREATE TABLE orders (order_id integer PRIMARY KEY,product_no integer REFERENCES products (product_no),quantity integer);&lt;/code&gt;orders 테이블의 제약조건을 위와 같이 주었을 때 products 테이블에 없는 product_no로는 데이터 생성이 불가능하다. 이 경우 다음과 같이 명칭 한다.- orders - referencing(참조하는) 테이블- products - referenced(참조된) 테이블### 2-2. 칼럼을 지정하지 않은 외래키(Foreign Keys)&lt;code&gt;CREATE TABLE orders (order_id integer PRIMARY KEY,product_no integer REFERENCES products,quantity integer);&lt;/code&gt;특정 칼럼을 지정하지 않는다면 reference 칼럼으로 Primary Key에 해당하는 칼럼을 자동으로 사용하기에 별도로 칼럼명을 명시하지 않아도 된다. (PK가 바뀔 일은 거의 없겠지만) 테이블 구조가 바뀔 수 있고, 명확한 제약조건을 명시하는 것이 좋기에 칼럼을 지정하는 것이 좋다.### 2-3. 복합 칼럼 외래키(Foreign Keys)FK 제약조건을 여러 개의 칼럼을 대상으로도 사용할 수 있다.&lt;code&gt;CREATE TABLE t1 (a integer PRIMARY KEY,b integer,c integer,FOREIGN KEY (b, c) REFERENCES other_table (c1, c2));&lt;/code&gt;물론 참조하는 칼럼과 참조되는 테이블의 칼럼 수는 일치하여야 한다.### 2-4. 자기 참조 외래키(Self-referential Foreign Keys)종종 다른 테이블이 아니라 같은 테이블 내의 칼럼의 FK로 두는 것이 효율적일 때가 있다. 이를 자기 참조 외래키 (self-referential foreign key)라고 한다. 예를 들어 트리 구조의 노드들을 테이블 row로 표현하고 싶을 때, 다음과 같이 parent_id를 node_id에 참조시키면 된다.&lt;code&gt;CREATE TABLE tree (node_id integer PRIMARY KEY,parent_id integer REFERENCES tree,name text,...);&lt;/code&gt;최상위 노드의 parent_id는 null이 될 것이고, parent_id가 null이 아닌 항목들은 해당 테이블의 유효한 노드를 참조하도록 제한된다.### 2-5. 다중 외래키(Foreign Keys)한 개의 테이블은 여러 개의 FK를 가질 수 있으며 이는 다대다 (many-to-many) 테이블 관계 구현에 사용된다. 기존 예제의 상품, 상품순서 구조에 추가로, 한 순서에 많은 상품을 포함할 수 있게 한다면 다음과 같은 테이블 구조를 사용할 수 있을 것이다.&lt;code&gt;CREATE TABLE products (product_no integer PRIMARY KEY,name text,price numeric);CREATE TABLE orders (order_id integer PRIMARY KEY,shipping_address text,...);CREATE TABLE order_items (product_no integer REFERENCES products,order_id integer REFERENCES orders,quantity integer,PRIMARY KEY (product_no, order_id));​&lt;/code&gt;## 3. 외래키(Foreign Keys) 옵션앞서 말한 대로, FK 제약조건이 걸려있다면 참조되지 않은 값으로는 데이터 생성이 불가능하다. 그러나 참조된 orders가 생성된 후에 product가 삭제되면 어떻게 될까? Postgresql에서는 다음 상황들 중에 선택적으로 사용이 가능하다.- 참조하는 데이터(orders)가 있을 경우 삭제 불가- 참조하는 데이터(orders)까지 함께 삭제- 등등&amp;hellip;데이터를 처리하는 상황을 선택하는 ON DELETE, ON UPDATE 등의 옵션과, 처리하는 방식을 선택하는 RESTRICT, CASCADE 등의 옵션을 조합하여 원하는 결과를 만들면 된다.다음 예제는 서로 다른 테이블에 참조된 칼럼 각각의 값이 삭제됐을 때 참조된 상위 값이 있을 때 각각 같이 삭제할지, 삭제를 방지할지를 설정한 테이블 생성 쿼리이다.&lt;code&gt;CREATE TABLE order_items (product_no integer REFERENCES products ON DELETE RESTRICT,order_id integer REFERENCES orders ON DELETE CASCADE,quantity integer,PRIMARY KEY (product_no, order_id));&lt;/code&gt;다음과 같이 설정하면 order_items에서 product_no를 삭제하려 할 때 참조하는 데이터가 있을 경우 삭제가 불가능하고, order_id의 경우 참조하는 데이터와 함께 삭제가 된다. (RESTRICT, CASCADE는 가장 기본적으로 사용되는 옵션)### 3-1. RESTRICTRestrict는 참조하는 열의 삭제를 방지한다. 참조하는 오브젝트가 존재할 시 실행 자체를 실패한다.### 3-2. NO ACTIONNO ACTION은 제약조건을 선택할 때 참조 행이 존재한다면 오류가 발생하고, 지정하지 않을 시에는 기본 동작(RESTRICT / CASCADE)이 된다. NO ACTION을 선택하나, NO ACTION을 선택하지 않고 RESTRICT를 설정하나 실행이 되지 않는 건 똑같지만, 본질적으로 NO ACTION은 무결성 체크하는 시점을 트랜잭션의 후반부까지 연기할 수 있다는 차이점이 있다. (DEFERRABLE, NOT DEFERRABLE 옵션으로 제어 가능하다.)### 3-3. CASCADE폭포 혹은 단계적인 이라는 의미로, 연관된 데이터의 일괄적인 적용을 의미한다.  CASCADE는 참조하는 열에도 함께 변경을 가한다.두 가지 옵션을 보면- SET NULL- SET DEFAULT이 옵션은 참조하는 테이블의 칼럼값(order_id)을 null로 변경할지, default 값으로 변경할지 선택하는 옵션이다.SET NULL, SET DEFAULT는 FK가 추가적인 정보를 나타낼 때 적절하다. 예를 들어, 위 예제에서 products 테이블에서 product manager의 정보가 참조되고 있고, product manager가 삭제될 때 product에 해당 참조 데이터를 null 혹은 default로 자동으로 바꿔준다면 별도의 추가 명령 없이 관리할 수 있을 것이다.하지만 SET NULL, SET DEFAULT가 참조 테이블의 다른 제약조건들까지 먼저 확인하지는 않기에, 만약 SET DEFAULT로 설정했는데 DEFAULT 값이 다른 제약조건에 부합하지 않을 경우 해당 동작은 실패한다.&lt;code&gt;CREATE TABLE tenants (tenant_id integer PRIMARY KEY);CREATE TABLE users (tenant_id integer REFERENCES tenants ON DELETE CASCADE,user_id integer NOT NULL,PRIMARY KEY (tenant_id, user_id));CREATE TABLE posts (tenant_id integer REFERENCES tenants ON DELETE CASCADE,post_id integer NOT NULL,author_id integer,PRIMARY KEY (tenant_id, post_id),FOREIGN KEY (tenant_id, author_id) REFERENCES users ON DELETE SET NULL (author_id));&lt;/code&gt;위 예제 테이블을 보면 posts의 데이터를 삭제할 때,  FK 제약조건으로 참조된 tenant_id를 null로 변경하려 하지만, tenant_id는 PK의 일부로 null이 될 수 없기에 실행되지 않는다.### 3-4. ON DELETEON DELETE 옵션은 테이블에 연관된 오브젝트의 유형에 따라 적절하게 사용되어야 한다. 참조하는 테이블이 참조된 테이블 값의 구성요소이며 독립적으로 존재할 수 없다면 CASCADE 옵션을 적용하여 한번에 처리하는 것이 적절하고, 두 테이블의 오브젝트가 독립적인 관계라면 RESTRICT, NO ACTION이 적합하다.예를 들어, 위의 예제에서 order_items는 orders의 일부분이고 독립적으로 사용될 일이 없기에 orders가 삭제될 때 자동으로 지워지는 것이 편할 것이다. orders와 products는 독립적으로 사용할 여지가 있기에 삭제 시 products를 자동으로 지우는 건 문제가 될 수 있다.### 3-5. ON UPDATEON DELETE와 유사하게 참조열이 업데이트될 때 호출되는 ON UPDATE도 있다. SET NULL, SET DEFAULT의 설정이 다른 제약조건에 위배된다면 적용할 수 없다는 점은 동일하다. CASCADE와 사용 시 참조하는 칼럼의 업데이트된 값이 참조된 열로 복사된다.## 4. 인덱스FK 제약조건은 PK이거나, Unique 제약조건(UK)이거나, &amp;ldquo;복합 인덱스의 일부가 아닌&amp;rdquo; 칼럼을 참조해야만 한다. 이 뜻은 참조된 칼럼은 인덱스를 항상 가지고 있다는 뜻이다. 하지만 참조하는 칼럼은 인덱스가 필수이거나 인덱스를 자동으로 생성하지 않는다. 열의 DELETE, UPDATE는 참조하는 테이블에서 이전 값을 찾아야 하기에, 참조하는 칼럼에도 인덱스를 설정하는 것이 유리한 경우도 있다.## 5. 장단점### 5-1. 장점- 데이터의 무결성- 쉬운 데이터 구조/관계 확인- update, delete 등의 로직 간소화### 5-2. 단점- 참조 테이블들을 스캔하는데 드는 추가 코스트- 테이블 구조 변경 시 고려해야 할 사항 증가- 데이터 변경이 찾을 때 특히 코스트가 증가- 테스트 데이터 생성 및 데이터 강제 보정이 번거로움## 6. 적용 검토이미 FK를 사용하는 테이블도 다수 존재한다. 중요 기준 데이터의 무결성을 보장하는 데는 효율적인 기능이라고 생각한다. 다만 &amp;ldquo;실시간으로 업데이트되는 대용량 데이터의 경우에도 무결성 보장을 위해 FK를 적용해야 하는가?&amp;ldquo;에 대한 판단을 위해 FK에 대한 내용을 좀 깊게 들여다보았다. 일단 결론은, 실시간으로 변동되는 대용량 데이터 베이스, 특히 타 시스템과의 동기화가 이루어지고 있는 산군의 데이터베이스에는 적합하지 않다고 판단된다.&amp;ldquo;FK의 스캔 코스트가 성능에 큰 영향이 없다&amp;rdquo;, 혹은 &amp;ldquo;적절한 인덱싱으로 스캔 코스트를 관리할 수 있다.&amp;ldquo;라는 상황 자체가 참조 테이블의 인덱스를 전제한다. FK만을 위한 인덱스를 참조된 테이블에 추가하거나, 참조된 테이블에 업데이트나 삭제가 발생하여 참조하는 테이블에서 이전 값을 조회하기 위해 역스캔하는 경우의 코스트를 줄이기 위해 인덱스를 추가해야 하는 상황이 발생한다.현재 운영 중인 데이터베이스는 최소 수백만 건에서 수천만건의 테이블이 서로 연결되어 있으며 read/write 간 동기화, ElasticSearch 등으로의 동기화를 고려하여 최적의 인덱스로 세팅, 되어있고 실시간 모니터링을 통한 튜닝이 지속적으로 진행 중이다. 모든 칼럼에 인덱스를 거는 것이 효율적이지 않은 것처럼, 한정된 자원으로 최적의 인덱싱을 찾아야 하고, 데이터의 변경에 민감하게 대응해야 하는 상황에서는 FK를 설정하는 것이 효율적이지 못하다는 판단이다.또한, 참조되는 테이블의 추가 인덱스 설정 없이 FK를 설정하는 것만으로는 코스트가 증가하지 않지만 실시간으로 몇천 건, 많게는 몇만 건의 데이터가 업데이트 및 동기화되는 상황에서 테이블의 Lock이 여러 테이블로 전파될 위험성도 무시할 수 없다.참고 : &lt;a href=&#34;https://www.postgresql.org/docs/16/ddl-constraints.html#DDL-CONSTRAINTS-FK&#34;&gt;https://www.postgresql.org/docs/16/ddl-constraints.html#DDL-CONSTRAINTS-FK&lt;/a&gt;#fk #PostgreSQL #Foreign Keys&lt;/p&gt;</description>
    </item>
    <item>
      <title>[PostgreSQL] 윈도우 함수(Window Functions)의 개념, 성능 및 사용법 (over, sum/rank/ntitle/cume_dist 등...)</title>
      <link>http://localhost:1313/posts/40/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/40/</guid>
      <description>&lt;p&gt;윈도우 함수는 행과 행 간의 관계를 쉽게 정의하기 위해 만든 함수이다. 이 기능은 일반 집계함수의 연산과 유사하지만, 일반 집계함수가 행 각각을 단일 그룹화해서 출력하는 반면에, 윈도우 함수는 각각의 행들이 그룹화되지 않으며 별도의 ID를 가진다. 그렇기에 윈도우 함수는 현재 row의 정보보다 더 많은 정보에 접근이 가능하다. 예를 들면 다음과 같다.&amp;gt; 일반집계함수 : COUNT() + GROUP BY-&amp;gt; 그룹별 1개의 행 출력 (그룹 개수만큼 출력, 자르기 + 집약)윈도우집계함수 : COUNT() OVER (PARTITION BY) -&amp;gt; ID개수만큼 행 출력 (행의 개수가 줄어들지 않는다, 자르기)다음의 공식문서 예제를 보며 윈도우 함수가 어떻게 작동하는지 알아보자. 임직원의 월급, 부서, 직원번호가 포함된 empsalary 테이블이 있다.&lt;code&gt;SELECT depname, empno, salary, avg(salary) OVER (PARTITION BY depname) FROM empsalary;depname  | empno | salary |          avg-----------+-------+--------+-----------------------develop   |    11 |   5200 | 5020.0000000000000000develop   |     7 |   4200 | 5020.0000000000000000develop   |     9 |   4500 | 5020.0000000000000000develop   |     8 |   6000 | 5020.0000000000000000develop   |    10 |   5200 | 5020.0000000000000000personnel |     5 |   3500 | 3700.0000000000000000personnel |     2 |   3900 | 3700.0000000000000000sales     |     3 |   4800 | 4866.6666666666666667sales     |     1 |   5000 | 4866.6666666666666667sales     |     4 |   4800 | 4866.6666666666666667(10 rows)&lt;/code&gt;첫 3개의 컬럼은 테이블의 데이터를 바로 사용하는 것이고, row 당 1개의 값을 가지고 있다. 4번째 컬럼은 같은 부서명의 ROW 끼리의 평균 월급을 나타낸다. (비윈도우 함수의 avg 함수와 동일하지만, over 구문을 사용할 경우 윈도우 함수로 취급받고, window frame 상에서 연산될 수 있게 해 준다.)윈도우 함수는 함수명, 혹은 변수 뒤에 항상 over를 바로 뒤에 붙여 사용한다. over 구문은 쿼리의 row들이 윈도우 함수에 의해 정확히 어떻게 분할되어 작동하는지에 대한 결정을 내린다. over 내의 partition by 구분은 동일한 값을 공유하는 groups 혹은 partitions으로 행을 분할한다. 이렇게 분할된 파티션 상에서 각 행과 동일한 파티션에 속하는 행끼리 연산하게 된다. over 내에 order by를 통해 윈도우 함수에 통과시킬 row의 순서를 정할 수 있다.&lt;code&gt;SELECT depname, empno, salary,rank() OVER (PARTITION BY depname ORDER BY salary DESC)FROM empsalary;depname  | empno | salary | rank-----------+-------+--------+------develop   |     8 |   6000 |    1develop   |    10 |   5200 |    2develop   |    11 |   5200 |    2develop   |     9 |   4500 |    4develop   |     7 |   4200 |    5personnel |     2 |   3900 |    1personnel |     5 |   3500 |    2sales     |     1 |   5000 |    1sales     |     4 |   4800 |    2sales     |     3 |   4800 |    2(10 rows)&lt;/code&gt;rank 함수는 해당 파티션 당 order by 값에 맞는 숫자 형태의 순위를 나타낸다. rank는 over 절에 의해서만 결정되기에 명시적인 매개 변수가 추가로 필요하지 않다.윈도우 함수는 from 절의 테이블에서 where, group by 그리고 having 절로 필터링된 &amp;ldquo;가상 테이블&amp;quot;의 행을 대상으로 작동하기에 조건에 부합하지 않아 제거된 row는 윈도우 함수 내에서 사용되지 않는다. 쿼리에 다양한 over 절을 사용하여 데이터를 분할할 수 있지만, 이 가상 테이블에 정의된 row를 대상으로 동일하게 작동한다. 행의 순서가 중요하지 않은 경우, order by를 생략해도 되는 것처럼, 단일 파티션이 전체 row를 포함하는 경우 partition by를 생략할 수도 있다.### 1-1. Window frame윈도우 함수에 관한 중요한 개념 중 하나는 window frame이다. window frame이라고 불리는 row의 집합이 파티션 내에 존재한다. 몇몇 윈도우 함수는 전체 파티션이 아닌, window frame의 row에 대해서만 동작한다. 기본적으로 ORDER BY를 사용하면 frame은 시작 행부터 현재 행까지의 정보로만 구성되며, order by 가 생략되면, 기본 frame은 파티션 내의 전체 row로 이루어진다. 다음 sum의 예제를 보면&lt;code&gt;SELECT salary, sum(salary) OVER () FROM empsalary;salary |  sum--------+-------5200 | 471005000 | 471003500 | 471004800 | 471003900 | 471004200 | 471004500 | 471004800 | 471006000 | 471005200 | 47100(10 rows)&lt;/code&gt;over 절에 order by가 없기에, window frame은 파티션 전체와 같고, 각 sum은 전체 테이블을 조회하여 일반 집계 함수와 동일한 결과를 가진다. 하지만 order by 가 들어갈 경우 결과가 달라진다. 아래 쿼리는 월급의 최저값 ROW부터 현재 ROW까지 (파티션의)의 합계이다.&lt;code&gt;SELECT salary, sum(salary) OVER (ORDER BY salary) FROM empsalary;salary |  sum--------+-------3500 |  35003900 |  74004200 | 116004500 | 161004800 | 257004800 | 257005000 | 307005200 | 411005200 | 411006000 | 47100(10 rows)&lt;/code&gt;### 1-2. 제약조건위도우 함수는 SELECT와 ORDER BY 절에서만 허용된다. group by, having, where 절 같은 곳에서는 사용이 불가능하다.논리적으로 해당 조건들을 모두 조회한 후에 작동하기 때문이다.그리고 윈도우 함수는 비윈도우집계함수 이후에 실행된다. 즉 윈도우 함수의 인수에 일반 집합 함수 호출을 포함하는 것은 가능하지만, 그 반대의 경우는 불가능하다. 만약 윈도우 함수의 연산 후에 filter 혹은 group by를 할 경우, 서브쿼리를 사용해야 한다. 아래와 같이 사용하면 내부 쿼리의 순위가 3 이하인 row 들만 보여준다.&lt;code&gt;SELECT depname, empno, salary, enroll_dateFROM(SELECT depname, empno, salary, enroll_date,rank() OVER (PARTITION BY depname ORDER BY salary DESC, empno) AS posFROM empsalary) AS ssWHERE pos### 1-3. WINDOW AS쿼리가 만약에 다수의 윈도우 함수를 포함한다면, 각각이 OVER문으로 작성하는 것이 가능하지만, 여러 함수에 대해 동일한 윈도우 설정 동작을 하는 경우 중복되고 에러가 발생하기 쉽다. 이럴 경우 WINDOW에 해당하는 레퍼런스를 설정하고 해당 값을 over에서 사용 이 가능하다.&lt;/code&gt;SELECT sum(salary) OVER w, avg(salary) OVER wFROM empsalaryWINDOW w AS (PARTITION BY depname ORDER BY salary DESC);&lt;code&gt;### 1-4. 성능윈도우 함수를 사용할 경우 집계, 순위 등의 쿼리를 편하게 사용할 수 있고, 테이블의 스캔 횟수도 훨씬 줄어든다. 다만 파티션 내 다른 행과 현재행의 관계정보로 다루어지기에, 윈도우 함수를 사용할 시 기본적으로 정렬하는 과정에서 자원이 소모된다. 테이블 및 데이터 정보에 따라 달라지겠지만, 분포율이 5~7%정도 되는 1200만 건의 데이터를 기준으로 윈도우 함수와 group by 정렬을 비교해 보았다.![](/images/posts/40/img_1.png)![](/images/posts/40/스크린샷 2023-10-31 오후 1.51.46.png)실제로 윈도우 함수를 포함한 경우 sort 과정에 자원이 많이 소모되어 데이터가 많을 경우 오히려 비윈도우 함수보다 효율이 좋지 않았다. 따라서 기능의 편의성 외에도 데이터의 양이나 테이블 구조에 맞춰 윈도우 함수를 사용하고, 서브쿼리나 조건절 튜닝을 통해 스캔해야할 행의 갯수를 줄인 후 사용하는 방법을 고려해야 한다.## 2. 윈도우 함수의 종류 및 사용법### 2-1. 일반집계함수- SUM - 파티션별 윈도우의 합계&lt;/code&gt;SELECT MGR, ENAME, SAL, SUM(SAL) OVER (PARTITION BY MGR ORDER BY SAL RANGE UNBOUNDED PRECEDING) as MGR_SUMFROM EMP;&lt;code&gt;- MAX - 파티션별 윈도우의 최댓값&lt;/code&gt;SELECT MGR, ENAME, SAL, MAX(SAL) OVER (PARTITION BY MGR) as MGR_MAXFROM EMP;&lt;code&gt;- MIN - 파티션별 윈도우의 최솟값&lt;/code&gt; SELECT MGR, ENAME, HIREDATE, SAL, MIN(SAL) OVER(PARTITION BY MGR ORDER BY HIREDATE) as MGR_MINFROM EMP;&lt;code&gt;- AVG - 파티션별 윈도우의 평균값&lt;/code&gt;SELECT MGR, ENAME, HIREDATE, SAL, ROUND (AVG(SAL) OVER (PARTITION BY MGR ORDER BY HIREDATE ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)) as MGR_AVGFROM EMP;&lt;code&gt;- COUNT - 파티션별 윈도우의 카운트&lt;/code&gt;SELECT ENAME, SAL, COUNT(*) OVER (ORDER BY SAL RANGE BETWEEN 50 PRECEDING AND 150 FOLLOWING) as SIM_CNTFROM EMP;&lt;code&gt;### 2-2. 그룹 내 행 순서 함수- FIRST_VALUE - 파티션별 윈도우에 가장 먼저 나오는 값&lt;/code&gt;SELECT DEPTNO, ENAME, SAL, FIRST_VALUE(ENAME) OVER (PARTITION BY DEPTNO ORDER BY SAL DESC ROWS UNBOUNDED PRECEDING) as DEPT_RICHFROM EMP;&lt;code&gt;- LAST_VALUE - 파티션별 윈도우에 가장 나중에 나오는 값&lt;/code&gt;SELECT DEPTNO, ENAME, SAL, LAST_VALUE(ENAME) OVER ( PARTITION BY DEPTNO ORDER BY SAL DESC ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) as DEPT_POORFROM EMP;&lt;code&gt;- LAG - 파티션별 윈도우의 이전 몇 번째 행의 값&lt;/code&gt;SELECT ENAME, HIREDATE, SAL, LAG(SAL) OVER (ORDER BY HIREDATE) as PREV_SALFROM EMPWHERE JOB = &amp;lsquo;SALESMAN&amp;rsquo;;&lt;code&gt;- LEAD - 파티션별 윈도우의 이후 몇번째 행의 값&lt;/code&gt;SELECT ENAME, HIREDATE, LEAD(HIREDATE, 1) OVER (ORDER BY HIREDATE) as &amp;ldquo;NEXTHIRED&amp;quot;FROM EMP;&lt;code&gt;### 2-3. 그룹 내 순위함수- RANK - 파티션 내 전체 윈도우에 대한 순위, 동일 값에 대해서는 동일한 순위, 그 다음 값은 순위는 동일한 순위 만큼 증가된 채로 부여 (ex. 1,1,1,4,5,6,7...)&lt;/code&gt;SELECT JOB, ENAME, SAL,RANK( ) OVER (ORDER BY SAL DESC) ALL_RANK,RANK( ) OVER (PARTITION BY JOB ORDER BY SAL DESC) JOB_RANKFROM EMP;&lt;code&gt;- DENSE_RANK - 파티션 내 전체 윈도우에 대한 순위, 동일 값에 대해서는 동일한 순위, 그 다음 값은 순위는 동일한 순위에 상관없이 다음값 부여 (ex. 1,1,1,2,3,4,5...)&lt;/code&gt;SELECT JOB, ENAME, SAL, RANK( ) OVER (ORDER BY SAL DESC) RANK, DENSE_RANK( ) OVER (ORDER BY SAL DESC) DENSE_RANKFROM EMP; &lt;code&gt;- ROW_NUMBER - 파티션 내 전체 윈도우에 대한 순번, 동일한 값이어도 고유한 순위 부여&lt;/code&gt;SELECT JOB, ENAME, SAL, RANK( ) OVER (ORDER BY SAL DESC) RANK, ROW_NUMBER() OVER (ORDER BY SAL DESC) ROW_NUMBERFROM EMP; &lt;code&gt;### 2-4. 그룹 내 비율 함수- RATIO_TO_REPORT - 파티션 내 전체 SUM에 대한 컬럼별 백분율 소수점 값&lt;/code&gt;SELECT ENAME, SAL, ROUND(RATIO_TO_REPORT(SAL) OVER (), 2) as R_RFROM EMPWHERE JOB = &amp;lsquo;SALESMAN&amp;rsquo;; &lt;code&gt;- PERCENT_RANK - 파티션별 윈도우에서 가장 먼저 나오는 것은 0, 제일 마지막에 나오는 것은 1로 나타낸 후 값에 상관없이 행의 순서만으로의 백분율 값&lt;/code&gt;SELECT DEPTNO, ENAME, SAL, PERCENT_RANK() OVER (PARTITION BY DEPTNO ORDER BY SAL DESC) as P_RFROM EMP; &lt;code&gt;- CUME_DIST - 파티션별 윈도우의 전체 건수에서 현재 행보다 작거나 같은 건에 대한 누적 백분률 값&lt;/code&gt;SELECT DEPTNO, ENAME, SAL, CUME_DIST() OVER (PARTITION BY DEPTNO ORDER BY SAL DESC) as CUME_DISTFROM EMP; &lt;code&gt;- NTITLE - 파티션별 전체 건수를 Argument로 N등분한 값&lt;/code&gt;SELECT ENAME, SAL, NTILE(4) OVER (ORDER BY SAL DESC) as QUAR_TILEFROM EMP ;`참고윈도우 함수별 기능 및 예제 - &lt;a href=&#34;http://www.gurubee.net/lecture/2382&#34;&gt;http://www.gurubee.net/lecture/2382&lt;/a&gt;윈도우 함수(WINDOW FUNCTION)제6절 윈도우 함수(WINDOW FUNCTION)WINDOW FUNCTION 종류그룹 내 순위함수.3.1 RANK 함수3.2 DENSE_RANK 함수3.3 ROW_NUMBER 함수일반 집계 함수3.4 ..www.gurubee.net윈도우 함수 공식 문서 - &lt;a href=&#34;https://www.postgresql.org/docs/current/tutorial-window.html&#34;&gt;https://www.postgresql.org/docs/current/tutorial-window.html&lt;/a&gt;#PostgreSQL #Window functions&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Spring] 단위 테스트, JUnit의 개념 및 단위 테스트 코드 작성 방법</title>
      <link>http://localhost:1313/posts/45/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/45/</guid>
      <description>&lt;p&gt;하나의 모듈을 기준으로 독립적으로 진행되는 가장 작은 단위의 테스트이다. 통합 테스트의 경우 시스템을 구성하는 컴포넌트들이 커질수록 테스트 시간이 길어지지만, 단위 테스트의 경우 해당 부분만 독립적으로 테스트하기에 코드의 변경이 있어도 빠르게 문제 여부를 확인할 수 있다. CleanCode 책에 의하면 깨끗한 테스트 코드는 다음 5가지 규칙을 따라야 한다.&amp;gt; Fast - 빠르게 동작하여 자주 돌릴 수 있어야 한다.Independent - 테스트는 독립적이며 서로 의존해서는 안된다.Repeatable -  어느 환경에서도 반복이 가능해야 한다.Self-validating - 테스트는 성공 또는 실패로 결과를 내어 자체 검증되어야 한다.Timely - 테스트는 적시에, 테스트하려는 실제코드를 구현하기 직전에 구현해야 한다.## 2. JunitJunit은 단위 테스트를 지원하는 오픈소스 프레임워크로 다음과 같은 특징을 가진다.- 문자 혹은 GUI 기반으로 실행- @Test 메서드를 호출할 때마다 새 인스턴스 생성- 예상결과를 검증하는 assertion 제공- 자동실행, 자체결과 확인 및 즉각적인 피드백 제공- 테스트 방식을 구분할 수 있는 어노테이션을 제공하며, 어노테이션만으로 간결하게 실행이 가능### ▶ 2-1. 어노테이션 종류&amp;gt; @DisplayName - 테스트 이름 명시@Test - 테스트를 수행할 메서드, Junit은 각 테스트끼리 영향을 주지 않도록 테스트 실행 객체를 매 테스트마다 만들고 종료 시 삭제@BeforeAll - 전체 테스트를 시작하기 전에 1회 실행 (ex. 데이터베이스 연결, 테스트환경 초기화, 전체 테스트 실행주기에 한 번만 호출)@BeforeEach - 테스트 케이스를 시작하기 전마다 실행 (테스트 메서드에 사용하는 객체 초기화, 테스트에 필요한 데이터 삽입 등)@AfterAll - 전체 테스트를 마치고 종료하기 전에 1회 실행 (데이터베이스 연결 종료, 공통으로 사용하는 자원 해제 등)@AfterEach - 테스트 케이스를 종료하기 전마다 실행 (테스트 이후 특정데이터를 삭제 등)### ▶ 2-2. AssertJJunit과 사용해 가독성을 높여주는 라이브러리로 다양한 문법을 지원한다. 기존 Junit은 기댓값과 실제 비교대상이 확실히 보이지 않아 잘 구분이 안되지만 isEqualTo 등 명확한 의미의 매머드로 대체가 가능하다.### ▶ 2-3. given-when-then 패턴요즘 단위테스트의 가장 보편적인 형태로 1개의 단위테스트를 3단계로 나눠서 처리하는 패턴이다.- given = 테스트 실행을 준비하는 단계 (어떤 상황, 데이터가 주어졌을 때)- when = 테스트를 진행하는 단계 (어떤 함수를 실행시키면 )- then = 테스트 결과를 검증하는 단계 (어떤 결과가 기대된다.)## 3. 단위 테스트 예제점수의 평균을 계산해주는 클래스에 대한 단위 테스트를 해보자. 해당 예제는 객체 간의 메시지 교환이 없는 단순한 값 비교, 예외 확인을 위한 테스트 케이스이다.(일반적으로 스프링 애플리케이션은 다양한 객체에서 메시지를 전달받아 의존성이 생기는데, 이럴 경우 Mock(가짜) 객체를 사용하여 테스트가 가능하다.)### ▶ 3-0. 테스트 대상인 평균점수 조회다음과 같이 0점 이상의 점수들에 대한 평균을 구하는 클래스가 있을 때&lt;code&gt;public class AverageScoreCalculator {private static Integer sum = 0;private static Integer count = 0;public void addScore(Integer score) {if (!validateScores(score))	{throw new IllegalStateException(&amp;quot;Invalid score&amp;quot;);}sum += score;count++;}private boolean validateScores(Integer score) {return score &amp;gt; 0;}public Double getAverageScore() {return (double) (sum / count);}}&lt;/code&gt;### ▶ 3-1. 점수의 평균이 일치하는지 테스트실제 평균의 값과, 클래스 연산 결과가 일치하는지 테스트한다.&lt;code&gt;@DisplayName(&amp;quot;점수의 평균 테스트&amp;quot;)@Testvoid averageScoreTest() {//givenAverageScoreCalculator averageScoreCalculator = new AverageScoreCalculator();int[] scores = {10,20,30,40,50};//whenfor (int i = 0; i&amp;lt;scores.length; i++)	{averageScoreCalculator.addScore(scores[i]);}Double averageScore = averageScoreCalculator.getAverageScore();//thenassertThat(averageScore).isEqualTo(Arrays.stream(scores).average().getAsDouble());}&lt;/code&gt;### ▶ 3-2. 평균점수의 범위 테스트점수의 평균이 1~100점 이내에 존재하는지 확인한다.&lt;code&gt;@DisplayName(&amp;quot;평균 점수 범위 테스트&amp;quot;)@Testvoid averageScoreRangeTest()	{//givenAverageScoreCalculator averageScoreCalculator = new AverageScoreCalculator();int[] scores = {10,20,30,40,50};//whenfor (int i = 0; i&amp;lt;scores.length; i++)	{averageScoreCalculator.addScore(scores[i]);}Double averageScore = averageScoreCalculator.getAverageScore();//thenassertThat(averageScore &amp;gt;= 0 &amp;amp;&amp;amp; averageScore &amp;lt;= 100).isTrue();}&lt;/code&gt;### ▶ 3-3. 개별점수 유효성 테스트유효하지 않은 점수가 인풋 될 경우 IllegalStateException이 기대되기에, assertThrow로 Exception을 테스트한다.&lt;code&gt;@DisplayName(&amp;quot;개별 잘못된 점수 테스트&amp;quot;)@Testpublic void averageScoreInvalidScoreTest(){//givenAverageScoreCalculator averageScoreCalculator = new AverageScoreCalculator();int[] scores = {10,20,30,40,-1};//whenfinal IllegalStateException exception = assertThrows(IllegalStateException.class, () -&amp;gt; {for (int i = 0; i&amp;lt;scores.length; i++)	{averageScoreCalculator.addScore(scores[i]);}});//thenassertThat(exception.getMessage()).isEqualTo(&amp;quot;Invalid score&amp;quot;);}&lt;/code&gt;## 4. 주요 Assert 메서드### ▶ 4-1. 주요 비교 검증 테스트 메서드메서드 이름설명isEqualTo(A)A 값과 같은지 검증isNotEqualTo(A)A 값과 다른지 검증contains(A)A 값을 포함하는지 검증doesNotContain(A)A 값을 포함하지 않는지 검증startWith(A)접두사가 A인지 검증endsWith(A)접미사가 A인지 검증isEmpty()비어 있는 값인지 검증isNotEmpty()비어 있지 않은 값인지 검증isPositive()양수인지 검증isNegative()음수인지 검증isGreaterThan(a)a보다 큰 값인지 검증isLessThan(a)a보다 작은 값인지 검증### ▶ 4-2. HTTP 주요 응답코드 테스트 메서드코드매핑 메서드설명200 OKisOk()HTTP 응답코드가 200 OK인지 검증201 CreatedisCreated()HTTP 응답코드가 201 Created 검증400 Bad RequestisBadRequest()HTTP 응답코드가 400 BadRequest검증403 ForbiddenisForbidden()HTTP 응답코드가 403 Forbidden검증404 Not FoundisNotFound()HTTP 응답코드가 404 Not Found 검증4&lt;strong&gt;is4xxClientError()HTTP 응답코드가 4&lt;/strong&gt; 검증500 Internal Server ErrorisInternalServerError()HTTP 응답코드가 500 InternalServerError 검증5&lt;strong&gt;is5xxClientError()HTTP 응답코드가 5&lt;/strong&gt; 검증참고도서 : 스프링 부트 3 백엔드 개발자 되기 - 자바 편&lt;a href=&#34;https://mangkyu.tistory.com/143&#34;&gt;https://mangkyu.tistory.com/143&lt;/a&gt;#spring #TDD #JUnit #단위테스트 #assertj&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Spring] 순환참조란? The dependencies of some of the beans in the application context form a cycle</title>
      <link>http://localhost:1313/posts/47/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/47/</guid>
      <description>&lt;p&gt;순환참조는 맞물린 의존성 주입 (DI) 상태에서 어떤 빈을 먼저 생성할지 결정하지 못해서 생기에 발생한다. BeanA에서 BeanB를 참조(BeanA-&amp;gt;BeanB) 일 경우 스프링은 BeanB를 먼저 생성 후 BeanA를 생성하기에, BeanB에서 다시 BeanA를 참조할 경우 (BeanA-&amp;gt;BeanB-&amp;gt;BeanA) 순환 참조가 발생하게된다.## 2. 의존성 주입의존성 주입의 3가지 상황 (생성자 주입방식, 필드 주입방식, Setter주입)에서 순환참조가 발생할수 있다. 다음 포스트 각각의 상세 내용을 확인할 수 있고, 이번 포스트에서는 각각의 경우에 순환참조가 발생하면 어떤 차이점이 있는지 확인해 보자.&lt;a href=&#34;https://junhkang.tistory.com/42&#34;&gt;2023.11.06 - [Spring] - [Spring] IoC(제어의 역전) &amp;amp; DI(의존성 주입)의 개념&lt;/a&gt;[Spring] IoC(제어의 역전) &amp;amp; DI(의존성 주입)의 개념1. IoC (Inversion of Control) 제어의 역전 IoC란 메인 프로그램에서 컨테이너나 프레임워크로 객체와 객체의 의존성에 대한 제어를 넘기는 것을 말한다. 프레임워크 없이 개발할 때는 각 객체에 대한junhkang.tistory.com### ▶ 2-1. 생성자 주입&lt;code&gt;@Componentpublic class BeanA {private BeanB beanB;public void BeanA(BeanB beanB){this.beanB = beanB;}}@Componentpublic class BeanB {private BeanA beanA;public void BeanB(BeanA beanA){this.beanA = beanA;}}&lt;/code&gt;생성자 주입의 경우, 애플리케이션 구동 시 스프링 컨테이너(IOC)는 BeanA 빈을 생성하기 위해 BeanB를 찾고 BeanB를 찾기 위해 Bean A를 찾기 때문에 순환참조가 발생하게 된다.### ▶ 2-2. 필드 주입, Setter 방식필드 주입, Setter 방식은 애플리케이션의 실행 시점에서는 에러가 발생되지 않는다. 어플리케이션의 실행 시점이 아닌, 실제로 사용되는 시점에 실행되는 메서드가 순환 호출되기 때문이다. 필요 없는 시점에는 null 상태로 유지 후 사용될 때 의존성이 주입되며 참조되기 시작한다.## 3. 해결책### ▶ 3-1. @Lazy 어노테이션&lt;code&gt;@Componentpublic class BeanA {private BeanB beanB;public void BeanA(BeanB beanB){this.beanB = beanB;}}@Componentpublic class BeanB {private BeanA beanA;public void BeanB(@Lazy BeanA beanA){this.beanA = beanA;}}&lt;/code&gt;다음과 같이 @Lazy 어노테이션을 통해 시점을 지연시킬 수 있으나 스프링에서는 이 방식을 추천하지 않는다. 애플리케이션 로딩시점이 아닌 Bean이 필요한 시점에 주입받기 때문에 특정 HTTP 요청을 받을 때 Heap 메모리가 증가할 수 있으며 메모리가 충분하지 않은 경우 장애로 이어질 수 있다. 또한 잘못된 빈의 생성시점을 늦추기에 문제상황에 대한 인식이 늦어질 수 있다.### ▶ 3-2. 설계 변경근본적으로 순환참조가 일어나지 않는 설계를 해야 한다.단순하게는 BeanA -&amp;gt; BeanB-&amp;gt; BeanA의 관계를 BeanA -&amp;gt; BeanB -&amp;gt; BeanC 형태로 참조가 순환되지 않도록 분리해야 한다.#spring #의존성주입 #순환참조&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Spring] 스프링 배치 ItemReader의 개념, (MybatisCursorItemReader, MybatisPagingItemReader 구현)</title>
      <link>http://localhost:1313/posts/32/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/32/</guid>
      <description>&lt;p&gt;스프링 배치의 ItemReader는 다음과 같은 과정을 거쳐 데이터를 처리한다.&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/posts/32/img_1.png&#34;&gt;대부분의 데이터 형태는 이미 ItemReader로 제공하고 있기에 ItemReader, ItemStream 인터페이스 자체를 구현할 필요는 없다.ItemReader는 Chunk 기반 트랜잭션을 다루며 Cursor, Paging 가 대표적인 2가지 방식이다.## 2. Cursor, Paging 형식&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/posts/32/img_2.png&#34;&gt;### 2-1. Cursor기반 ItemReader- JDBC ResultSet의 기본 기능이다.- ResultSet이 Open 될 때마다 데이터베이스의 데이터가 반환된다.- 데이터베이스와 연결 맺은 후 데이터를 Streaming 방식으로 I/O이다.- 현재 행에서Cursor를 유지하며 다음 데이터를 호출하면 Cursor를 한 칸씩 옮기면서 데이터를 가져온다.- 하나의 Connection으로 배치가 끝날때까지 사용되기에 Batch가 끝나기 전에 데이터베이스와 애플리케이션의 연결이 먼저 끊어질 수 있어 데이터베이스와 SocketTimeout을 충분한 값으로 설정하여야 한다.- 모든 결과를 메모리에 할당 하기 때문에 메모리 사용량이 많아진다.- Chunk 사이즈 만큼의 트랜잭션 단위로 데이터를 처리한다.Cursor 기반 ItemReader 구현체&amp;gt; - JdbcCursorItemReader- HibernateCursorItemReader- StoredProcedureItemReader- MybatisCursorItemReader### 2-2. Paging기반 ItemReader- Chunk로 데이터베이스에서 데이터를 검색- Page Size 만큼만 한 번에 메모리로 가져오기에 메모리 사용량이 적어진다.- 페이지 단위로 컨넥션을 맺고 끊기를 반복하기에 대량 데이터를 처리하기 좋다.- 쿼리자체에 반환하고자하는 데이터 범위를 지정하여 사용한다. (offset, limit)- 컨넥션 유지시간이 길지 않고 메모리를 효율적으로 사용해야 하는 데이터에 적합하다.Paging 기반 ItemReader 구현체&amp;gt; - JdbcPagingItemReader- HibernatePagingItemReader- JpaPagingItemReader- MybatisPagingItemReader## 3. MybatisItemReader 구현### 3-1. MybatisCursorItemReaderMybatisCursorItemReader로 구현시 간단하다. 한 번에 조회해온 결과를 chunk만큼 트랜잭션을 분할하여 대용량 처리를 한다.BatchConfig.java&lt;code&gt;@Beanpublic T jobStep(StepBuilderFactory steps) throws Exception {return stepBuilderFactory.get(&amp;quot;JOB&amp;quot;). chunk(1000) -- Chunk 사이즈 조절.reader(itemReader.reader(sqlSessionFactory)).processor(new itemProcessor()).writer(new itemWriter()).build();}&lt;/code&gt;ItemReader.Java&lt;code&gt;MyBatisCursorItemReader databaseReader = new MyBatisCursorItemReader&amp;lt;&amp;gt;();databaseReader.setSqlSessionFactory(sqlSessionFactory);databaseReader.setQueryId(QueryId);databaseReader.setParameterValues(map);databaseReader.setSaveState(true);return databaseReader;&lt;/code&gt;데이터베이스에서 모든 결과를 메모리에 할당한 후, Chunk 사이즈만큼의 트랜잭션 단위로 데이터를 처리한다.### 3-2. MyBatisPagingItemReader 구현다음과 같이 조회 쿼리 자체에 OFFSET, LIMIT을 설정하여, 한 페이지당 조회할 데이터 위치를 파악한다.MyBatisPagingItemReader에서는 다음 파라미터로 페이징 관련 값들에 바로 접근이 가능하다.&amp;gt; _page : 읽을 page 수량 (0부터 시작)_pagesize : 한번에 읽을 페이지 수량 (리턴 받을 데이터의 수량)_skiprows : _page * _pagesize (다음 페이지 시작 포인트, offset)해당 값들을 쿼리에서 바로 사용 가능하며 다음과 같이 적용할 수 있다.&lt;code&gt;SELECT id, name, job FROM employees ORDER BY id ASCOFFSET #{_skiprows} LIMIT #{_pagesize}&lt;/code&gt;한번에 가져올 페이지 사이즈 (_pagesize)는 ItemReader.Java에서 setPageSize()를 통해 설정가능하다. (쿼리의 LIMIT에 해당하는 값)&lt;code&gt;MyBatisPagingItemReader databaseReader = new MyBatisPagingItemReader&amp;lt;&amp;gt;();databaseReader.setSqlSessionFactory(sqlSessionFactory);databaseReader.setQueryId(QueryId);databaseReader.setParameterValues(map);databaseReader.setPageSize(1000); -- Paging에서는 한번에 읽을 Page수량을 추가해야한다. default = 10databaseReader.setSaveState(true);return databaseReader;&lt;/code&gt;#### 주의사항매 페이지를 신규 조회할때 데이터의 변경되어 전체 페이징 기준이 달라진다면 누락되거나 중복처리되는 데이터가 있을 수 있다.같은 이유로, order by를 적절하게 하지 않을 경우 미처리, 혹은 중복처리 되는 데이터가 발견될 수 있다. 매 Paging마다 그 시점의 페이징 데이터를 조회하기 때문이다.특히 처리완료 데이터를 마킹하면서 처리하고, 미처리 데이터를 조회조건에 넣는다면, 데이터가 처리될 때마다 특정 페이지의 값들이 달라질 것이다. 이 경우 Cursor를 사용하면 쉽게 해결되지만, 메모리 및 컨넥션 타임 문제로 Paging을 꼭 사용하여야 하는 경우에는 쿼리에서 offset을 제거하거나 _page변수를 항상 0으로 지정해 주면 된다.MybatisPagingItemReader.java의 내부 구조를 확인해보면&lt;code&gt;@Overrideprotected void doReadPage() {if (sqlSessionTemplate == null) {sqlSessionTemplate = new SqlSessionTemplate(sqlSessionFactory, ExecutorType.BATCH);}Map parameters = new HashMap&amp;lt;&amp;gt;();if (parameterValues != null) {parameters.putAll(parameterValues);}parameters.put(&amp;quot;_page&amp;quot;, getPage());parameters.put(&amp;quot;_pagesize&amp;quot;, getPageSize());parameters.put(&amp;quot;_skiprows&amp;quot;, getPage() * getPageSize());if (results == null) {results = new CopyOnWriteArrayList&amp;lt;&amp;gt;();} else {results.clear();}results.addAll(sqlSessionTemplate.selectList(queryId, parameters));}&lt;/code&gt;_page는 getPage() 값을 사용하기 때문에&lt;code&gt;MyBatisPagingItemReader reader = new MyBatisPagingItemReader(){@Overridepublic int getPage()	{return 0;}};&lt;/code&gt;다음과 같이 매 Paging 조회마다 페이지 값을 0으로 리셋해주면 매 page를 조회할 때마다 offset = 0인 채로 조회가 가능하다.doReadPage()를 override 하여 페이지 읽는 로직 자체를 커스터마이징 하는 것도 가능하다.참고&lt;a href=&#34;https://mybatis.org/spring/batch.html&#34;&gt;https://mybatis.org/spring/batch.html&lt;/a&gt;&lt;a href=&#34;https://ojt90902.tistory.com/780&#34;&gt;https://ojt90902.tistory.com/780&lt;/a&gt;&lt;a href=&#34;https://junuuu.tistory.com/611&#34;&gt;https://junuuu.tistory.com/611&lt;/a&gt;&lt;a href=&#34;https://jojoldu.tistory.com/336&#34;&gt;https://jojoldu.tistory.com/336&lt;/a&gt;#spring #Batch #ItemReader&lt;/p&gt;</description>
    </item>
    <item>
      <title>[네트워크] REST, RESTful API의 개념 및 설계 가이드</title>
      <link>http://localhost:1313/posts/48/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/48/</guid>
      <description>&lt;p&gt;REST란 Representational State Transfer의 약자로 자원을 이름으로 구분하여 자원의 상태를 주고받는 것을 의미한다. HTTP URI를 통해 자원을 명시하고 HTTP Method를 통해 행위를 적용한다.### 1-1. REST 구성요소- 자원(Resource) : HTTP URI - 서버는 고유한 리소스 식별자로 각 리소스를 식별- 행위(Verb) : HTTP Method (GET, POST, PUT, DELETE)- 내용(Representations) : HTTP Message Pay Load - 하나의 자원은 JSON,XML, TEST, RSS 등 여러 형태의 Representaion으로 나타내어질 수 있다.### 1-2. REST의 특징- Stateless (무상태성) - 서버가 이전의 모든 요청과 독립적으로 클라이언트 요청을 완료함을 의미- Cacheable(캐쉬 가능성) - 일부 응답을 저장하는 프로세스인 캐싱을 지원함을 의미- Layered System (계층화) - 클라이언트는 REST API Server만 호출하지만, 클라이언트 요청을 이행하기 위해 함께 작동하는 비즈니스 로직(보안, 암호화 등)을 여러 계층으로 실행될 수 있도록 유연하게 설계 가능함을 의미- Uniform Interface (균일한 인터페이스) - 서버가 표준 형식으로 정보를 전송함을 의미### 1-3. 장점- HTTP 프로토콜을 그대로 사용하기에 별도 인프라를 구축할 필요가 없음- HTTP 프로토콜을 따르는 모든 플랫폼에서 사용 가능- API의 의도를 쉽고 명확하게 파악 가능- 클라이언트, 서버를 완전히 분리하기에 각 부분이 독립적으로 발전 가능- 사용되는 기술과 독립적이기에 API 설계에 영향을 주지 않고 다양한 프로그래밍 언어로 작성이 가능### 1-4. 단점- 표준이 존재하지 않아 정의가 필요함- HTTP Method 형태가 제한적## 2. RESTful API란?REST 아키텍쳐를 따르는 API를 RESTful API (Representaional state transfer API)라고 하며 REST 아키텍처를 구현하는 웹서비스를 RESTful 웹 서비스라고 한다. REST는 복잡한 네트워크에서 통신을 관리하기 위한 지침으로 만들어 졌으며, 대규모의 고성능 통신을 안정적으로 지원할 수 있고 쉽게 구현 및 수정할 수 있어 파악에 용이하고 여러 시스템에서 사용이 가능하다.## 3. RESTful API 설계- 동사 보다는 명사, 대문자보다 소문자 사용&lt;code&gt;/getArticles/1 -&amp;gt; /articles/1&lt;/code&gt;- 컬렉션 이름으로는 복수 명사 사용&lt;code&gt;/article/1 -&amp;gt; /articles/1&lt;/code&gt;- HTTP Method를 포함하지 않음&lt;code&gt;/get/articles/1 -&amp;gt; GET /articles/1&lt;/code&gt;- 행위에 대한 동사 표현이 포함하지 않음&lt;code&gt;/show/articles/1 -&amp;gt; /articles/1&lt;/code&gt;- 경로 부분 중 변하는 부분은 유일값으로 대체&lt;code&gt;/articles/{articleId} -&amp;gt; 각 articleId은 유일한 결과값을 가진다.&lt;/code&gt;- 마지막에 / 포함하지 않음&lt;code&gt;/articles/1/ -&amp;gt; /articles/1&lt;/code&gt;- 언더바 대신 하이픈 사용&lt;code&gt;/newest_ariticles/1 -&amp;gt; /newest-ariticles/1&lt;/code&gt;- 파일 확장자는 URI에 포함하지 않음&lt;code&gt;/articles/1/photo.jpg -&amp;gt; /articles/1/photo [Accept: image/jpg]&lt;/code&gt;참고&lt;a href=&#34;https://aws.amazon.com/ko/what-is/restful-api/&#34;&gt;https://aws.amazon.com/ko/what-is/restful-api/&lt;/a&gt;&lt;a href=&#34;https://khj93.tistory.com/entry/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-REST-API%EB%9E%80-REST-RESTful%EC%9D%B4%EB%9E%80&#34;&gt;https://khj93.tistory.com/entry/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-REST-API%EB%9E%80-REST-RESTful%EC%9D%B4%EB%9E%80&lt;/a&gt;#Rest #restful api&lt;/p&gt;</description>
    </item>
    <item>
      <title>[네트워크] TCP/IP의 개념</title>
      <link>http://localhost:1313/posts/31/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/31/</guid>
      <description>&lt;p&gt;패킷 전송방식의 인터넷 프로토콜인 IP와 전송 조절 프로토콜인 TCP로 이루어져 있다. IP는 패킷 전달 여부를 보증하지 않고, 패킷을 보낸 순서대로 받는 것을 보장하지 않지만, TCP는 IP 위에서 동작하는 프로토콜로 데이터의 전달을 보증하고 보낸 순서대로 받게 해 준다. IP가 패킷 간의 관계를 이해하지 못하고 목적지를 찾아가는 데만 집중한다면 TCP는 Endpoint 간 통신할 준비가 되어있는지, 데이터 전송이 제대로 되었는지, 데이터가 변질되지 않은지, 데이터 유실은 없는지 등을 점검한다.즉 IP주소 체계를 따르고 IP Routing을 통해 목적지에 도달하여 TCP의 특성을 활용하여 송신자와 수신자의 논리적 연결을 생성, 신뢰성 유지한다.(HTTP, FTP, SFTP 등 TCP를 기반으로 하는 많은 어플리케이션 프로토콜들이 IP 위에서 동작하기 때문에 묶어서 TCP/IP로 부르기도 한다.)&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/posts/31/img.png&#34;&gt;### 1-1. IP (Internet Protocol)클라이언트와 서버는 각 IP주소를 가지고 지정한 IP 주소에 패킷 단위로 데이터를 전송한다. 전송된 데이터는 인터넷 망 내의 노드를 거쳐 목적지에 도달하게 된다. IP프로토콜만으로 통신할 경우 한계가 존재한다.&amp;gt; 비연결성 - IP프로토콜만으로는 클라이언트에서 서버가 패킷을 받을 수 있는 상황인지 확인할 수 없다. 따라서 수신 서버가 없거나 서비스가 불가능하더라도 패킷을 전송한다.비신뢰성 - 인터넷 망 내 노드에 문제가 생기는 경우 패킷이 손실 없이 안전하게 도달하지 못할 수 있거나 전송순서가 바뀔 수 있다.또한, 같은 IP를 사용하는 서버에서 통신하는 애플리케이션이 둘 이상이라면 IP 만으로는 구분할 수 없다.### 1-2. TCP (Transmission Control Protocol)IP의 핵심 프로토콜 중 하나로 근거리 통신망이나 인트라넷, 인터넷에 연결된 컴퓨터에서 실행되는 프로그램 간의 일련의 옥텟을데이터 패킷에 일련의 번호를 부여함으로써 안정적으로 순서대로 에러 없이 교환하도록 한다. Transport Layer에 위치하며 네트워크 정보전달을 통제하는 프로토콜이다. Endpoint 간 연결을 생성하고 데이터를 얼마나 보냈는지, 받았는지가 TCP 헤더에 담겨있으며 흐름제어, 혼잡제어에 관여할 수 있는 다양한 요소들이 포함되어 있다. TCP는 3-way handshake를 통해 통신을 시도한다.&lt;a href=&#34;https://junhkang.tistory.com/30&#34;&gt;2023.10.18 - [네트워크] - [네트워크] 소켓(SOCEKT) 통신, 3-way handshake의 개념&lt;/a&gt;[네트워크] 소켓(SOCEKT) 통신, 3-way handshake의 개념1. 소켓(Socket)이란 소켓은 떨어져 있는 두 호스트를 연결해 주는 도구로써 인터페이스 역할을 한다. TCP/IP 기반 네트워크 통신에서 데이터 송수신의 앤드포인트이며 앤드포인트는 IP, Port조합으로junhkang.tistory.comTCP의 특징신뢰성보장 &amp;amp; 흐름제어분할된 데이터의 고유번호를 통해 수신자가 어디까지 받았는지 지속적으로 확인가능하며, 이를 통해 데이터의 손실 없이 전송이 가능하다. TCP Header 내의 window size를 사용해 한번에 송/수신할 수 있는 데이터 양을 정하며 수신 측에서 받을 수 있는 양을 기준으로 Window size가 정해진다(3-hand shake 간 결정됨). 송수신시 계속 확인 응답을 받기에 신뢰도가 높지만, 데이터 용량이 증가하여 수신속도가 떨어진다.혼잡제어Endpoint 간의 흐름제어 외에 네트워크망의 혼잡제어를 한다. 송신자는 연결초기에 데이터 송출량을 낮게 잡고 보내면서 수신자의 수신을 확인하며 데이터 송출량을 조금씩 늘린다. 이를 통해 네트워크에 가장 적합한 데이터 송출량을 확인할 수 있으며 이를 &amp;lsquo;Slow Start&amp;rsquo;라고 한다.멀티캐스트 불가능1:1 전송방식으로 유니캐스트성이다. 단일 송신자와 단일수신자 간의 경로 연결이 설정된다.#네트워크 #iP #TCP/IP #tcp&lt;/p&gt;</description>
    </item>
    <item>
      <title>[운영체제(OS)] 스레드 (Thread), 멀티스레드(Multithreaded Programming)란?</title>
      <link>http://localhost:1313/posts/28/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/28/</guid>
      <description>&lt;p&gt;CPU 수행의 기본단위이며 특히 프로세스 안의 흐름의 단위이다. 스레드가 수행되는 환경을 Task라고 하며 Thread ID, Program counter, register set, Stack space로 구성된다. 각각의 스레드는 레지스터 상태와 스택을 갖는다. Code, Data 섹션이나 운영체제 자원들은 스레드끼리 공유한다.### 스레드의 종류스레드는 지원 주체에 따라 2가지로 나눌 수 있다.User Threads- 유저 스레드는 사용자 수준의 스레드 라이브러리가 관리하는 스레드- 라이브러리는 스레드의 생성 및 스케쥴링 등 관리 기능을 제공한다.- 동일 메모리에서 스레드가 생성 및 관리되므로 속도가 빠르다.- 여러 개의 사용자 스레드 중 하나의 스레드가 시스템 호출 등으로 중단되면 나머지 스레드가 같이 종료된다. (커널이 프로세스 내부 스레드를 인식하지 못하여 해당 프로세스를 대기상태로 전환시키기 때문)- 스레드 라이브러리에는 POSIX, Pthreads, Win32 threads, Java threads 대표적이다Kernel Threads- 커널 스레드는 커널이 지원하는 스레드- 커널 스레드를 사용하면 안정적이지만 유저모드에서 커널모드로 계속 바꿔줘야 하기에 성능이 저하된다.- 반대로 유저 스레드를 사용하면 안정성은 떨어지지만 성능이 저하되지는 않는다.- 스레드가 시스템 호출 등으로 중단되어도 다른 스레드를 중단시키지 않고 계속 실행시킨다.### Thread Group (스레드 그룹)Thread Group (스레드 그룹)이란 관련 있는 스레드를 그룹으로 묶어 다루는 장치이다. 쓰레드 그룹은 다른 스레드그룹에 포함될 수 있으며, 트리형태로 연결된다. 스레드는는 자신이 포함된 스레드 그룹이나 하위 그룹에는 접근가능 하지만, 다른 그룹에는 접근할 수 없다.### Deamon Thread(데몬 스레드)- 다른 일반 스레드의 작업을 돕는 보조 쓰레드- 일반 스레드가 모두 종료되면 자동으로 종료- 일정시간마도 자동수행되는 저장/ 화면 갱신등에 사용### Thread Pools스레드를 요청할 때마다 매번 새로 생성하고, 수행하고, 지우고 반복하면 성능저하로 이어진다.그래서 미리 스레드 풀에 여러 개의 스레드를 만들어두고 요청이 오면 스레드풀에서 스레드를 할당해 주는 방법을 사용한다.## 2. 멀티스레드란?한 번에 하나의 작업만 수행하면 싱글 스레드, 하나의 프로세스가 둘 이상의 스레드가 동시에 작업을 수행하면 멀티스레드라 한다.멀티프로세싱 시스템이 여러 개의 완전한 처리 장치들을 포함하는 반면 멀티스레딩은 스레드 수준뿐 아니라 명령어 수준의 병렬 처리에까지 신경을 쓰면서 하나의 코어에 대한 이용성을 증가하는 것에 초점을 두고 있다.### 멀티스레드의 장점두 프로세스가 하나의 데이터를 공유하려면 공유메모리 또는 파이프를 사용해야 하지만, 효율이 떨어지고 구현/관리하기 힘들다.프로세스사이 콘텍스트 스위치가 지속적으로 일어나면 성능저하 발생 (스레드 전환 시에도 일어나지만 속도가 더 빠르다)응답성 : 대화형 프로그램을 멀티스레드화 하면 일부 스레드가 중단되거나 긴 작업을 수행하더라도  다른 스레드가 별도의 작업을 할 수 있어 응답성이 좋다.자원공유 : 프로세스 내의 자원과 메모리를 공유함으로 시스템 자원의 낭비가 적다. 또한 같은 주소 공간 내에 여러 개의 활동성 스레드를 가질 수 있다는 장점이 있다.경제성 : 메모리와 자원할당은 많인 비용이 소모된다. 스레드는 프로세스 내 자원을 공유하기에 스레드생성과 Context Switching을 하는 것이 효율적이다.멀티프로세서 활용 : 각각의 스레드가 다른 프로세스에서 병렬로 수행 가능하다. 단일 쓰레드 프로세스는 CPU가 많아도 1개의 CPU에서만 실행되지만, 다중 스레드화 하면 다중 CPU에서 병렬성이 증간된다.프로세스와 비교두 프로세스가 하나의 데이터를 공유하려면 공유메모리 또는 파이프를 사용해야 하지만, 효율이 떨어지고 구현/관리하기 힘들다.스레드, 프로세스 사이 콘텍스트 스위치가 지속적으로 일어나면 성능저하 발생하나 스레드의 Context Switching의 속도가 더 빨라서 효율적이다.### 멀티스레드의 단점- 캐시, 변환 생인 버퍼(TLB) 등의 하드웨어 리소스를 공유할 때 서로 간섭할 수 있다.- Context Switching 시간이 길수록 멀티 쓰레딩의 효율은 저하되어 단순 계산은 싱글 스레드 보다 실행시간이 개선되지 않고 오히려 지연될 수 있다.- 멀티 쓰레딩의 하드웨어 지원을 위해 애플리케이션, 운영체제 모두에 최적화 변경이 필요하다.- 각 스레드 중 어떤 것이 먼저 실행될지 그 순서를 알 수 없다.예를 들어 스레드 1, 스레드 2로 다음 작업을 수행할 때,&amp;gt; - 공유되는 변수 i의 값을 레지스터에 저장- 레지스터의 값을 1 증가- 변수 i에 그 값을 저장쓰레드동작i스레드 1의 레지스터스레드 2의 레지스터스레드 1i의 값을 레지스터에 저장00스레드 1레지스터 값을 1 증가01스레드 1i에 값 저장11스레드 2i의 값을 레지스터에 저장111스레드 2레지스터 값을 1 증가112스레드 2i에 값 저장212스레드 순서가 정상적으로 처리된다면 다음과 같이 최종적으로 i = 2가 되지만, 스레드 실행 순서가 달라진다면스레드동작i스레드 1의 레지스터스레드 2의 레지스터스레드 1i의 값을 레지스터에 저장00스레드 2i의 값을 레지스터에 저장000스레드 1레지스터 값을 1 증가010스레드 2레지스터 값을 1 증가011스레드 1i에 값 저장111스레드 2i에 값 저장111i = 1 이 되기에 의도와 다른 수행이 일어나며, 스레드의 실행조건에 따라 다른 결과를 나타내기에 원인 파악이 힘들다.이러한 문제를 경쟁조건이라고 하며 세마포어 같은 방법으로 공유데이터에 접근하는 스레드의 개수를 한 개 이하로 유지하여 해결할 수 있다.### Context Switching컴퓨터가 동시에 처리할 수 있는 작업 수는 CPU의 코어 수량과 같다. CPU 코어보다 많은 스레드가 동시에 실행되면 각 코어별로 정해진 시간만큼 번갈아가며 작업을 수행한다. 각 스레드가 서로 번갈아가며 교체될때 쓰레드간 현재까지의 작업상태나 다음 작업에 필요한 데이터를 저장하고 읽는 작업을 하는데 이를 Context Switching라고 한다. Context Switching 시간이 길수록 멀티 쓰레딩의 효율은 저하된다. 그래서 많은 양의 단순계산은 싱글 쓰레드로 처리하는 것이 효율적인 경우가 있기에 쓰레드 수가 많은 게 항상 고성능은 아니다.### Multithreaded Server Architecture서버와 클라이언트 사이에도 멀티 스레드를 구현한다. 클라이언트가 새로운 요청을 하면 서버는 스레드를 새로 생성해서 요청을 수행한다.  프로세스 보다 쓰레드를 생성하는 것이 더 빠르기 때문에 효율적이다.### Multicore Programming동시성(Concurrency)동시성은 싱글 프로세스에서 사용되는 방식으로 프로세서가 여러 개의 스레드를 번갈아가면 수행하며 동시에 실행되는 것처럼 보이게 한다.병렬성(Parallelism)병렬성은 멀티코어 방식에서 사용되는 방식으로 여러 개의 코어가 스레드를 동시에 수행한다.## 3. Multithreading Models유저 스레드와 커널 쓰레드 관계를 정의하는 방식이다.### Many-to-One Model- 하나의 커널 스레드에 여러 개의 유저 스레드 연결- 사용자 수준에서의 스레드 관리- 주로 커널 스레드를 지원하지 않는 시스템에서 사용- 한 번에 하나의 유저스레드만 커널에 접근가능- 멀티코어 시스템에서 병렬적인 수행이 불가능### One-to-One Model- 하나의 유저 스레드에 하나의 커널 스레드가 대응하는 모델- Many-to-One방식에서 시스템 호출 시 다른 스레드들이 중단되는 문제를 해결할 수 있어 동시성 향상- 멀티프로세서 시스템에서는 동시에 여러 개 쓰레드 수행 가능- 유저 스레드 증가분만큼 커널 스레드가 증가.- 커널 스레드를 생성하는 것은 오버헤드가 큰 작업이기에 성능저하 발생가능### Many-to-Many Model- 여러 유저스레드가 더 적거나 같은 수의 커널 스레드에 대응하는 모델- 운영체제에 충분한 수의 커널 스레드를 생성가능- 커널 스레드의 구체적 개수는 프로그램이나 작동기기에 따라 상이- 멀티프로세서 프로그램에서는 싱글프로세서 보다 더 많은 커널 스레드가 생성- 커널이 사용자 스레드와 커널 스레드의 매핑을 적절하게 조절### Two-level Model- Many-to-Many 모델과 유사- 특정 유저 스레드를 위한 커널 스레드를 따로 제공하는 모델- 점유율이 높아야 하는 유저 스레드를 더 빠르게 처리 가능참고&lt;a href=&#34;https://ko.wikipedia.org/wiki/%EC%8A%A4%EB%A0%88%EB%93%9C_(%EC%BB%B4%ED%93%A8%ED%8C%85)&#34;&gt;https://ko.wikipedia.org/wiki/스레드_(컴퓨팅)&lt;/a&gt;&lt;a href=&#34;https://rebro.kr/174&#34;&gt;https://rebro.kr/174&lt;/a&gt;&lt;a href=&#34;http://www.tcpschool.com/java/java_thread_multi&#34;&gt;http://www.tcpschool.com/java/java_thread_multi&lt;/a&gt;&lt;a href=&#34;https://ko.wikipedia.org/wiki/%EB%A9%80%ED%8B%B0%EC%8A%A4%EB%A0%88%EB%94%A9&#34;&gt;https://ko.wikipedia.org/wiki/%EB%A9%80%ED%8B%B0%EC%8A%A4%EB%A0%88%EB%94%A9&lt;/a&gt;#OS #운영체제 #스레드 #멀티스레드&lt;/p&gt;</description>
    </item>
    <item>
      <title>[이펙티브 자바] 1. 생성자 대신 정적 팩터리 메서드를 고려하라</title>
      <link>http://localhost:1313/posts/53/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/53/</guid>
      <description>&lt;p&gt;클라이언트가 클래스 인스턴스를 얻는 방법에는 전통적인 방법 중 하나는  public이다. 하지만 정적 팩터리 메서드(static factory method)도 꼭 알아두어야한다.## 1. 정적 팩터리 메서드란?그렇다면 정적 팩터리 메서드는 무엇일까? 간단히 말해 객체 생성의 역할을 하는 클래스 메서드로, static 메서드를 통해 인스턴스를 생성하는 것이다. 다음은 java의 기본 Boolean 클래스 내 정적 팩토리 메서드의 간단한 예시이다.![](/images/posts/53/스크린샷 2024-01-17 오후 5.02.29.png)이팩티브 자바에서는 정적 팩토리 메서드를 사용할 시의 5가지 장점과 2가지 단점에 대해 서술하고 있어 자세한 비교를 통해 하나하나 알아보려 한다.## 2. 정적 팩토리 메서드 (static factory method)의 장점#### 2-1. 이름을 가질 수 있다.인스턴스를 대표하는 생성자가 명확하거나, 반환될 객체에 대한 설명이 필요하지 않을 경우에는 크게 느껴지지 않는 차이일 수 있지만, 이름을 가질 수 있어 반환될 객체의 특징을 설명할 수 있다는 것은 굉장한 장점이다. 이펙티브 자바에서는 BigInteger와 BigInteger.probablePrime의 차이를 예로 들고 있다. 먼저 BigInteger(int, int, Random)의 예제를 보면![](/images/posts/53/스크린샷 2024-01-17 오후 5.09.35.png)설명을 읽어보면 &amp;ldquo;지정된 bitLength의 소수일 가능성이 있는 임의의 BigInteger를 생성한다.&amp;ldquo;는 것을 이해할 수 있지만, 그전에는 명확한 반환될 객체의 특성을 알 수 없다. 또한, 하나의 시그니처로는 생성자를 한 개만 만들 수 있기에 제약이 있다. 예를 들어 동일한 BigInteger(int, int, Random) 생성자는 다른 의미를 가질 수 없다. BigInteger(int, Random, int)와 같이 순서를 바꾸거나, 추가하는 식으로 피해 갈 수는 있지만, 당연히 좋지 않은 방식이다. (추가될 때마다 클래스 설명 문서를 확인해야 하고, 호출하는데 실수가 있을 수 있다.)그에 비해 자바 4에서 추가된 BigInteger.probablePrime을 보면![](/images/posts/53/스크린샷 2024-01-17 오후 5.16.10.png)&amp;ldquo;값이 소수인 BigInteger를 반환한다&amp;quot;라는 의미를 이름만으로도 충분히 유추가 가능하다. 따라서 한 클래스에 시그니처 생성자가 여러 개 필요하다면 생성자를 정적 팩토리 메서드로 바꾸고 그 특징을 설명할 수 있는 이름을 붙이자#### 2-2. 호출될 때마다 인스턴스를 새로 생성하지 않아도 된다.개인적으로는 큰 시스템일수록 가장 큰 장점이 되지 않을까 싶은데, 불변 클래스는 인스턴스를 미리 만들어 놓거나 새로 생성된 인스턴스를 캐싱하여 활용하기에 불필요한 객체 생성 방지한다. 위의 예제에서 본 Boolean.valueOf(boolean)을 보면 객체를 아얘 생성하지 않는다. 그래서 (특히 생성 비용이 큰) 같은 객체가 자주 요청되는 상황이라면 성능을 상당히 올려준다. 또한 이는 인스턴스의 생명 주기를 철저히 컨트롤 가능하는 뜻이며, 클래스를 싱글턴 혹은 인스턴스화 불가 상태로 만들 수도 있다.#### 2-3. 반환 타입의 하위 타입 객체를 반환할 수 있다.반환할 객체의 클래스를 자유롭게 선택할 수 있는 유연성을 제공한다. 응용하면 API를 만들 때 구현 클래스를 공개하지 않고 객체를 반환할 수 있어 API를 작게 유지가 가능하다. 자바 8 전에는 인터페이스에 정적 메서드 선언 불가하였고, 이름이 &amp;ldquo;Type&amp;quot;인 인터페이스를 반환하는 정적 메서드가 필요하면 &amp;ldquo;Types&amp;quot;라는 (인스턴스화 불가인) 동반 클래스를 만들어 그 안에 정의하는 것이 관례였다.예를 들어 자바 컬렉션 프레임워크는 핵심 인터페이스들에 수정 불가나 동기화 등의 기능을 붙인 총 45개의 유틸리티 구현체를 제공하고, 이 구현체 대부분을 단 하나의 인스턴스화 불가 클래스인 java.util.Collections에서 정적 팩토리 메서드를 통해 얻도록 한다.다음은 java.util.Collections의 동기화 기능의 정적 팩터리 메서드이다.![](/images/posts/53/스크린샷 2024-01-17 오후 5.31.56.png)![](/images/posts/53/스크린샷 2024-01-17 오후 5.30.15.png)컬렉션 프레임워크 자체는 이 45개 클래스를 공개하지 않기 때문에 API 외견을 훨씬 작게 만들 수 있었다. API가 작아진 것은 물론 개념적인 무게, 프로그래머가 API를 사용하기 위해 익혀야 하는 개념의 수와 난이도도 낮아졌다.(프로그래머는 명시한 인터페이스 대로 동작하는 객체를 얻을 것임을 알기에 굳이 문서를 찾거나 실제 구현클래스가 무엇인지 알아보지 않아도 된다. 나아가 정적 팩토리 메서드를 사용하는 클라이언트는 얻은 객체를 인터페이스만으로 다루게 된다.)추가로, 자바 8부터는 인터페이스가 정적 메서드를 가질 수 없다는 제한이 풀렸기에 인스턴스화 불가 동반 클래스를 둘 이유가 별로 없다. 동반 클래스에 두었던 public 정적 멤버들 상당수를 그냥 인터페이스 자체에 두면 된다. (자바 9에서는 private 정적 메서드까지 허용하지만 정적 필드와 정적 멤버 클래스는 여전히 public이어야만 함)#### 2-4. 입력 매개 변수에 따라 매번 다른 클래스의 객체를 반환할 수 있다.반환 타입의 하위타입이기만 하면 어떤 클래스의 객체를 반환하던 상관없다. 심지어 다음 릴리즈에서는 또 다른 클래스의 객체를 반환해도 된다. 즉 하위 타입이기만 하면 API 변경 시 또 다른 클래스의 객체를 반환해도 된다. 예를들어 EnumSet 클래스는 public 생성자 없이 정적 팩토리만 제공하는데 openjdk에서는 원소의 수에 따라 두 가지 하위 클래스중 하나의 인스턴스를 반환한다.![](/images/posts/53/스크린샷 2024-01-17 오후 5.39.08.png)반환 값을 보면 원소가 64개 이하면 long변수 하나로 원소를 관리하는 RegularEnumSet을, 65개 이상이면 long 배열로 관리하는 JumboEnumSet을 반환한다. 만약 원소가 적을 때 RegularEnumSet을 사용할 이점이 없어진다면 다음 릴리즈에는 이를 삭제해도 클라이언트는 아무런 변화도 알 수 없을 것이다.  클라이언트는 팩토리가 건네주는 객체가 어느 클래스의 인스턴스인지 알 수 없고 알 필요도 없다. EnumSet의 하위 클래스이기만 하면 된다.#### 2-5. 정적 팩토리 메서드를 작성하는 시점에는 반환할 객체의 클래스가 존재하지 않아도 된다.메서드를 작성하는 시점에 반환할 객체의 클래스가 존재하지 않는다는 것은 서비스 제공자 프레임워크 (service provider framework)의 근간이다. 이 뜻을 자세히 살펴보면서비스 제공자 프레임워크의 제공자(provider)는 서비스의 구현체이고, 이 구현체들을 클라이언트에 제공하는 역할을 프레임워크가 통제하여 클라이언트를 구현체로부터 분리해준다.서비스 제공자 프레임워크의 핵심 프레임워크 3가지는&amp;gt; 서비스 인터페이스 (service interface) - 구현체의 동작을 정의한다.제공자 등록 API(provider registration API) - 제공자가 구현체를 등록할 때 사용서비스 접근 API(service access API) - 클라이언트가 서비스의 인스턴스를 얻을 때 사용 (클라이언트는 서비스 접근 API를 사용할 때 원하는 구현체의 조건 명시 가능, 조건을 명시하지 않으면 기본 구현체를 반환하거나 지원하는 구현체들을 하나씩 돌아가며 반환)이 중 서비스 접근 API가 바로 앞서 말한 서비스 제공자 프레임워크의 근간이라고 한 유연한 정적팩토리의 실체이다. (추가로 서비스 인터페이스의 인스턴스를 생성하는 팩터리 객체를 설명하는 서비스 제공자 인터페이스 (Service Provider Interface)가 쓰이기도 한다.)익숙한 프레임워크이자 대표적인 서비스 제공자 프레임워크인 JDBC(java database connectivity)를 살펴보면 이해가 쉽다.&amp;gt; Connection - 서비스 인터페이스 역할DriverManager.registerDriver - 제공자 등록 API 역할DriverManager.getConnection - 서비스 접근 API 역할Driver - 서비스 제공자 인터페이스 역할(자바 6부터는 java.util.ServiceLoader라는 범용 서비스 제공자 프레임워크가 제공되어 프레임워크를 직접 만들 필요가 거의 없지만, JDBC는 6전에 등장하였기에 ServiceLoader를 사용하지 않는다.)## 3. 정적 팩토리 메서드 (static factory method)의 단점#### 3-1. 상속을 하려면 public, protected 생성자가 필요하니 정적 팩토리 메서드만 제공하면 하위 클래스 생성이 불가하다.컬렉션 프레임워크의 유틸리티 구현 클래스들은 상속이 불가하다는 말이다. 상속보다 컴포지션을 사용하도록 유도하고 불변타입으로 만들려면 이 제약을 지켜야 한다는 점에서 오히려 장점일 수도 있다.#### 3-2. 정적 팩토리 메서드는 프로그래머가 찾기 힘들다.생성자처럼 API 설명에 명확히 드러나지 않으니 사용자는 정적 팩터리 메서드 방식 클래스를 인스턴스화할 방법을 알아내야 한다. API 문서를 규격화하고, 메서드 명도 널리 알려진 규약에 따라 짓는 것으로 어느 정도 해결해야 한다.#### 3-3. 정적 팩토리 메서드에서 흔히 사용하는 네이밍- From : 매개변수를 하나 받아서 해당 타입 인스턴스를 반환하는 형변환 메서드 ex)&lt;code&gt;Date d = Date.from(instant);&lt;/code&gt;- Of : 여러 매개변수를 받아 적합한 타입의 인스턴스를 반환하는 집계 메서드&lt;code&gt;Set faceCards =EnumSet.of(JACK,QUEEN, KING);&lt;/code&gt;- valueOf : from과 of의 더 자세한 버전&lt;code&gt;BigInteger prime = BigInteger.valueOf(Integer.MAX_VALUE);&lt;/code&gt;- Instance / getInstance : (매개변수를 받는다면) 매개변수로 명시한 인스턴스를 반환하지만, 같은 인스턴스임을 보장하지는 않음&lt;code&gt;StackWalker Luke = StackWalker.getInstance(options);&lt;/code&gt;- Create/newInstance : instance/getInstance와 같지만, 매번 새로운 인스턴스를 생성해 반환함을 보장 ex)&lt;code&gt;Object newArray = Array.newInstance(classObject, arrayLen);&lt;/code&gt;- getType : getInstance와 같으나, 생성할 클래스가 아닌 다른 클래스에 팩토리 매서드를 정의할 때 사용. &amp;ldquo;Type&amp;quot;은 팩터리 메서드가 반환할 객체의 타입&lt;code&gt;FileStore fs = Files.getFileStore(path)&lt;/code&gt;- newType : newInstance와 같으나, 생성할 클래스가 아닌 다른 클래스에 팩터리 메서드를 정의할 때 사용. &amp;ldquo;Type&amp;quot;은 팩터리 메서드가 반환할 객체의 타입&lt;code&gt;BufferReader br = Files.newFufferedReader(path)&lt;/code&gt;- type : getType과 newType의 간결한 버전&lt;code&gt;List litany = Collections.list(legacyLitany);&lt;/code&gt;## 4. 정리적정 팩터리 매서드와 public 생성자는 각각 쓰임새가 있으니 장담점을 이해하고 써야 한다. 정적 팩토리를 사용할 경우가 유리한 경우가 더 많기에 무작정 public 생성자를 썼다면 다시 한번 생각해 보자책의 예제 소스와 상세 내용은 다음 repo에서 확인 가능하다.&lt;a href=&#34;https://github.com/junhkang/effective-java-summary&#34;&gt;https://github.com/junhkang/effective-java-summary&lt;/a&gt;GitHub - junhkang/effective-java-summary: A personal summary of Effective Java (by Joshua Bloch)A personal summary of Effective Java (by Joshua Bloch) - GitHub - junhkang/effective-java-summary: A personal summary of Effective Java (by Joshua Bloch)github.com#Effective Java #정적 팩토리 메서드&lt;/p&gt;</description>
    </item>
    <item>
      <title>[이펙티브 자바] 5. 자원을 직접 명시하지 말고 의존 객체 주입을 사용하라</title>
      <link>http://localhost:1313/posts/76/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/76/</guid>
      <description>&lt;p&gt;한 클래스 내에서 여러 개의 자원에 의존하여 사용되는 경우에 의존 객체 주입을 통해 유연성과 테스트 용이성을 개선하는 내용이다. 스프링의 의존성 주입 개념을 생각해 본다면 이미 당연하게 사용하고 있는 경우가 많을 것이지만, 의존 객체 주입의 장점을 다시 한번 생각해 볼 수 있는 내용이다.&lt;a href=&#34;https://junhkang.tistory.com/42&#34;&gt;2023.11.06 - [Spring] - [Spring] IoC(제어의 역전) &amp;amp; DI(의존성 주입)의 개념&lt;/a&gt;[Spring] IoC(제어의 역전) &amp;amp; DI(의존성 주입)의 개념1. IoC (Inversion of Control) 제어의 역전 IoC란 메인 프로그램에서 컨테이너나 프레임워크로 객체와 객체의 의존성에 대한 제어를 넘기는 것을 말한다. 프레임워크 없이 개발할 때는 각 객체에 대한junhkang.tistory.com이펙티브 자바 책에서는 &amp;ldquo;맞춤법 검사기 (SpellChecker)&amp;rdquo; 클레스에서 &amp;ldquo;사전 (Dictionary)&amp;rdquo; 자원을 사용하는 예제를 들고 있다. 맞춤법 검사기 (SpellChecker)는 사전(dictionary) 자원에 의존하는 상황을 정적 유틸리티, 싱글턴, 의존객체 주입의 차이를 비교하고 있다.## 2. 예제### 2-1. 정적 유틸리티정적 유틸리티를 잘못 사용한 예 - 유연하지 않고 테스트하기 어렵다.&lt;code&gt;public class SpellCheckerStatic {private static final Lexicon dictionary = new Lexicon();private SpellCheckerStatic() {} // 객체 생성 방지public static boolean isValid(String word) {return dictionary.isValid(word);}public static List suggestions(String typo) {return dictionary.suggestions(typo);}}&lt;/code&gt;### 2-2. 싱글턴싱글턴을 잘못 사용한 예 - 유연하지 않고 테스트하기 어렵다.&lt;code&gt;public class SpellCheckerSingleton {private static final Lexicon dictionary = new Lexicon();private SpellCheckerSingleton() {}public static SpellCheckerSingleton INSTANCE = new SpellCheckerSingleton();public static boolean isValid(String word) {return dictionary.isValid(word);}public static List suggestions(String typo) {return dictionary.suggestions(typo);}}&lt;/code&gt;두방식 모두 dictionary 자원을 한 가지만 사용한다. 그렇기에 다양한 언어의 사전을 사용하거나 특수 언어용 사전을 별도로 쓰는 경우의 확장성을 생각해 보면 좋지 않다. 일반적으로 클래스에서 여러 자원을 참조할 경우 흔히 발생하는 상황이다. 위의 Spellchecker클래스에서 여러 사전을 유연하게 쓸 수 있도록 수정하려면 대표적으로 다음 두 가지 방식이 있을 것이다.1. 단순히 dictionary 필드에서 final 제한을 제거하고 다른 사전으로 교체하는 메서드를 추가한다.어색하고, 오류를 내기 쉬우며 멀티 스레드 환경에서 사용 불가능하다. 사용하는 자원 객체에 따라 이후 동작이 달라지는 클래스에서는 정적 유틸리티 클래스나 싱글턴 방식이 적합하지 않다.2. 인스턴스를 생성하는 시점에 필요에 맞는 dictionary를 넘겨주는 방식으로 변경한다. (의존객체 주입)의존 객체 주입의 한 형태로 맞춤법 검사기를 생성할 때 의존 객체인 사전을 주입해주면 된다. SpellChecker 클래스가 여러 인스턴스를 지원해야 하며 클라이언트가 원하는 자원을 사용해야 한다.### 2-3. 의존객체 주입의존객체 주입은 유연성과 테스트 용이성을 높여준다.&lt;code&gt;public class SpellCheckerInjection {private static final Lexicon dictionary;public SpellCheckerInjection(Lexicon disctionary) {this.dictionary = Objects.requireNonNull(disctionary);}public static boolean isValid(String word) {return dictionary.isValid(word);}public static List suggestions(String typo) {return dictionary.suggestions(typo);}}&lt;/code&gt;예시에서는 dictionary라는 하나의 자원만 사용하지만 여러 개의 자원을 참조하는 경우가 대부분이다. 그런 상황에서 의존객체 주입을 사용했을 때의 장점을 생각해 보자.- 몇 개의 자원에 의존하던 관계없이 실행된다.- 불변성을 보장하여 여러 클라이언트가 의존객체들을 안심하고 공유할 수 있다.- 테스트가 용이하다.이에 변형으로 생성자에 자원 팩토리 자체를 넘겨주는 방식도 별도로 소개하고 있다. 호출될 때마다 특정 타입의 인스턴스를 반복해서 만들어주는 객체를 말한다.(팩토리 메서드 패턴) 책에서는 자바 8의 Supplier  인터페이스를 팩토리를 표현한 완벽한 예제로 소개하고 있다.![](/images/posts/76/스크린샷 2024-03-25 오후 2.00.21.png)팩토리의 타입 매개변수를 제한하며, 이 방식을 사용해 클라이언트는 자신이 명시한 타입의 하위 타입이라면 무엇이든 생성할 수 있는 팩토리를 넘길 수 있다. 다음 코드는 클라이언트가 제공한 팩토리가 생성한 타일들로 구성된 Mosaic를 만드는 샘플 메서드이다.&lt;code&gt;Mosaic create(Supplier&amp;lt;&amp;gt; extends Tile&amp;gt; tileFactory) {Tile tile = tileFactory.get();return new Mosaic(tile);}&lt;/code&gt;## 3. 정리클래스가 내부에서 하나 이상의 자원에 의존할 때 클래스 동작에 영향을 주는 자원이 있다면 싱글턴, 정적 유틸 클래스는 사용하지 않는 것이 좋고 이 자원들을 클래스가 새로 생성해서도 안된다. 필요한 자원들을 생성자에 넘겨주는 생성자 주입 방식으로 구현하면 클래스의 유연성, 재사용성, 테스트 용이성을 매우 개선해 준다.책의 예제 소스와 상세 내용은 다음 repo에서 확인 가능하다.&lt;a href=&#34;https://github.com/junhkang/effective-java-summary&#34;&gt;https://github.com/junhkang/effective-java-summary&lt;/a&gt;GitHub - junhkang/effective-java-summary: Effective Java (by Joshua Bloch) 내용 및 예제 정리Effective Java (by Joshua Bloch) 내용 및 예제 정리. Contribute to junhkang/effective-java-summary development by creating an account on GitHub.github.com#이펙티브자바&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
