<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>333333 | Jun Kang&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="">
<meta name="author" content="Jun Kang">
<link rel="canonical" href="http://localhost:1313/tags/333333/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/tags/333333/index.xml">
<link rel="alternate" hreflang="en" href="http://localhost:1313/tags/333333/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Jun Kang&#39;s Blog (Alt + H)">Jun Kang&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>
    333333
  </h1>
</header>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">[PostgreSQL] 대량 데이터 인서트 시 성능 개선 및 주의 사항
    </h2>
  </header>
  <div class="entry-content">
    <p>최초 서비스 배포나 데이터 마이그레이션을 할 때 대량의 데이터를 한 번에 인서트 하는 경우가 있다. PostgreSQL 공식문서에서는 대량 인서트 시에 효율적으로 진행할 수 있는 방법을 제시해 준다. (대량 데이터를 인서트 할 때 효율적인 설정이지 데이터베이스 조회나 업데이트 등실제 운영 시에 사용할 방법은 아니다.)## 1. Autocommit 옵션 해제대량의 인서트 실행 시, Autocommit 옵션을 해제하고 한 트랜잭션에서 작업 후에 커밋을 진행해야 한다(일반적으로 SQL를 실행 시에 자동으로 시작 시 BEGIN, 끝날 때 COMMIT으로 트랜잭션 처리가 되지만, 확실히 되고 있는지 확인필요하다.). 대량 데이터 인서트의 각각을 별도로 commit 한다면, PostgreSQL은 인서트 되는 각 열에 대해 너무 많은 작업을 수행하게 된다. 또한 모든 인서트를 한 트랜잭션에 처리할 경우에는 한 INSERT가 실패할 경우 그 시점까지 인서트 된 모든 작업이 취소되기에 실패 작업에 대한 부분 보완 및 무결성을 고려하지 않아도 된다.## 2. COPY, PREPARE 사용INSERT를 여러 번 실행시키기보다 COPY 커맨드 한 번으로 해결하라. COPY 명령어는 많은 ROW를 로드하는데 최적화되어 있다. INSERT 구문보다 덜 유연하지만, 대량 데이터를 로딩하는데 훨씬 적은 오버헤드를 발생시킨다. (COPY는 단일 명령어이기에 실행 시 Autocommit을 따로 비활성화시킬 필요 없다.) COPY를 사용할 수 없는 상황이라면 PREPAPRE 구문을 통해 준비된 INSERT 구문을 EXECUTE를 통해 필요한 만큼 실행시키는 방법도 있다. 이 방법은 INSERT 구분을 반복적으로 사용할 때 드는 파싱과 실행계획의 오버헤드를 줄여준다.COPY를 사용한 대량 데이터 로딩은 INSERT보다 거의 모든 경우에 더 빠르다. (PREPARE 구문을 사용하고 단일 트랜잭션에서 배치를 통해 INSERT를 한다고 해도 COPY를 사용하는 것이 더 빠르기 때문에 가능하다면 COPY를 사용하는 것이 유리하다.) COPY는 해당 테이블의 CREATE TABLE 혹은 TRUNCATE 명령어와 같이 쓸 때 더 빠르다. 이 경우에 에러가 날 경우에 최신으로 로드된 데이터를 포함하고 있는 파일은 무조건 삭제되기 때문에 WAL write가 필요 없다. (WAL의 개념은 다음 포스트를 참고)https://junhkang.tistory.com/66[PostgreSQL] WAL (Write-Ahead Logging) / 아카이브 모드 백업(Archive mode backup)의 개념 및 장단점1. WAL (Write-Ahead Logging) / 아카이브 모드 백업(Archive mode backup)이란? 아카이브 모드 백업을 이해하기 위해 WAL에 대한 개념을 먼저 살펴보자. WAL은 PostgreSQL에서 데이터의 무결성을 보장하는 표준 방junhkang.tistory.com그러나 wal_level이 minimal로 설정되어 있을 경우에만 유효하다.## 3. 인덱스를 삭제하라아얘 새로운 테이블을 생성하는 상황이라면, 가장 빠른 방법은 테이블을 만들고 COPY를 사용해서 테이블 데이터를 채우고 테이블에 필요한 인덱스를 그 후에 생성하는 것이다. 이미 존재하는 데이터에 인덱스를 생성하는 것이 데이터를 인서트 하면서 인덱스를 생성하는 것보다 더 빠르다.기존 테이블에 데이터를 인서트 하는 상황이라면, 인덱스를 삭제하고, 데이터를 채우고, 인덱스를 다시 생성하는 것이 유리하다. 물론 인덱스가 사리진 기간 동안 해당 데이터에 접근하는 다른 사용자들은 성능 이슈를 겪을 것이고 Unique 인덱스를 drop 하는 경우에는 인덱스가 누락된 동안 무결성에 영향을 줄 수 있기에 작업과정에서의 영향도를 철저히 확인해야 한다.## 4. FK 제약을 삭제하라인덱스와 동일하게, FK는 벌크로 한 번에 체크하는 것이 row단위로 체크하는 것보다 효율적이다. 그래서 FK제약을 작업 전에 삭제하고, 데이터를 넣고, FK제약조건을 다시 생성한다. 인덱스와 동일하게 인서트 속도가 향상되는 반면 제약조건이 없는 사이에 데이터에 대한 무결성이 깨질 수 있다.이미 존재하는 FK 제약조건이 있는 테이블에 데이터를 넣을 때, FK 제약조건을 체크하는 행위가 서버의 pending trigger 이벤트 목록에 추가된다. 수백만건의 데이터를 인서트 하는 경우에 트리거 이벤트 큐의 가용 메모리를 초과하여 적정량 이상의 메모리 스왑이 발생하거나 실행명령이 완전히 실패할 수도 있다. 그래서 많은 양의 데이터를 인서트 할 때는 FK 조건을 삭제하고 다시 설정해야 한다. 제약조건을 일시적으로 해제할 수 없다면, 더 작은 트랜잭션 단위로 분할하는 것이 유일한 방법일 수도 있다.## 5. maintenance_work_mem 증가일시적으로 maintenance_work_mem 설정 값을 늘리는 것은 대량 데이터를 로딩 시 성능을 향상할 수 있다. 이 옵션은 CREATE INDEX, ALTER TABLE ADD FOREIGN KEY 명령어의 속도를 올려준다. COPY 자체의 속도를 올려주진 않기에 CREATE INDEX, ALTER TABLE ADD FOREIGN KEY를 사용할 때만 유효하다.## 6. max_wal_size 증가일시적으로 max_wal_size 설정값을 늘리는 것은 대량 데이터 로딩을 더 빠르게 해 준다. PostgreSQL에서의 대량 데이터 로딩은 일반적인 checkpoint 체크(checkpoint_timeout 옵션에 해당하는) 보다 더 잦은 checkpoint확인을 요한다. checkpoint가 발생할 때마다, 모든 부적절한 pages가 디스크에서 flush 된다. max_wal_Size를 늘림으로써 필요한 checkpoint의 빈도를 낮출 수 있다.## 7. WAL Archival, Streaming Replication을 미사용WAL Archiving이나 Streaming replication을 사용하는 경우 대량 데이터를 로딩할 때, 급격히 증가하는 WAL 데이터를 처리하는 것보다 로드가 완료된 후 새로운 백업을 실행하는 것이 유리하다. 데이터 로딩 중에 WAL 로깅이 증가하는 것을 방지하기 위해 WAL Archiving, streaming replication을 중지한다. (다음 3가지 옵션을 통해 설정가능)- wal_level = minimal- archive_mode = off- max_wal_senders = 0변경 후 서버를 재시작해야 하고, 기존의 기본 백업 정보들을 아카이브 복구나 standby서버로 사용이 불가능하게 된다. 데이터 손실로 이어질 수 있기 때문에 기존 백업 데이터에 대한 확인이 필요하다.해당 옵션을 적용하면 Archiver 시간이나 WAL 샌더가 WAL데이터를 처리하는 시간을 줄여주는 것뿐만 아니라, 이 작업을 하면 실제로 특정 명령어들의 실행 속도를 더 빠르게 해 준다. WAL_LEVEL 이 mininal일 때, 현재 트랜잭션 혹은 상위 트랜잭션에서 테이블 변경 혹은 인덱스 생성/삭제할 때 WAL을 전혀 작성하지 않기 때문이다. (WAL 아카이빙을 하지 않아도, 마지막에 fsync 실행으로 충돌을 더 효율적으로 방지할 수 있다.)## 8. 작업 후 ANALYZE 실행테이블 내의 데이터 분포를 크게 변경하는 경우에는 ANALYZE를 실행시켜야 한다. 대량 데이터를 인서트 할 때도 유효하며, ANALYZE 혹은 VACUUM ANALZYE를 실행시키면 플래너가 테이블의 최신 통계를 가져오는 것을 확인할 수 있다. 정확하지 않은 통계나 수집되지 않은 통계가 있을 때 플래너는 쿼리 실행계획을 비효율적으로 세울 수 있고, 이는 테이블의 성능저하를 유발한다. (Autovacuum 데몬이 실행 중이라면 Analyze를 자동으로 실행하고 있을 것이다.)## 9. pg_dump 확인 시 주의사항pg_dump에 의해 생성된 Dump script는 대량 데이터 인서트시에 위의 가이드라인(18)의 일부만을 자동으로 적용시킨다. pg_dump의 덤프를 최대한 빨리 복구하려면, 몇몇 추가 세팅을 수동으로 해야 한다. (이 작업은 dump를 복구하는데 적용되는 거지, 생성할 때 적용되는 것이 아니다.)Default로, pg_dump는 COPY를 사용하며(가이드 2번 적용), 완벽한 schema-and-data 덤프를 생성할 때는 인덱스 및 외부키를 생성하기 전에 데이터를 로드 (가이드 3,4 적용) 하기 때문에 몇몇 가이드라인이 자동으로 적용되고, 유저는 다음 항목들만 정의하면 된다.- maintenance_work_mem, max_wal_size를 적절한 값으로 설정- WAL archiving, streaming replication을 사용 중이라면, 덤프 복구 중에는 미사용을 고려- archive_mode = off, wal_level = minimal, max_wal_sender = 0을 덤프를 로딩하기 전에 설정하고, 덤프 복구 후에 원래 값으로 되돌리고 기본 백업을 최신으로 실행- pg_dump, pg_restore의 병렬 dump 및 복구 모드를 테스트해서 최적의 동시 job 실행 개수를 적용. (-j 옵션을 사용하여 덤프 및 복원을 적절하게 병렬로 수행하면 직렬보다 더 높은 성능 가능하다.)- 모든 덤프가 단일 트랜잭션으로 복구되도록 설정 (-1 혹은 –single-transaction 커맨드라인 옵션을 psql 혹은 pg_restore에 날리면 된다.) 다만, 이 모드를 사용하면, 아주 작은 에러라도 날 경우 전체 복구 과정이 롤백되어 몇 시간의 작업을 날릴 수도 있게 된다- DB서버에 여러 개의 CPU를 가용하는 것이 가능하다면 pg_restore의 –jobs 옵션을 쓸 수 있다. 이를 통해 병렬로 동시에 데이터를 인서트 하고 인덱스도 생성할 수 있다.- 작업 이후 ANALYZE를 실행시킨다data-only 덤프는 여전히 COPY를 사용하지만 인덱스를 삭제하거나 재생성하지 않고 FK에 영향을 주지 않는다. 그래서 data-only 덤프를 로딩할 때, 인덱스나 FK를 삭제한 후 다시 생성하는 방식을 사용할지 말지는 유저의 선택이다. 대량 데이터를 인서트 할 때와 동일하게, max_wal_size를 증가시키는 것은 유리하다, 하지만 data-only 덤프는 maintenance_work_mem을 늘리는 것에 영향을 받지 않는다. 그보다 인덱스와 FK키를 삭제 후 수동으로 다시 생성하는 것이 효율적이다.(* FK를 삭제하는 것 대신에 –disable-triggers 옵션으로 FK검증트리거 실행을 방지할 수 있지만, 단지 체크를 지연시키는 것으로 무결성에 위배되는 데이터가 들어올 수 있음은 동일하다)## 10. 결론 및 적용 검토실제로 산군의 데이터베이스는 수백만 건의 데이터를 마이그레이션 &amp; 백업하는 작업이 주기적으로 있다. 일부 작업의 경우 타 테이블을 참조하거나 대량 업데이트가 포함되어 있어 해당 테이블의 조회 성능에 직접적인 영향을 주는 경우가 있기에 트랜잭션을 분할하여 배치성으로 작업을 하거나, 특정 테이블을 격리 후 작업 및 동기화를 진행하고 있다. 인서트 시간이 길어지는 만큼 부담이 가는 작업인 만큼, 속도 향상을 위해 다양한 방법을 시도하고 있고, 공식문서에 나온 다음 가이드들을 추가 검토해 보게 되었다.18 가이드 중 직접적으로 적용 가능한 부분이 있을까?- 1. Autocommit 해제 - 작업은 기존에도 사용 중 (데이터 무결성, 작업 내용 검증 및 백업을 위해 트랜잭션 컨트롤은 필수)- 2. COPY, PREPARE - COPY 명령어도 사용 중이지만 한계가 있는 경우가 많고, 배치성으로는 이미 작업 중- 3, 4 인덱스, FK 삭제 후 재설정 - 실제 운영 중인 라이브 테이블에 인덱스, FK를 일시적으로 없애는 것은 효율적이지 못함- 5,6,7,8 PG옵션 추가 - 실제 운영 중인 DB에 설정값 적용 및 재부팅을 시도하는 것은 불가능간단하게 적용가능한 1,2번은 보통 다 사용중일 것이고, 전체적으로 운영 중인 데이터베이스에 실행하기엔 위험부담이 큰 작업들이 대부분이다(특히 58 옵션을 통해 DB자체를 다운시켜야 하거나 기존 백업을 사용할 수 없는 상황은 적용 불가). 대부분 최초 서비스 배포 혹은 리뉴얼 시에 적용 가능한 방법들로 보인다. 현재 운영 중인 데이터베이스의 특정 테이블을 격리하는 방식으로 34번이 부분적용이 가능해 보이기는 하지만, 인덱스를 삭제하고 데이터를 인서트시 코스트가 줄어들더라도 대량의 데이터가 들어간 후의 테이블에 인덱스를 추가하는 시간도 만만치 않을 것이다.운영 중인 데이터베이스에 대량 데이터를 인서트시 인덱스, FK에 대한 조정, 옵션값 변경 후 DB 재실행이 불가능한 상황이라면, 배치성, 트랜잭션 분할을 통해 (데이터베이스 성능과 데이터 무결성) 모니터링을 함께 진행하는 것이 가장 효과적일 듯하다.https://www.postgresql.org/docs/current/populate.htmlhttps://postgresql.kr/docs/9.6/wal-intro.html#PostgreSQL #대량 데이터 INSERT #DB성능향상
...</p>
  </div>
  <footer class="entry-footer">Jun Kang</footer>
  <a class="entry-link" aria-label="post link to [PostgreSQL] 대량 데이터 인서트 시 성능 개선 및 주의 사항" href="http://localhost:1313/posts/65/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">[PostgreSQL] 문자열내 중복 공백, 단어 제거
    </h2>
  </header>
  <div class="entry-content">
    <p> 특정 문자열에 대해서 중복 공백 제거를 하고 싶다면 postgresql 정규식을 사용해서 가능하다.(공백 외에 단일 문자에 대한 중복제거도 동일한 방법으로 가능하다.)select regexp_replace(name, &#39; &#43;&#39;, &#39; &#39;, &#39;g&#39;) from TABLE; -- &#39;g&#39; 옵션을 제거할 경우 최초 건에 대에서만 변경## 2. 중복 단어 제거&gt; 컬럼 단위 중복제거는 distinct, group by를 통해 쉽게 가능하지만, 컬럼 내 문자열의 중복 단어 제거의 경우 다음과 같다.(쉼표 기준으로 컬럼을 분리, 중복을 제거한 후 다시 연결)select id, array_to_string(array_agg(distinct token), &#39; &#39;) from (SELECT unnest(string_to_array(COLUMN, &#39; &#39;)) as token, id FROM TABLE) as tmpgroup by id## 3. 실습### 3-1. 테이블 생성-- 테스트 테이블 생성create table duplicate_test (id serial primary key,name varchar(255) not null);### 3-2. 테스트 데이터 insert-- 테스트 데이터 입력insert into duplicate_test(name)values(&#39;서울 서울 대구 서울 부산&#39;),(&#39;서울 서울 대구 서울&#39;),(&#39;부산 대구 대구 서울 서울서울 서울 광주&#39;),(&#39;서울 에서 대구 갔다가 부산 거쳐 다시 서울 로&#39;),(&#39;광주광주대구 대구 대 구 서울 &#39;),(&#39;서울 서울 서울 &#39;),(&#39;서울 대구 대 구 서울 &#39;),(&#39;서울 대구 대구 대 구 서울 &#39;),(&#39;서울 대구 대구 대구 부산부산 서울 부산 서울부 산&#39;);![](/images/posts/12/스크린샷 2023-10-04 오후 7.20.58.png)### 3-3. 중복 공백 제거select regexp_replace(name, &#39; &#43;&#39;, &#39; &#39;, &#39;g&#39;) from DUPLICATE_TEST; -- &#39;g&#39; 옵션을 제거할 경우 최초 건에 대에서만 변경select regexp_replace(name, &#39;단일문자열&#43;&#39;, &#39; &#39;, &#39;g&#39;) from DUPLICATE_TEST; -- 단일문자열에 대한 중복 제거도 동일한 방법으로 가능하다.![](/images/posts/12/스크린샷 2023-10-04 오후 7.20.03.png)### 3-4. 중복 단어 제거select id, array_to_string(array_agg(distinct token), &#39; &#39;) from (SELECT unnest(string_to_array(name, &#39; &#39;)) as token, id FROM DUPLICATE_TEST) as tmpgroup by id![](/images/posts/12/스크린샷 2023-10-04 오후 7.20.20.png)#SQL #PostgreSQL
...</p>
  </div>
  <footer class="entry-footer">Jun Kang</footer>
  <a class="entry-link" aria-label="post link to [PostgreSQL] 문자열내 중복 공백, 단어 제거" href="http://localhost:1313/posts/12/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">[PostgreSQL] 문자열에서 날짜/시간 변환 및 처리 과정
    </h2>
  </header>
  <div class="entry-content">
    <p>PostgreSQL의 날짜형태의 칼럼을 조회할 때, 종종 정확한 날짜 형태를 사용하는 것이 아닌, 문자열, 혹은 숫자 형태로 간편하게 조회하는 경우가 있다. 예를 들어 2024/05/02 이후의 값을 조회할 때 다음 두 가지 조회 방법을 사용할 수 있다.&gt; date_column &gt; ‘20240502’date_column &gt; TO_DATE(‘20240502’, ‘YYYYMMDD’)예제와 같이 PostgreSQL은 일련의 문자/숫자열을 조건에 맞는 날짜형으로 자동으로 디코딩을 해주는데, 문자열을 인식하는 상세 과정을 순서대로 알아보자.## 2. 문자열에서 날짜/시간으로의 디코딩 과정### 2-1. 문자열을 토큰으로 분리하고 각 토큰을 시간, 시간대, 또는 숫자로 분류한다.예제들에서는 정상적으로 날짜 및 시간이 변환되는지 확인하기 위해 강제로 TIMESTAMP 및 DATE로 형 변환을 하였지만, 날짜 형태의 데이터와 문자열 그대로를 비교하여도 날짜 및 시간 비교가 가능하다.- 숫자 토큰이 “:“를 포함한다면, 시간 문자열로 인식되며, 하나라도 발견되면 이후의 모든 숫자와 콜론은 시간 문자열의 일부로 취급SELECT &#39;20240202 13:45:30&#39;::TIMESTAMP![](/images/posts/88/스크린샷 2024-05-02 오후 5.36.09.png)13:45:30에 “:“를 포함하였으니 해당 토큰 전체를 시간 문자열로 취급하여, 13시 45분 30초로 해석된다. (hh:mm, hh:mm:ss 등의 표준규격에 맞는 경우에만)- 숫자 토큰에 하이픈(-), 슬래시(/) 또는 두개이상의 점(.)이 포함되어 있으면 날짜 문자열 취급SELECT &#39;2024-05-02&#39;::DATE;SELECT &#39;05/02/2024&#39;::DATE;SELECT &#39;02.05.2024&#39;::DATE;![](/images/posts/88/스크린샷 2024-05-02 오후 5.36.24.png)yyyy mm dd의 표준 규격에 맞는 경우 모두 날짜 형태로 해석된다.- 이미 날짜 토큰이 확인된경우 문자열을 시간대 이름 (ex America/New_York)으로 해석SELECT &#39;2023-12-25 America/New_York&#39;::TIMESTAMP WITH TIME ZONE;![](/images/posts/88/스크린샷 2024-05-02 오후 5.38.56.png)2023-12-25 America/New_York 은, 앞부분에서 이미 날짜 토큰이 확인되었기에, 뒷부분은 시간대 이름으로 해석된다.- 토큰이 숫자만으로 구성되어 있으면 단일필드이거나 ISO 8601 형식의 날짜로 해석된다.SELECT &#39;19990113&#39;::DATE;![](/images/posts/88/스크린샷 2024-05-02 오후 5.39.45.png)19990113 = 1999년 1월 13일 또는 (141516 = 14시 15분 16초)- 토큰이 &#43;, -로 시작하면 숫자 시간대 또는 특별필드이다.&#43;0200 : UTC보다 2시간 빠른 시간대-0500 : UTC보다 5시간 늦은 시간대&#43;15 : 현재 날짜로부터 15일 후 날짜-3 : 현재 날짜로부터 3일 전 날짜SELECT &#39;20240202 12:00 &#43;0200&#39;::TIMESTAMP WITH TIME ZONE;![](/images/posts/88/스크린샷 2024-05-02 오후 5.49.12.png)### 2-2. 토큰이 알파벳 문자열이라면 가능한 문자열 사전을 조회한다.- 토큰이 알려진 시간대 약어 중에 일치하는 게 있는지 확인SELECT &#39;20240202 12:00 PM EST&#39;::TIMESTAMP WITH TIME ZONE;![](/images/posts/88/스크린샷 2024-05-02 오후 5.41.50.png)“12:00 PM EST” 에서 EST는 동부표준시(Eastern Standard Time)의 약어로, UTC 오프셋 매핑 딕셔너리에 포함되어 있어 사용 가능- 발견된 문자열이 없으면 내부 테이블을 검색하여 토큰을 특별 문자열 (ex, today, Thursday, January) 혹은 at, on 같은 조사와 매칭시킨다SELECT &#39;Today&#39;::DATE &#43; 1;![](/images/posts/88/스크린샷 2024-05-02 오후 5.42.17.png)‘Today’::date &#43; 1 은 내일 날짜를 반환한다.- 문자열이 위 두조건에 부합하지 않는다면 에러 발생### 2-3. 토큰이 숫자, 숫자 필드로만 이루어져 있을 때- 6 , 8자리이며 다른 날짜 필드가 발견되기 전이라면 날짜 형태로 해석(YYYYMMDD 혹은 YYMMDD)SELECT &#39;20240201&#39;::DATESELECT &#39;240201&#39;::DATE20240201, 240201 모두 2024년 02월 01일로 해석된다.- 토큰이 3자리 숫자이고, 이미 연도가 발견되었다면, 그 해의 n번째 일수로 해석SELECT &#39;2024 021&#39;::DATE;![](/images/posts/88/스크린샷 2024-05-02 오후 5.43.23.png)‘2024 021’::date 은 2024년의 21번째 날로 2024년 01월 21일로 해석된다.- 네 자리 또는 여섯 자리 숫자이고 이미 날짜가 발견되었다면, 시간(HHMM 또는 HHMMSS)으로 해석SELECT &#39;20240502 123422&#39;::TIMESTAMP;![](/images/posts/88/스크린샷 2024-05-02 오후 5.44.27.png)‘20240502 123422’::timestamp 에서 앞부분 토큰에 날짜는 이미 발견되었기에, 6자리 숫자가 시간으로 해석된다. (2024년 05월 02일의 12:34:22)- 세 자리 이상의 숫자이고 아직 날짜 필드가 발견되지 않았다면, 연도로 해석 (기본적으로 yy-mm-dd 순서이며 서버의 DateStyle 설정에 따라 mm-dd-yy, dd-mm-yy 등으로 변경할 수 있다.SELECT &#39;240522&#39;::TIMESTAMP![](/images/posts/88/스크린샷 2024-05-02 오후 5.45.27.png)‘240522’::timestamp는 처음 발견된 날짜형 토큰이기에 기본 설정인 yy-mm-dd에 따라 해석된다. (2024년 05월 02일)- 월/일 필드가 범위를 벗어나거나 유효하지 않은 값이라면 오류 발생### 2-4. BC(기원전) 설정- bc 문자열을 통해 기원전 설정이 가능하며, BC(기원전)이 설정되어 있다면, 내부적으로는 연도를 음수로 바꾸고 1을 더한 후 저장한다. (그레고리력에는 연도 0이 없으므로 수치상 1 BC는 연도 0이 됨)SELECT &#39;bc 1200201&#39;::DATE![](/images/posts/88/스크린샷 2024-05-02 오후 5.46.24.png)- BC가 지정되지 않고 연도 필드가 두 자리 숫자인 경우, 연도를 4자리로 조정한다. 해당 필드가 70보다 작으면 2000을 더하고, 그렇지 않으면 1900을 더한다.- ‘800502’::date - 년도필드(80)가 70보다 크기에 1900을 더한 ‘1980년 05월 02일’을 의미- ‘240502’::date - 년도필드(24)가 70보다 작기에 2000을 더한 ‘2024년 05월 02일’을 의미SELECT &#39;800502&#39;::DATE ;![](/images/posts/88/스크린샷 2024-05-02 오후 5.47.10.png)SELECT &#39;240502&#39;::DATE ;![](/images/posts/88/스크린샷 2024-05-02 오후 5.47.27.png)참고https://www.postgresql.org/docs/16/datetime-input-rules.html#시간 #날짜 #PostgreSQL
...</p>
  </div>
  <footer class="entry-footer">Jun Kang</footer>
  <a class="entry-link" aria-label="post link to [PostgreSQL] 문자열에서 날짜/시간 변환 및 처리 과정" href="http://localhost:1313/posts/88/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">[PostgreSQL] 시퀀스(Sequence)의 개념과 사용법(생성, 삭제, 조회 등)
    </h2>
  </header>
  <div class="entry-content">
    <p>1-1. 생성, 삭제, 조회-- 101부터 시작하는 기본 시퀀스 생성CREATE SEQUENCE serial START 101;-- 시퀀스 다음값 조회SELECT nextval(&#39;serial&#39;);-- 시퀀스 현재값 조회select currval(&#39;serial&#39;);-- 시퀀스 삭제DROP SEQUENCE serial;-- 시퀀스로 INSERT하기INSERT INTO distributors VALUES (nextval(&#39;serial&#39;), &#39;nothing&#39;);-- COPY FROM 후에 시퀀스 시작값 변경하기BEGIN;COPY distributors FROM &#39;input_file&#39;;SELECT setval(&#39;serial&#39;, max(id)) FROM distributors;END;-- SynopsisCREATE [ TEMPORARY | TEMP ] SEQUENCE [ IF NOT EXISTS ] 이름[ AS 자료형 ][ INCREMENT [ BY ] 증가값 ][ MINVALUE 최소값 | NO MINVALUE ] [ MAXVALUE 최대값 | NO MAXVALUE ][ START [ WITH ] 시작값 ] [ CACHE 캐시 ] [ [ NO ] CYCLE ][ OWNED BY { 테이블이름.칼럼이름 | NONE } ]### 1-2. 사용 중인 시퀀스 확인select n.nspname as sequence_schema,c.relname as sequence_name,u.usename as ownerfrom pg_class cjoin pg_namespace n on n.oid = c.relnamespacejoin pg_user u on u.usesysid = c.relownerwhere c.relkind = &#39;S&#39;and u.usename = current_user;## 2. 시퀀스 생성시 상세 옵션- TEMPORARY or TEMP현재 세션에서만 사용하는 시퀀스를 생성하며, 세션이 종료되면 시퀀스는 자동 삭제된다.- IF NOT EXISTS동일명의 시퀀스가 있다면 알림만 보여주고 작업은 생략한다.- AS 자료형시퀀스의 자료형을 설정한다. smallint, integer, and bigint 세 종류로 bigint형이 기본값이다.- INCREMENT BY 증가값시퀀스 채번식 증가값을 더하여 구한다. 양수라면 증가 시퀀스, 음수면 감소시퀀스이다. default는 1이다.- MINVALUE / NO MINVALUE해당 시퀀스의 최솟값을 설정한다. 기본 값은 1이며, 감소 시퀀스의 경우 해당 자료형의 최솟값이다.- MAXVALUE / NO MAXVALUE해당 시퀀스의 최댓값을 설정한다. 기본값은 해당 자료형의 최댓값이다. 감소 시퀀스의 경우 -1이다.- START WITH시퀀스의 시작값을 설정한다. 기본값은 증가시퀀스의 경우 최솟값이며, 감소 시퀀스는 최댓값이다.- CACHE시퀀스 채번을 빠르게 하기 위해 메모리에서만 처리하는 캐시값이다.기본값은 1이며 캐시를 사용하지 않고 매번 디스크를 사용함을 뜻한다.- CYCLE / NO CYCLE시퀀스가 최댓값/최솟값에 도달했을 때 순환하며 다시 시작한다.NO CYCLE의 경우 최솟값/최댓값에 도달하면 에러로 처리한다.기본 설정은 NO CYCLE이다.- OWNED BY / OWNED BY NONE해당 칼럼과 시퀀스의 의존관계를 생성한다.테이블/칼럼이 삭제되면 시퀀스는 자동으로 삭제되며, 테이블/시퀀스의 소유자가 같아야 한다.OWNED BY NONE이 기본 옵션이며 어떠한 의존관계도 없는 상태이다.## 3. 시퀀스의 개념CREATE SEQUENCE는 일련번호 생성기인 SEQUENCE를 생성한다. 시퀀스를 생성하면, 내부적으로 지정한 명칭으로 단일 로우의 특수 테이블을 만들고, 그 로우의 값을 초기화한다. 시퀀스는 특수 용도의 “테이블” 이기 때문에 다음과 같은 쿼리를 사용할 수 있지만,SELECT * FROM seq_name;![](/images/posts/23/스크린샷 2023-10-11 오후 1.28.47.png)이 테이블을 직접 조작하면 안되며, 결과에서 last_value (nextval)은 “실행 시점&#34;의 최신 값이다. (그 후로는 다른 세션에서 호출되어 바뀌었을 수도 있는 값이다.)## 4. 시퀀스의 특징- 시퀀스명은 시퀀스 / 테이블 / 인덱스 / 뷰 명과 겹칠 수 없다.- 시퀀스는 기본적으로 bigint형으로 계산한다. (-9223372036854775808 ~ 9223372036854775807)- nextval, setval은 취소가 되지 않는다. 때문에 연속되지 않는 일련번호를 처리하는 용도로 사용할 수 없고, 락을 통해 구현은 가능하지만 시퀀스를 사용하지 않는 것보다 더 높은 코스트가 소모된다. 특히 동시 접속 많은 서비스라면 더 비효율적이다.## 5. CACHE 옵션### 5-1. CACHE 옵션이란?CACHE 옵션을 사용하는 경우 다중 세션에서 시퀀스가 순차적으로 채번 되지 않는다. 각세션 별로 캐시값만큼 생략 후 그다음 세션시작번호를 채번 한다. (last_Value &#43;캐시-1) 즉 시퀀스 캐시는 한 세션 내의 캐시를 의미한다. 이를 통해 다른 세션에서 이전 세션에서 미사용 한 일련번호를 사용할 수 없도록 하고, 그렇기 때문에 시퀀스가 연속적이지 않는 경우가 종종 발생한다.### 5-2. CACHE 옵션 사용시 주의 사항캐시를 사용하면 시퀀스가 크다는 의미가 꼭 나중에 생성된 데이터라는 보장이 없다. 그렇기에 순차적 시퀀스를 사용해야 하는 경우라면 캐시값을 1로 설정해야 만한다. 예를 들어&gt; 캐시값은 10으로 설정할 경우- A 세션이 110 시퀀스를 선점- B 세션이 1120 시퀀스를 선점이 상태에서 B세션에서 더 빨리 시퀀스를 호출하더라도 A세션의 1~10 시퀀스보다 낮은 값을 가질 수 없기 때문이다.(CACHE를 사용하더라도 유니크한 시퀀스를 채번함에는 전혀 지장이 없기에, 순차적 의미로써 시퀀스를 사용할 것이 아니라면 사용하여도 무관하다.)참고https://www.postgresql.kr/docs/10/sql-createsequence.html#Sequence #PostgreSQL</p>
  </div>
  <footer class="entry-footer">Jun Kang</footer>
  <a class="entry-link" aria-label="post link to [PostgreSQL] 시퀀스(Sequence)의 개념과 사용법(생성, 삭제, 조회 등)" href="http://localhost:1313/posts/23/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">[PostgreSQL] 외래키(Foreign Keys) 개념, 사용법, 장단점, 적용검토
    </h2>
  </header>
  <div class="entry-content">
    <p>Foreign key constraint 외래키 제약은 특정 칼럼 혹은 칼럼들의 값이 다른 테이블의 특정 row와 매칭되어야 하는 제약조건이다. 이를 두 관련 테이블 사이의 참조 무결성 (referential integrity)를 유지한다고 말한다. 그렇게 복잡한 개념은 아니니 바로 사용법을 확인해 보도록 하자## 2. 예제### 2-1. 기본 외래키(Foreign Keys) 생성products 테이블은 물품의 이름, 가격 정보 테이블이고, orders 테이블은 존재하는 물품 각각에 대한 순서 정보가 들어있는 테이블이다. orders, products 테이블의 product_no에 외래키 제약을 적용하는 예제이다.CREATE TABLE products (product_no integer PRIMARY KEY,name text,price numeric);CREATE TABLE orders (order_id integer PRIMARY KEY,product_no integer REFERENCES products (product_no),quantity integer);orders 테이블의 제약조건을 위와 같이 주었을 때 products 테이블에 없는 product_no로는 데이터 생성이 불가능하다. 이 경우 다음과 같이 명칭 한다.- orders - referencing(참조하는) 테이블- products - referenced(참조된) 테이블### 2-2. 칼럼을 지정하지 않은 외래키(Foreign Keys)CREATE TABLE orders (order_id integer PRIMARY KEY,product_no integer REFERENCES products,quantity integer);특정 칼럼을 지정하지 않는다면 reference 칼럼으로 Primary Key에 해당하는 칼럼을 자동으로 사용하기에 별도로 칼럼명을 명시하지 않아도 된다. (PK가 바뀔 일은 거의 없겠지만) 테이블 구조가 바뀔 수 있고, 명확한 제약조건을 명시하는 것이 좋기에 칼럼을 지정하는 것이 좋다.### 2-3. 복합 칼럼 외래키(Foreign Keys)FK 제약조건을 여러 개의 칼럼을 대상으로도 사용할 수 있다.CREATE TABLE t1 (a integer PRIMARY KEY,b integer,c integer,FOREIGN KEY (b, c) REFERENCES other_table (c1, c2));물론 참조하는 칼럼과 참조되는 테이블의 칼럼 수는 일치하여야 한다.### 2-4. 자기 참조 외래키(Self-referential Foreign Keys)종종 다른 테이블이 아니라 같은 테이블 내의 칼럼의 FK로 두는 것이 효율적일 때가 있다. 이를 자기 참조 외래키 (self-referential foreign key)라고 한다. 예를 들어 트리 구조의 노드들을 테이블 row로 표현하고 싶을 때, 다음과 같이 parent_id를 node_id에 참조시키면 된다.CREATE TABLE tree (node_id integer PRIMARY KEY,parent_id integer REFERENCES tree,name text,...);최상위 노드의 parent_id는 null이 될 것이고, parent_id가 null이 아닌 항목들은 해당 테이블의 유효한 노드를 참조하도록 제한된다.### 2-5. 다중 외래키(Foreign Keys)한 개의 테이블은 여러 개의 FK를 가질 수 있으며 이는 다대다 (many-to-many) 테이블 관계 구현에 사용된다. 기존 예제의 상품, 상품순서 구조에 추가로, 한 순서에 많은 상품을 포함할 수 있게 한다면 다음과 같은 테이블 구조를 사용할 수 있을 것이다.CREATE TABLE products (product_no integer PRIMARY KEY,name text,price numeric);CREATE TABLE orders (order_id integer PRIMARY KEY,shipping_address text,...);CREATE TABLE order_items (product_no integer REFERENCES products,order_id integer REFERENCES orders,quantity integer,PRIMARY KEY (product_no, order_id));​## 3. 외래키(Foreign Keys) 옵션앞서 말한 대로, FK 제약조건이 걸려있다면 참조되지 않은 값으로는 데이터 생성이 불가능하다. 그러나 참조된 orders가 생성된 후에 product가 삭제되면 어떻게 될까? Postgresql에서는 다음 상황들 중에 선택적으로 사용이 가능하다.- 참조하는 데이터(orders)가 있을 경우 삭제 불가- 참조하는 데이터(orders)까지 함께 삭제- 등등…데이터를 처리하는 상황을 선택하는 ON DELETE, ON UPDATE 등의 옵션과, 처리하는 방식을 선택하는 RESTRICT, CASCADE 등의 옵션을 조합하여 원하는 결과를 만들면 된다.다음 예제는 서로 다른 테이블에 참조된 칼럼 각각의 값이 삭제됐을 때 참조된 상위 값이 있을 때 각각 같이 삭제할지, 삭제를 방지할지를 설정한 테이블 생성 쿼리이다.CREATE TABLE order_items (product_no integer REFERENCES products ON DELETE RESTRICT,order_id integer REFERENCES orders ON DELETE CASCADE,quantity integer,PRIMARY KEY (product_no, order_id));다음과 같이 설정하면 order_items에서 product_no를 삭제하려 할 때 참조하는 데이터가 있을 경우 삭제가 불가능하고, order_id의 경우 참조하는 데이터와 함께 삭제가 된다. (RESTRICT, CASCADE는 가장 기본적으로 사용되는 옵션)### 3-1. RESTRICTRestrict는 참조하는 열의 삭제를 방지한다. 참조하는 오브젝트가 존재할 시 실행 자체를 실패한다.### 3-2. NO ACTIONNO ACTION은 제약조건을 선택할 때 참조 행이 존재한다면 오류가 발생하고, 지정하지 않을 시에는 기본 동작(RESTRICT / CASCADE)이 된다. NO ACTION을 선택하나, NO ACTION을 선택하지 않고 RESTRICT를 설정하나 실행이 되지 않는 건 똑같지만, 본질적으로 NO ACTION은 무결성 체크하는 시점을 트랜잭션의 후반부까지 연기할 수 있다는 차이점이 있다. (DEFERRABLE, NOT DEFERRABLE 옵션으로 제어 가능하다.)### 3-3. CASCADE폭포 혹은 단계적인 이라는 의미로, 연관된 데이터의 일괄적인 적용을 의미한다. CASCADE는 참조하는 열에도 함께 변경을 가한다.두 가지 옵션을 보면- SET NULL- SET DEFAULT이 옵션은 참조하는 테이블의 칼럼값(order_id)을 null로 변경할지, default 값으로 변경할지 선택하는 옵션이다.SET NULL, SET DEFAULT는 FK가 추가적인 정보를 나타낼 때 적절하다. 예를 들어, 위 예제에서 products 테이블에서 product manager의 정보가 참조되고 있고, product manager가 삭제될 때 product에 해당 참조 데이터를 null 혹은 default로 자동으로 바꿔준다면 별도의 추가 명령 없이 관리할 수 있을 것이다.하지만 SET NULL, SET DEFAULT가 참조 테이블의 다른 제약조건들까지 먼저 확인하지는 않기에, 만약 SET DEFAULT로 설정했는데 DEFAULT 값이 다른 제약조건에 부합하지 않을 경우 해당 동작은 실패한다.CREATE TABLE tenants (tenant_id integer PRIMARY KEY);CREATE TABLE users (tenant_id integer REFERENCES tenants ON DELETE CASCADE,user_id integer NOT NULL,PRIMARY KEY (tenant_id, user_id));CREATE TABLE posts (tenant_id integer REFERENCES tenants ON DELETE CASCADE,post_id integer NOT NULL,author_id integer,PRIMARY KEY (tenant_id, post_id),FOREIGN KEY (tenant_id, author_id) REFERENCES users ON DELETE SET NULL (author_id));위 예제 테이블을 보면 posts의 데이터를 삭제할 때, FK 제약조건으로 참조된 tenant_id를 null로 변경하려 하지만, tenant_id는 PK의 일부로 null이 될 수 없기에 실행되지 않는다.### 3-4. ON DELETEON DELETE 옵션은 테이블에 연관된 오브젝트의 유형에 따라 적절하게 사용되어야 한다. 참조하는 테이블이 참조된 테이블 값의 구성요소이며 독립적으로 존재할 수 없다면 CASCADE 옵션을 적용하여 한번에 처리하는 것이 적절하고, 두 테이블의 오브젝트가 독립적인 관계라면 RESTRICT, NO ACTION이 적합하다.예를 들어, 위의 예제에서 order_items는 orders의 일부분이고 독립적으로 사용될 일이 없기에 orders가 삭제될 때 자동으로 지워지는 것이 편할 것이다. orders와 products는 독립적으로 사용할 여지가 있기에 삭제 시 products를 자동으로 지우는 건 문제가 될 수 있다.### 3-5. ON UPDATEON DELETE와 유사하게 참조열이 업데이트될 때 호출되는 ON UPDATE도 있다. SET NULL, SET DEFAULT의 설정이 다른 제약조건에 위배된다면 적용할 수 없다는 점은 동일하다. CASCADE와 사용 시 참조하는 칼럼의 업데이트된 값이 참조된 열로 복사된다.## 4. 인덱스FK 제약조건은 PK이거나, Unique 제약조건(UK)이거나, “복합 인덱스의 일부가 아닌” 칼럼을 참조해야만 한다. 이 뜻은 참조된 칼럼은 인덱스를 항상 가지고 있다는 뜻이다. 하지만 참조하는 칼럼은 인덱스가 필수이거나 인덱스를 자동으로 생성하지 않는다. 열의 DELETE, UPDATE는 참조하는 테이블에서 이전 값을 찾아야 하기에, 참조하는 칼럼에도 인덱스를 설정하는 것이 유리한 경우도 있다.## 5. 장단점### 5-1. 장점- 데이터의 무결성- 쉬운 데이터 구조/관계 확인- update, delete 등의 로직 간소화### 5-2. 단점- 참조 테이블들을 스캔하는데 드는 추가 코스트- 테이블 구조 변경 시 고려해야 할 사항 증가- 데이터 변경이 찾을 때 특히 코스트가 증가- 테스트 데이터 생성 및 데이터 강제 보정이 번거로움## 6. 적용 검토이미 FK를 사용하는 테이블도 다수 존재한다. 중요 기준 데이터의 무결성을 보장하는 데는 효율적인 기능이라고 생각한다. 다만 “실시간으로 업데이트되는 대용량 데이터의 경우에도 무결성 보장을 위해 FK를 적용해야 하는가?“에 대한 판단을 위해 FK에 대한 내용을 좀 깊게 들여다보았다. 일단 결론은, 실시간으로 변동되는 대용량 데이터 베이스, 특히 타 시스템과의 동기화가 이루어지고 있는 산군의 데이터베이스에는 적합하지 않다고 판단된다.“FK의 스캔 코스트가 성능에 큰 영향이 없다”, 혹은 “적절한 인덱싱으로 스캔 코스트를 관리할 수 있다.“라는 상황 자체가 참조 테이블의 인덱스를 전제한다. FK만을 위한 인덱스를 참조된 테이블에 추가하거나, 참조된 테이블에 업데이트나 삭제가 발생하여 참조하는 테이블에서 이전 값을 조회하기 위해 역스캔하는 경우의 코스트를 줄이기 위해 인덱스를 추가해야 하는 상황이 발생한다.현재 운영 중인 데이터베이스는 최소 수백만 건에서 수천만건의 테이블이 서로 연결되어 있으며 read/write 간 동기화, ElasticSearch 등으로의 동기화를 고려하여 최적의 인덱스로 세팅, 되어있고 실시간 모니터링을 통한 튜닝이 지속적으로 진행 중이다. 모든 칼럼에 인덱스를 거는 것이 효율적이지 않은 것처럼, 한정된 자원으로 최적의 인덱싱을 찾아야 하고, 데이터의 변경에 민감하게 대응해야 하는 상황에서는 FK를 설정하는 것이 효율적이지 못하다는 판단이다.또한, 참조되는 테이블의 추가 인덱스 설정 없이 FK를 설정하는 것만으로는 코스트가 증가하지 않지만 실시간으로 몇천 건, 많게는 몇만 건의 데이터가 업데이트 및 동기화되는 상황에서 테이블의 Lock이 여러 테이블로 전파될 위험성도 무시할 수 없다.참고 : https://www.postgresql.org/docs/16/ddl-constraints.html#DDL-CONSTRAINTS-FK#fk #PostgreSQL #Foreign Keys
...</p>
  </div>
  <footer class="entry-footer">Jun Kang</footer>
  <a class="entry-link" aria-label="post link to [PostgreSQL] 외래키(Foreign Keys) 개념, 사용법, 장단점, 적용검토" href="http://localhost:1313/posts/64/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">[PostgreSQL] 윈도우 함수(Window Functions)의 개념, 성능 및 사용법 (over, sum/rank/ntitle/cume_dist 등...)
    </h2>
  </header>
  <div class="entry-content">
    <p>윈도우 함수는 행과 행 간의 관계를 쉽게 정의하기 위해 만든 함수이다. 이 기능은 일반 집계함수의 연산과 유사하지만, 일반 집계함수가 행 각각을 단일 그룹화해서 출력하는 반면에, 윈도우 함수는 각각의 행들이 그룹화되지 않으며 별도의 ID를 가진다. 그렇기에 윈도우 함수는 현재 row의 정보보다 더 많은 정보에 접근이 가능하다. 예를 들면 다음과 같다.&gt; 일반집계함수 : COUNT() &#43; GROUP BY-&gt; 그룹별 1개의 행 출력 (그룹 개수만큼 출력, 자르기 &#43; 집약)윈도우집계함수 : COUNT() OVER (PARTITION BY) -&gt; ID개수만큼 행 출력 (행의 개수가 줄어들지 않는다, 자르기)다음의 공식문서 예제를 보며 윈도우 함수가 어떻게 작동하는지 알아보자. 임직원의 월급, 부서, 직원번호가 포함된 empsalary 테이블이 있다.SELECT depname, empno, salary, avg(salary) OVER (PARTITION BY depname) FROM empsalary;depname | empno | salary | avg-----------&#43;-------&#43;--------&#43;-----------------------develop | 11 | 5200 | 5020.0000000000000000develop | 7 | 4200 | 5020.0000000000000000develop | 9 | 4500 | 5020.0000000000000000develop | 8 | 6000 | 5020.0000000000000000develop | 10 | 5200 | 5020.0000000000000000personnel | 5 | 3500 | 3700.0000000000000000personnel | 2 | 3900 | 3700.0000000000000000sales | 3 | 4800 | 4866.6666666666666667sales | 1 | 5000 | 4866.6666666666666667sales | 4 | 4800 | 4866.6666666666666667(10 rows)첫 3개의 컬럼은 테이블의 데이터를 바로 사용하는 것이고, row 당 1개의 값을 가지고 있다. 4번째 컬럼은 같은 부서명의 ROW 끼리의 평균 월급을 나타낸다. (비윈도우 함수의 avg 함수와 동일하지만, over 구문을 사용할 경우 윈도우 함수로 취급받고, window frame 상에서 연산될 수 있게 해 준다.)윈도우 함수는 함수명, 혹은 변수 뒤에 항상 over를 바로 뒤에 붙여 사용한다. over 구문은 쿼리의 row들이 윈도우 함수에 의해 정확히 어떻게 분할되어 작동하는지에 대한 결정을 내린다. over 내의 partition by 구분은 동일한 값을 공유하는 groups 혹은 partitions으로 행을 분할한다. 이렇게 분할된 파티션 상에서 각 행과 동일한 파티션에 속하는 행끼리 연산하게 된다. over 내에 order by를 통해 윈도우 함수에 통과시킬 row의 순서를 정할 수 있다.SELECT depname, empno, salary,rank() OVER (PARTITION BY depname ORDER BY salary DESC)FROM empsalary;depname | empno | salary | rank-----------&#43;-------&#43;--------&#43;------develop | 8 | 6000 | 1develop | 10 | 5200 | 2develop | 11 | 5200 | 2develop | 9 | 4500 | 4develop | 7 | 4200 | 5personnel | 2 | 3900 | 1personnel | 5 | 3500 | 2sales | 1 | 5000 | 1sales | 4 | 4800 | 2sales | 3 | 4800 | 2(10 rows)rank 함수는 해당 파티션 당 order by 값에 맞는 숫자 형태의 순위를 나타낸다. rank는 over 절에 의해서만 결정되기에 명시적인 매개 변수가 추가로 필요하지 않다.윈도우 함수는 from 절의 테이블에서 where, group by 그리고 having 절로 필터링된 “가상 테이블&#34;의 행을 대상으로 작동하기에 조건에 부합하지 않아 제거된 row는 윈도우 함수 내에서 사용되지 않는다. 쿼리에 다양한 over 절을 사용하여 데이터를 분할할 수 있지만, 이 가상 테이블에 정의된 row를 대상으로 동일하게 작동한다. 행의 순서가 중요하지 않은 경우, order by를 생략해도 되는 것처럼, 단일 파티션이 전체 row를 포함하는 경우 partition by를 생략할 수도 있다.### 1-1. Window frame윈도우 함수에 관한 중요한 개념 중 하나는 window frame이다. window frame이라고 불리는 row의 집합이 파티션 내에 존재한다. 몇몇 윈도우 함수는 전체 파티션이 아닌, window frame의 row에 대해서만 동작한다. 기본적으로 ORDER BY를 사용하면 frame은 시작 행부터 현재 행까지의 정보로만 구성되며, order by 가 생략되면, 기본 frame은 파티션 내의 전체 row로 이루어진다. 다음 sum의 예제를 보면SELECT salary, sum(salary) OVER () FROM empsalary;salary | sum--------&#43;-------5200 | 471005000 | 471003500 | 471004800 | 471003900 | 471004200 | 471004500 | 471004800 | 471006000 | 471005200 | 47100(10 rows)over 절에 order by가 없기에, window frame은 파티션 전체와 같고, 각 sum은 전체 테이블을 조회하여 일반 집계 함수와 동일한 결과를 가진다. 하지만 order by 가 들어갈 경우 결과가 달라진다. 아래 쿼리는 월급의 최저값 ROW부터 현재 ROW까지 (파티션의)의 합계이다.SELECT salary, sum(salary) OVER (ORDER BY salary) FROM empsalary;salary | sum--------&#43;-------3500 | 35003900 | 74004200 | 116004500 | 161004800 | 257004800 | 257005000 | 307005200 | 411005200 | 411006000 | 47100(10 rows)### 1-2. 제약조건위도우 함수는 SELECT와 ORDER BY 절에서만 허용된다. group by, having, where 절 같은 곳에서는 사용이 불가능하다.논리적으로 해당 조건들을 모두 조회한 후에 작동하기 때문이다.그리고 윈도우 함수는 비윈도우집계함수 이후에 실행된다. 즉 윈도우 함수의 인수에 일반 집합 함수 호출을 포함하는 것은 가능하지만, 그 반대의 경우는 불가능하다. 만약 윈도우 함수의 연산 후에 filter 혹은 group by를 할 경우, 서브쿼리를 사용해야 한다. 아래와 같이 사용하면 내부 쿼리의 순위가 3 이하인 row 들만 보여준다.SELECT depname, empno, salary, enroll_dateFROM(SELECT depname, empno, salary, enroll_date,rank() OVER (PARTITION BY depname ORDER BY salary DESC, empno) AS posFROM empsalary) AS ssWHERE pos### 1-3. WINDOW AS쿼리가 만약에 다수의 윈도우 함수를 포함한다면, 각각이 OVER문으로 작성하는 것이 가능하지만, 여러 함수에 대해 동일한 윈도우 설정 동작을 하는 경우 중복되고 에러가 발생하기 쉽다. 이럴 경우 WINDOW에 해당하는 레퍼런스를 설정하고 해당 값을 over에서 사용 이 가능하다.SELECT sum(salary) OVER w, avg(salary) OVER wFROM empsalaryWINDOW w AS (PARTITION BY depname ORDER BY salary DESC);### 1-4. 성능윈도우 함수를 사용할 경우 집계, 순위 등의 쿼리를 편하게 사용할 수 있고, 테이블의 스캔 횟수도 훨씬 줄어든다. 다만 파티션 내 다른 행과 현재행의 관계정보로 다루어지기에, 윈도우 함수를 사용할 시 기본적으로 정렬하는 과정에서 자원이 소모된다. 테이블 및 데이터 정보에 따라 달라지겠지만, 분포율이 5~7%정도 되는 1200만 건의 데이터를 기준으로 윈도우 함수와 group by 정렬을 비교해 보았다.![](/images/posts/40/img_1.png)![](/images/posts/40/스크린샷 2023-10-31 오후 1.51.46.png)실제로 윈도우 함수를 포함한 경우 sort 과정에 자원이 많이 소모되어 데이터가 많을 경우 오히려 비윈도우 함수보다 효율이 좋지 않았다. 따라서 기능의 편의성 외에도 데이터의 양이나 테이블 구조에 맞춰 윈도우 함수를 사용하고, 서브쿼리나 조건절 튜닝을 통해 스캔해야할 행의 갯수를 줄인 후 사용하는 방법을 고려해야 한다.## 2. 윈도우 함수의 종류 및 사용법### 2-1. 일반집계함수- SUM - 파티션별 윈도우의 합계SELECT MGR, ENAME, SAL, SUM(SAL) OVER (PARTITION BY MGR ORDER BY SAL RANGE UNBOUNDED PRECEDING) as MGR_SUMFROM EMP;- MAX - 파티션별 윈도우의 최댓값SELECT MGR, ENAME, SAL, MAX(SAL) OVER (PARTITION BY MGR) as MGR_MAXFROM EMP;- MIN - 파티션별 윈도우의 최솟값 SELECT MGR, ENAME, HIREDATE, SAL, MIN(SAL) OVER(PARTITION BY MGR ORDER BY HIREDATE) as MGR_MINFROM EMP;- AVG - 파티션별 윈도우의 평균값SELECT MGR, ENAME, HIREDATE, SAL, ROUND (AVG(SAL) OVER (PARTITION BY MGR ORDER BY HIREDATE ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)) as MGR_AVGFROM EMP;- COUNT - 파티션별 윈도우의 카운트SELECT ENAME, SAL, COUNT(*) OVER (ORDER BY SAL RANGE BETWEEN 50 PRECEDING AND 150 FOLLOWING) as SIM_CNTFROM EMP;### 2-2. 그룹 내 행 순서 함수- FIRST_VALUE - 파티션별 윈도우에 가장 먼저 나오는 값SELECT DEPTNO, ENAME, SAL, FIRST_VALUE(ENAME) OVER (PARTITION BY DEPTNO ORDER BY SAL DESC ROWS UNBOUNDED PRECEDING) as DEPT_RICHFROM EMP;- LAST_VALUE - 파티션별 윈도우에 가장 나중에 나오는 값SELECT DEPTNO, ENAME, SAL, LAST_VALUE(ENAME) OVER ( PARTITION BY DEPTNO ORDER BY SAL DESC ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) as DEPT_POORFROM EMP;- LAG - 파티션별 윈도우의 이전 몇 번째 행의 값SELECT ENAME, HIREDATE, SAL, LAG(SAL) OVER (ORDER BY HIREDATE) as PREV_SALFROM EMPWHERE JOB = ‘SALESMAN’;- LEAD - 파티션별 윈도우의 이후 몇번째 행의 값SELECT ENAME, HIREDATE, LEAD(HIREDATE, 1) OVER (ORDER BY HIREDATE) as “NEXTHIRED&#34;FROM EMP;### 2-3. 그룹 내 순위함수- RANK - 파티션 내 전체 윈도우에 대한 순위, 동일 값에 대해서는 동일한 순위, 그 다음 값은 순위는 동일한 순위 만큼 증가된 채로 부여 (ex. 1,1,1,4,5,6,7...)SELECT JOB, ENAME, SAL,RANK( ) OVER (ORDER BY SAL DESC) ALL_RANK,RANK( ) OVER (PARTITION BY JOB ORDER BY SAL DESC) JOB_RANKFROM EMP;- DENSE_RANK - 파티션 내 전체 윈도우에 대한 순위, 동일 값에 대해서는 동일한 순위, 그 다음 값은 순위는 동일한 순위에 상관없이 다음값 부여 (ex. 1,1,1,2,3,4,5...)SELECT JOB, ENAME, SAL, RANK( ) OVER (ORDER BY SAL DESC) RANK, DENSE_RANK( ) OVER (ORDER BY SAL DESC) DENSE_RANKFROM EMP; - ROW_NUMBER - 파티션 내 전체 윈도우에 대한 순번, 동일한 값이어도 고유한 순위 부여SELECT JOB, ENAME, SAL, RANK( ) OVER (ORDER BY SAL DESC) RANK, ROW_NUMBER() OVER (ORDER BY SAL DESC) ROW_NUMBERFROM EMP; ### 2-4. 그룹 내 비율 함수- RATIO_TO_REPORT - 파티션 내 전체 SUM에 대한 컬럼별 백분율 소수점 값SELECT ENAME, SAL, ROUND(RATIO_TO_REPORT(SAL) OVER (), 2) as R_RFROM EMPWHERE JOB = ‘SALESMAN’; - PERCENT_RANK - 파티션별 윈도우에서 가장 먼저 나오는 것은 0, 제일 마지막에 나오는 것은 1로 나타낸 후 값에 상관없이 행의 순서만으로의 백분율 값SELECT DEPTNO, ENAME, SAL, PERCENT_RANK() OVER (PARTITION BY DEPTNO ORDER BY SAL DESC) as P_RFROM EMP; - CUME_DIST - 파티션별 윈도우의 전체 건수에서 현재 행보다 작거나 같은 건에 대한 누적 백분률 값SELECT DEPTNO, ENAME, SAL, CUME_DIST() OVER (PARTITION BY DEPTNO ORDER BY SAL DESC) as CUME_DISTFROM EMP; - NTITLE - 파티션별 전체 건수를 Argument로 N등분한 값SELECT ENAME, SAL, NTILE(4) OVER (ORDER BY SAL DESC) as QUAR_TILEFROM EMP ;`참고윈도우 함수별 기능 및 예제 - http://www.gurubee.net/lecture/2382윈도우 함수(WINDOW FUNCTION)제6절 윈도우 함수(WINDOW FUNCTION)WINDOW FUNCTION 종류그룹 내 순위함수.3.1 RANK 함수3.2 DENSE_RANK 함수3.3 ROW_NUMBER 함수일반 집계 함수3.4 ..www.gurubee.net윈도우 함수 공식 문서 - https://www.postgresql.org/docs/current/tutorial-window.html#PostgreSQL #Window functions
...</p>
  </div>
  <footer class="entry-footer">Jun Kang</footer>
  <a class="entry-link" aria-label="post link to [PostgreSQL] 윈도우 함수(Window Functions)의 개념, 성능 및 사용법 (over, sum/rank/ntitle/cume_dist 등...)" href="http://localhost:1313/posts/40/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">[Spring] 단위 테스트, JUnit의 개념 및 단위 테스트 코드 작성 방법
    </h2>
  </header>
  <div class="entry-content">
    <p>하나의 모듈을 기준으로 독립적으로 진행되는 가장 작은 단위의 테스트이다. 통합 테스트의 경우 시스템을 구성하는 컴포넌트들이 커질수록 테스트 시간이 길어지지만, 단위 테스트의 경우 해당 부분만 독립적으로 테스트하기에 코드의 변경이 있어도 빠르게 문제 여부를 확인할 수 있다. CleanCode 책에 의하면 깨끗한 테스트 코드는 다음 5가지 규칙을 따라야 한다.&gt; Fast - 빠르게 동작하여 자주 돌릴 수 있어야 한다.Independent - 테스트는 독립적이며 서로 의존해서는 안된다.Repeatable - 어느 환경에서도 반복이 가능해야 한다.Self-validating - 테스트는 성공 또는 실패로 결과를 내어 자체 검증되어야 한다.Timely - 테스트는 적시에, 테스트하려는 실제코드를 구현하기 직전에 구현해야 한다.## 2. JunitJunit은 단위 테스트를 지원하는 오픈소스 프레임워크로 다음과 같은 특징을 가진다.- 문자 혹은 GUI 기반으로 실행- @Test 메서드를 호출할 때마다 새 인스턴스 생성- 예상결과를 검증하는 assertion 제공- 자동실행, 자체결과 확인 및 즉각적인 피드백 제공- 테스트 방식을 구분할 수 있는 어노테이션을 제공하며, 어노테이션만으로 간결하게 실행이 가능### ▶ 2-1. 어노테이션 종류&gt; @DisplayName - 테스트 이름 명시@Test - 테스트를 수행할 메서드, Junit은 각 테스트끼리 영향을 주지 않도록 테스트 실행 객체를 매 테스트마다 만들고 종료 시 삭제@BeforeAll - 전체 테스트를 시작하기 전에 1회 실행 (ex. 데이터베이스 연결, 테스트환경 초기화, 전체 테스트 실행주기에 한 번만 호출)@BeforeEach - 테스트 케이스를 시작하기 전마다 실행 (테스트 메서드에 사용하는 객체 초기화, 테스트에 필요한 데이터 삽입 등)@AfterAll - 전체 테스트를 마치고 종료하기 전에 1회 실행 (데이터베이스 연결 종료, 공통으로 사용하는 자원 해제 등)@AfterEach - 테스트 케이스를 종료하기 전마다 실행 (테스트 이후 특정데이터를 삭제 등)### ▶ 2-2. AssertJJunit과 사용해 가독성을 높여주는 라이브러리로 다양한 문법을 지원한다. 기존 Junit은 기댓값과 실제 비교대상이 확실히 보이지 않아 잘 구분이 안되지만 isEqualTo 등 명확한 의미의 매머드로 대체가 가능하다.### ▶ 2-3. given-when-then 패턴요즘 단위테스트의 가장 보편적인 형태로 1개의 단위테스트를 3단계로 나눠서 처리하는 패턴이다.- given = 테스트 실행을 준비하는 단계 (어떤 상황, 데이터가 주어졌을 때)- when = 테스트를 진행하는 단계 (어떤 함수를 실행시키면 )- then = 테스트 결과를 검증하는 단계 (어떤 결과가 기대된다.)## 3. 단위 테스트 예제점수의 평균을 계산해주는 클래스에 대한 단위 테스트를 해보자. 해당 예제는 객체 간의 메시지 교환이 없는 단순한 값 비교, 예외 확인을 위한 테스트 케이스이다.(일반적으로 스프링 애플리케이션은 다양한 객체에서 메시지를 전달받아 의존성이 생기는데, 이럴 경우 Mock(가짜) 객체를 사용하여 테스트가 가능하다.)### ▶ 3-0. 테스트 대상인 평균점수 조회다음과 같이 0점 이상의 점수들에 대한 평균을 구하는 클래스가 있을 때public class AverageScoreCalculator {private static Integer sum = 0;private static Integer count = 0;public void addScore(Integer score) {if (!validateScores(score))	{throw new IllegalStateException(&#34;Invalid score&#34;);}sum &#43;= score;count&#43;&#43;;}private boolean validateScores(Integer score) {return score &gt; 0;}public Double getAverageScore() {return (double) (sum / count);}}### ▶ 3-1. 점수의 평균이 일치하는지 테스트실제 평균의 값과, 클래스 연산 결과가 일치하는지 테스트한다.@DisplayName(&#34;점수의 평균 테스트&#34;)@Testvoid averageScoreTest() {//givenAverageScoreCalculator averageScoreCalculator = new AverageScoreCalculator();int[] scores = {10,20,30,40,50};//whenfor (int i = 0; i&lt;scores.length; i&#43;&#43;)	{averageScoreCalculator.addScore(scores[i]);}Double averageScore = averageScoreCalculator.getAverageScore();//thenassertThat(averageScore).isEqualTo(Arrays.stream(scores).average().getAsDouble());}### ▶ 3-2. 평균점수의 범위 테스트점수의 평균이 1~100점 이내에 존재하는지 확인한다.@DisplayName(&#34;평균 점수 범위 테스트&#34;)@Testvoid averageScoreRangeTest()	{//givenAverageScoreCalculator averageScoreCalculator = new AverageScoreCalculator();int[] scores = {10,20,30,40,50};//whenfor (int i = 0; i&lt;scores.length; i&#43;&#43;)	{averageScoreCalculator.addScore(scores[i]);}Double averageScore = averageScoreCalculator.getAverageScore();//thenassertThat(averageScore &gt;= 0 &amp;&amp; averageScore &lt;= 100).isTrue();}### ▶ 3-3. 개별점수 유효성 테스트유효하지 않은 점수가 인풋 될 경우 IllegalStateException이 기대되기에, assertThrow로 Exception을 테스트한다.@DisplayName(&#34;개별 잘못된 점수 테스트&#34;)@Testpublic void averageScoreInvalidScoreTest(){//givenAverageScoreCalculator averageScoreCalculator = new AverageScoreCalculator();int[] scores = {10,20,30,40,-1};//whenfinal IllegalStateException exception = assertThrows(IllegalStateException.class, () -&gt; {for (int i = 0; i&lt;scores.length; i&#43;&#43;)	{averageScoreCalculator.addScore(scores[i]);}});//thenassertThat(exception.getMessage()).isEqualTo(&#34;Invalid score&#34;);}## 4. 주요 Assert 메서드### ▶ 4-1. 주요 비교 검증 테스트 메서드메서드 이름설명isEqualTo(A)A 값과 같은지 검증isNotEqualTo(A)A 값과 다른지 검증contains(A)A 값을 포함하는지 검증doesNotContain(A)A 값을 포함하지 않는지 검증startWith(A)접두사가 A인지 검증endsWith(A)접미사가 A인지 검증isEmpty()비어 있는 값인지 검증isNotEmpty()비어 있지 않은 값인지 검증isPositive()양수인지 검증isNegative()음수인지 검증isGreaterThan(a)a보다 큰 값인지 검증isLessThan(a)a보다 작은 값인지 검증### ▶ 4-2. HTTP 주요 응답코드 테스트 메서드코드매핑 메서드설명200 OKisOk()HTTP 응답코드가 200 OK인지 검증201 CreatedisCreated()HTTP 응답코드가 201 Created 검증400 Bad RequestisBadRequest()HTTP 응답코드가 400 BadRequest검증403 ForbiddenisForbidden()HTTP 응답코드가 403 Forbidden검증404 Not FoundisNotFound()HTTP 응답코드가 404 Not Found 검증4is4xxClientError()HTTP 응답코드가 4 검증500 Internal Server ErrorisInternalServerError()HTTP 응답코드가 500 InternalServerError 검증5is5xxClientError()HTTP 응답코드가 5 검증참고도서 : 스프링 부트 3 백엔드 개발자 되기 - 자바 편https://mangkyu.tistory.com/143#spring #TDD #JUnit #단위테스트 #assertj
...</p>
  </div>
  <footer class="entry-footer">Jun Kang</footer>
  <a class="entry-link" aria-label="post link to [Spring] 단위 테스트, JUnit의 개념 및 단위 테스트 코드 작성 방법" href="http://localhost:1313/posts/45/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">[Spring] 순환참조란? The dependencies of some of the beans in the application context form a cycle
    </h2>
  </header>
  <div class="entry-content">
    <p>순환참조는 맞물린 의존성 주입 (DI) 상태에서 어떤 빈을 먼저 생성할지 결정하지 못해서 생기에 발생한다. BeanA에서 BeanB를 참조(BeanA-&gt;BeanB) 일 경우 스프링은 BeanB를 먼저 생성 후 BeanA를 생성하기에, BeanB에서 다시 BeanA를 참조할 경우 (BeanA-&gt;BeanB-&gt;BeanA) 순환 참조가 발생하게된다.## 2. 의존성 주입의존성 주입의 3가지 상황 (생성자 주입방식, 필드 주입방식, Setter주입)에서 순환참조가 발생할수 있다. 다음 포스트 각각의 상세 내용을 확인할 수 있고, 이번 포스트에서는 각각의 경우에 순환참조가 발생하면 어떤 차이점이 있는지 확인해 보자.2023.11.06 - [Spring] - [Spring] IoC(제어의 역전) &amp; DI(의존성 주입)의 개념[Spring] IoC(제어의 역전) &amp; DI(의존성 주입)의 개념1. IoC (Inversion of Control) 제어의 역전 IoC란 메인 프로그램에서 컨테이너나 프레임워크로 객체와 객체의 의존성에 대한 제어를 넘기는 것을 말한다. 프레임워크 없이 개발할 때는 각 객체에 대한junhkang.tistory.com### ▶ 2-1. 생성자 주입@Componentpublic class BeanA {private BeanB beanB;public void BeanA(BeanB beanB){this.beanB = beanB;}}@Componentpublic class BeanB {private BeanA beanA;public void BeanB(BeanA beanA){this.beanA = beanA;}}생성자 주입의 경우, 애플리케이션 구동 시 스프링 컨테이너(IOC)는 BeanA 빈을 생성하기 위해 BeanB를 찾고 BeanB를 찾기 위해 Bean A를 찾기 때문에 순환참조가 발생하게 된다.### ▶ 2-2. 필드 주입, Setter 방식필드 주입, Setter 방식은 애플리케이션의 실행 시점에서는 에러가 발생되지 않는다. 어플리케이션의 실행 시점이 아닌, 실제로 사용되는 시점에 실행되는 메서드가 순환 호출되기 때문이다. 필요 없는 시점에는 null 상태로 유지 후 사용될 때 의존성이 주입되며 참조되기 시작한다.## 3. 해결책### ▶ 3-1. @Lazy 어노테이션@Componentpublic class BeanA {private BeanB beanB;public void BeanA(BeanB beanB){this.beanB = beanB;}}@Componentpublic class BeanB {private BeanA beanA;public void BeanB(@Lazy BeanA beanA){this.beanA = beanA;}}다음과 같이 @Lazy 어노테이션을 통해 시점을 지연시킬 수 있으나 스프링에서는 이 방식을 추천하지 않는다. 애플리케이션 로딩시점이 아닌 Bean이 필요한 시점에 주입받기 때문에 특정 HTTP 요청을 받을 때 Heap 메모리가 증가할 수 있으며 메모리가 충분하지 않은 경우 장애로 이어질 수 있다. 또한 잘못된 빈의 생성시점을 늦추기에 문제상황에 대한 인식이 늦어질 수 있다.### ▶ 3-2. 설계 변경근본적으로 순환참조가 일어나지 않는 설계를 해야 한다.단순하게는 BeanA -&gt; BeanB-&gt; BeanA의 관계를 BeanA -&gt; BeanB -&gt; BeanC 형태로 참조가 순환되지 않도록 분리해야 한다.#spring #의존성주입 #순환참조
...</p>
  </div>
  <footer class="entry-footer">Jun Kang</footer>
  <a class="entry-link" aria-label="post link to [Spring] 순환참조란? The dependencies of some of the beans in the application context form a cycle" href="http://localhost:1313/posts/47/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">[Spring] 스프링 배치 ItemReader의 개념, (MybatisCursorItemReader, MybatisPagingItemReader 구현)
    </h2>
  </header>
  <div class="entry-content">
    <p>스프링 배치의 ItemReader는 다음과 같은 과정을 거쳐 데이터를 처리한다.대부분의 데이터 형태는 이미 ItemReader로 제공하고 있기에 ItemReader, ItemStream 인터페이스 자체를 구현할 필요는 없다.ItemReader는 Chunk 기반 트랜잭션을 다루며 Cursor, Paging 가 대표적인 2가지 방식이다.## 2. Cursor, Paging 형식### 2-1. Cursor기반 ItemReader- JDBC ResultSet의 기본 기능이다.- ResultSet이 Open 될 때마다 데이터베이스의 데이터가 반환된다.- 데이터베이스와 연결 맺은 후 데이터를 Streaming 방식으로 I/O이다.- 현재 행에서Cursor를 유지하며 다음 데이터를 호출하면 Cursor를 한 칸씩 옮기면서 데이터를 가져온다.- 하나의 Connection으로 배치가 끝날때까지 사용되기에 Batch가 끝나기 전에 데이터베이스와 애플리케이션의 연결이 먼저 끊어질 수 있어 데이터베이스와 SocketTimeout을 충분한 값으로 설정하여야 한다.- 모든 결과를 메모리에 할당 하기 때문에 메모리 사용량이 많아진다.- Chunk 사이즈 만큼의 트랜잭션 단위로 데이터를 처리한다.Cursor 기반 ItemReader 구현체&gt; - JdbcCursorItemReader- HibernateCursorItemReader- StoredProcedureItemReader- MybatisCursorItemReader### 2-2. Paging기반 ItemReader- Chunk로 데이터베이스에서 데이터를 검색- Page Size 만큼만 한 번에 메모리로 가져오기에 메모리 사용량이 적어진다.- 페이지 단위로 컨넥션을 맺고 끊기를 반복하기에 대량 데이터를 처리하기 좋다.- 쿼리자체에 반환하고자하는 데이터 범위를 지정하여 사용한다. (offset, limit)- 컨넥션 유지시간이 길지 않고 메모리를 효율적으로 사용해야 하는 데이터에 적합하다.Paging 기반 ItemReader 구현체&gt; - JdbcPagingItemReader- HibernatePagingItemReader- JpaPagingItemReader- MybatisPagingItemReader## 3. MybatisItemReader 구현### 3-1. MybatisCursorItemReaderMybatisCursorItemReader로 구현시 간단하다. 한 번에 조회해온 결과를 chunk만큼 트랜잭션을 분할하여 대용량 처리를 한다.BatchConfig.java@Beanpublic T jobStep(StepBuilderFactory steps) throws Exception {return stepBuilderFactory.get(&#34;JOB&#34;). chunk(1000) -- Chunk 사이즈 조절.reader(itemReader.reader(sqlSessionFactory)).processor(new itemProcessor()).writer(new itemWriter()).build();}ItemReader.JavaMyBatisCursorItemReader databaseReader = new MyBatisCursorItemReader&lt;&gt;();databaseReader.setSqlSessionFactory(sqlSessionFactory);databaseReader.setQueryId(QueryId);databaseReader.setParameterValues(map);databaseReader.setSaveState(true);return databaseReader;데이터베이스에서 모든 결과를 메모리에 할당한 후, Chunk 사이즈만큼의 트랜잭션 단위로 데이터를 처리한다.### 3-2. MyBatisPagingItemReader 구현다음과 같이 조회 쿼리 자체에 OFFSET, LIMIT을 설정하여, 한 페이지당 조회할 데이터 위치를 파악한다.MyBatisPagingItemReader에서는 다음 파라미터로 페이징 관련 값들에 바로 접근이 가능하다.&gt; _page : 읽을 page 수량 (0부터 시작)_pagesize : 한번에 읽을 페이지 수량 (리턴 받을 데이터의 수량)_skiprows : _page * _pagesize (다음 페이지 시작 포인트, offset)해당 값들을 쿼리에서 바로 사용 가능하며 다음과 같이 적용할 수 있다.SELECT id, name, job FROM employees ORDER BY id ASCOFFSET #{_skiprows} LIMIT #{_pagesize}한번에 가져올 페이지 사이즈 (_pagesize)는 ItemReader.Java에서 setPageSize()를 통해 설정가능하다. (쿼리의 LIMIT에 해당하는 값)MyBatisPagingItemReader databaseReader = new MyBatisPagingItemReader&lt;&gt;();databaseReader.setSqlSessionFactory(sqlSessionFactory);databaseReader.setQueryId(QueryId);databaseReader.setParameterValues(map);databaseReader.setPageSize(1000); -- Paging에서는 한번에 읽을 Page수량을 추가해야한다. default = 10databaseReader.setSaveState(true);return databaseReader;#### 주의사항매 페이지를 신규 조회할때 데이터의 변경되어 전체 페이징 기준이 달라진다면 누락되거나 중복처리되는 데이터가 있을 수 있다.같은 이유로, order by를 적절하게 하지 않을 경우 미처리, 혹은 중복처리 되는 데이터가 발견될 수 있다. 매 Paging마다 그 시점의 페이징 데이터를 조회하기 때문이다.특히 처리완료 데이터를 마킹하면서 처리하고, 미처리 데이터를 조회조건에 넣는다면, 데이터가 처리될 때마다 특정 페이지의 값들이 달라질 것이다. 이 경우 Cursor를 사용하면 쉽게 해결되지만, 메모리 및 컨넥션 타임 문제로 Paging을 꼭 사용하여야 하는 경우에는 쿼리에서 offset을 제거하거나 _page변수를 항상 0으로 지정해 주면 된다.MybatisPagingItemReader.java의 내부 구조를 확인해보면@Overrideprotected void doReadPage() {if (sqlSessionTemplate == null) {sqlSessionTemplate = new SqlSessionTemplate(sqlSessionFactory, ExecutorType.BATCH);}Map parameters = new HashMap&lt;&gt;();if (parameterValues != null) {parameters.putAll(parameterValues);}parameters.put(&#34;_page&#34;, getPage());parameters.put(&#34;_pagesize&#34;, getPageSize());parameters.put(&#34;_skiprows&#34;, getPage() * getPageSize());if (results == null) {results = new CopyOnWriteArrayList&lt;&gt;();} else {results.clear();}results.addAll(sqlSessionTemplate.selectList(queryId, parameters));}_page는 getPage() 값을 사용하기 때문에MyBatisPagingItemReader reader = new MyBatisPagingItemReader(){@Overridepublic int getPage()	{return 0;}};다음과 같이 매 Paging 조회마다 페이지 값을 0으로 리셋해주면 매 page를 조회할 때마다 offset = 0인 채로 조회가 가능하다.doReadPage()를 override 하여 페이지 읽는 로직 자체를 커스터마이징 하는 것도 가능하다.참고https://mybatis.org/spring/batch.htmlhttps://ojt90902.tistory.com/780https://junuuu.tistory.com/611https://jojoldu.tistory.com/336#spring #Batch #ItemReader
...</p>
  </div>
  <footer class="entry-footer">Jun Kang</footer>
  <a class="entry-link" aria-label="post link to [Spring] 스프링 배치 ItemReader의 개념, (MybatisCursorItemReader, MybatisPagingItemReader 구현)" href="http://localhost:1313/posts/32/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">[네트워크] REST, RESTful API의 개념 및 설계 가이드
    </h2>
  </header>
  <div class="entry-content">
    <p>REST란 Representational State Transfer의 약자로 자원을 이름으로 구분하여 자원의 상태를 주고받는 것을 의미한다. HTTP URI를 통해 자원을 명시하고 HTTP Method를 통해 행위를 적용한다.### 1-1. REST 구성요소- 자원(Resource) : HTTP URI - 서버는 고유한 리소스 식별자로 각 리소스를 식별- 행위(Verb) : HTTP Method (GET, POST, PUT, DELETE)- 내용(Representations) : HTTP Message Pay Load - 하나의 자원은 JSON,XML, TEST, RSS 등 여러 형태의 Representaion으로 나타내어질 수 있다.### 1-2. REST의 특징- Stateless (무상태성) - 서버가 이전의 모든 요청과 독립적으로 클라이언트 요청을 완료함을 의미- Cacheable(캐쉬 가능성) - 일부 응답을 저장하는 프로세스인 캐싱을 지원함을 의미- Layered System (계층화) - 클라이언트는 REST API Server만 호출하지만, 클라이언트 요청을 이행하기 위해 함께 작동하는 비즈니스 로직(보안, 암호화 등)을 여러 계층으로 실행될 수 있도록 유연하게 설계 가능함을 의미- Uniform Interface (균일한 인터페이스) - 서버가 표준 형식으로 정보를 전송함을 의미### 1-3. 장점- HTTP 프로토콜을 그대로 사용하기에 별도 인프라를 구축할 필요가 없음- HTTP 프로토콜을 따르는 모든 플랫폼에서 사용 가능- API의 의도를 쉽고 명확하게 파악 가능- 클라이언트, 서버를 완전히 분리하기에 각 부분이 독립적으로 발전 가능- 사용되는 기술과 독립적이기에 API 설계에 영향을 주지 않고 다양한 프로그래밍 언어로 작성이 가능### 1-4. 단점- 표준이 존재하지 않아 정의가 필요함- HTTP Method 형태가 제한적## 2. RESTful API란?REST 아키텍쳐를 따르는 API를 RESTful API (Representaional state transfer API)라고 하며 REST 아키텍처를 구현하는 웹서비스를 RESTful 웹 서비스라고 한다. REST는 복잡한 네트워크에서 통신을 관리하기 위한 지침으로 만들어 졌으며, 대규모의 고성능 통신을 안정적으로 지원할 수 있고 쉽게 구현 및 수정할 수 있어 파악에 용이하고 여러 시스템에서 사용이 가능하다.## 3. RESTful API 설계- 동사 보다는 명사, 대문자보다 소문자 사용/getArticles/1 -&gt; /articles/1- 컬렉션 이름으로는 복수 명사 사용/article/1 -&gt; /articles/1- HTTP Method를 포함하지 않음/get/articles/1 -&gt; GET /articles/1- 행위에 대한 동사 표현이 포함하지 않음/show/articles/1 -&gt; /articles/1- 경로 부분 중 변하는 부분은 유일값으로 대체/articles/{articleId} -&gt; 각 articleId은 유일한 결과값을 가진다.- 마지막에 / 포함하지 않음/articles/1/ -&gt; /articles/1- 언더바 대신 하이픈 사용/newest_ariticles/1 -&gt; /newest-ariticles/1- 파일 확장자는 URI에 포함하지 않음/articles/1/photo.jpg -&gt; /articles/1/photo [Accept: image/jpg]참고https://aws.amazon.com/ko/what-is/restful-api/https://khj93.tistory.com/entry/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-REST-API%EB%9E%80-REST-RESTful%EC%9D%B4%EB%9E%80#Rest #restful api
...</p>
  </div>
  <footer class="entry-footer">Jun Kang</footer>
  <a class="entry-link" aria-label="post link to [네트워크] REST, RESTful API의 개념 및 설계 가이드" href="http://localhost:1313/posts/48/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="prev" href="http://localhost:1313/tags/333333/">
      «&nbsp;Prev&nbsp;
    </a>
    <a class="next" href="http://localhost:1313/tags/333333/page/3/">Next&nbsp;&nbsp;»
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Jun Kang&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
