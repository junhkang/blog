[{"content":" 몇 년 전 운영 프로젝트 설정 시 자세하게 봤던 내용이지만, 트래픽이 대폭 증가하고, DBMS에 연결된 프로젝트와 모듈이 늘어남에 따라 재설정을 위해 개념을 다시 정리하게 되었다.\n1. 기본적인 데이터베이스 연결과정 DB Connection 열기- 데이터베이스 드라이버를 사용하여 데이터베이스 서버와의 연결 TCP 소켓 열기 - 데이터베이스 전송을 위해 TCP 소켓을 생성하고 데이터베이스 서버와 통신채널을 설정 데이터 통신 수행 - 생성된 소켓을 통해 SQL 쿼리를 전송하고 데이터를 Read / Write DB연결 닫기 - 데이터 통신이 완료되면 데이터베이스와의 연결을 종료 TCP 소켓 닫기 - 사용한 TCP 소켓을 닫아 통신 채널 해제 웹 어플리케이션은 클라이언트의 HTTP 요청이 들어오면 스레드를 생성한다. 각 요청 시 DB서버로부터 데이터를 얻기 위해서 DB에 지속적으로 접근하는 작업이 필요하다. 스프링부트를 예로 들면, DB에 직접 연결하는 경우, JDBC 드라이버는 애플리케이션 시작 시 한번 로드되고, 사용자 요청 시마다 새로운 connection 객체 생성하여 데이터베이스에 연결한 후 종료되어야 한다. 이렇게 사용자 요청 시 매번 connection 객체를 생성/연결/종료해야 한다면 굉장히 비효율적이다.\n2. Connection Pool 2-1. Connection Pool이란? 일정량의 컨넥션 객체를 미리 생성하여 pool에 저장하고, 클라이언트에서 요청이 오면 connection 객체를 빌려주고, 임무가 완료되면 다시 객체를 반납 받아서 pool에 다시 저장하는 기법이다. 일반적인 pool 패턴에서 사용되는 방식으로, 자원을 효율적으로 재사용함으로써 성능 최적화할 수 있는 방법이다. 스프링부트에선 컨테이너 구동시 일정 양의 connection 객체를 생성하여, 클라이언트 요청에 따라 데이터베이스 접근이 필요하면, connection pool에서 connection객체를 받아와 작업 후, 컨넥션풀에 다시 객체를 반납한다. 출처 : https://steady-coding.tistory.com/564 2-2. HikariCP Connection Pool 획득 과정 컨넥션 요청 이전에 사용했던 컨넥션 정보확인 이전에 사용했던 정보 중 컨넥션 가능한 존재 여부 확인 전체 컨넥션 목록 중 사용가능한 컨넥션 존재여부 확인 hikariCP에선 Thread가 연결요청시 컨넥션풀에 이전에 사용했던 컨넥션을 확인하고(3) 이를 우선반환하는 특징이 있다. 사용가능한 컨넥션이 없을 경우(4) HandOffQueue를 폴링 하면서 다른 Thread가 컨넥션을 반납하는 것을 기다린다. 지정 타임아웃 시간까지 대기하다가 시간이 만료되면 예외를 던진다. 객체를 획득해서 사용완료하면, 해당 객체를 컨넥션 풀에 반납한다. 컨넥션 풀은 사용내용을 관리하고, HandOffQueue에 반납된 컨넥션을 객체를 삽입한다. 해당 큐를 폴링하던 스레드는 컨넥션을 획득하고 작업을 처리한다.\n3. Connection Pool 장점 불필요한 객체 생성/삭제가 사라지고, 클라이언트가 빠르게 데이터베이스에 접근 가능 디비 접속 모듈 공통화로 디비서버 환경이 바뀔경우 쉬운 유지보수 데이터베이스 컨넥션 수를 제한할 수 있어, 서버 자원을 효율적으로 관리 가능 연결이 끝난 컨넥션 객체를 재사용해서 객체 생성비용 감소 4. Connection Pool 설정시 주의사항 4-1. Connection Pool 증가 4-1-1. 과도한 데이터베이스 접속 발생 시 대기 문제\n데이터베이스 접속이 너무 많이 발생할 경우, 컨넥션 수량이 한정되어있으면 반납할 때까지 계속 대기하게 된다. 그렇다고 컨넥션풀을 많이 생성하면, 커넥션이 메모리를 차지하고 서비스 성능을 떨어트린다.\n-\u0026gt; 결국, 컨넥션 풀을 크게설정하면, 메모리 소모가 큰 대신 사용자 대기시간이 줄어들지만 컨넥션 풀이 작으면 그만큼 대기시간이 길어진다. 4-1-2. 컨넥션 풀 증가의 한계\n이처럼 컨넥션풀이 늘어난다고 무조건 성능이 좋아지는건아니다. 컨넥션의 생성 주체는 결국 Thread이기 때문에 같이 고려해야 한다.\n4-2. Connection Pool \u0026amp; Thread Pool 스레드 풀 - 데이터베이스 연결 요청을 처리하기 위해 풀을 설정하는 것처럼, 웹 애플리케이션은 클라이언트의 요청을 처리하기 위해 스레드를 할당받는다. 이 스레드가 데이터베이스 작업을 수행할 때 컨넥션 풀에서 객체를 빌려 사용하게 된다. 일반적으로 컨넥션 풀의 크기는 스레드 풀의 크기와 비슷하거나 약간 더 큰 정도로 설정한다.\n4-3. Connection Pool, Thread Pool의 동시 증가 4-3-1. 메모리 소모 및 자원 비효율성\n사용하지 않는 컨넥션들이 메모리만을 소비하게 되어, 메모리 사용량이 증가하고 시스템 자원이 비효율적으로 사용됨 4-3-2. 콘텍스트 스위칭 오버헤드\n스레드 수가 증가함에 따라 운영체제가 스레드 간 콘텍스트 스위칭을 더 자주 하게 된다. CPU 리소스를 소모하게 됨으로 스레드 수가 과도하게 많아지면 전체 시스템 성능이 저하될 수 있다. 4-3-3. 데이터베이스 서버의 스레드 관리 부담\n많은 수의 데이터베이스 연결은 데이터베이스 서버에서도 많은 스레드를 필요로 하고, 스레드 관리에 따른 오버헤드를 증가시킨다. 데이터 베이스 서버 측의 콘텍스트 스위칭 오버헤드도 증가하여 응답속도를 저하시킬 수 있다. 결국 스레드풀, 컨넥션풀을 계속 늘린다고 해서 성능이 계속 증가하는 것이 아니라 한계가 존재한다. 컨넥션 풀과 스레드 풀의 크기는 시스템의 하드웨어 사양, 애플리케이션의 특성, 데이터베이스 서버의 성능 등에 따라 최적의 크기가 다르다. 이 최적의 크기를 초과하여 풀을 늘리면, 오히려 성능이 저하될 수 있다.\n5. 적정 Connection Pool 5-1. HikariCP 공식문서 적정 Connection Pool 히카리 CP 기준 공식 문서에 따르면 다음과 같이 적정 컨넥션 풀을 정의하고 있다.\n1 connections = ((core_count) * 2) + effective_spindle_count) core_count - 현재 사용하는 서버 환경에서의 CPU 개수, CPU코어 수는 동시에 처리할 수 있는 스레드의 기본 수용능력을 나타냄 core_count * 2 : 공식 문서에서는 스레드 풀의 스레드 수를 CPU코어 수의 2배로 설정하는 것을 권장, I/O 작업으로 인한 블로킹 시간 동안 다른 스레드가 CPU를 활용할 수 있게 하기 위함 effective_spindle_count - 기본적으로 디비 서버가 관리할 수 있는 동시 I/O요청수 하드 디스크 하나는 spindle 하나를 갖는다 spindle은 디스크의 물리적 회전축을 의미 디스크가 16개인 경우, 시스템은 동시에 16개의 I/O처리 가능 이는 데이터베이스 서버가 효율적인 디스크 사용을 위해, 동시에 처리할 수 있는 I/O 작업의 수를 제한한다. 5-2. 서버 환경 예시 CPU 코어 수 : 8개 디스크 스핀들 수 : 16개 (디스크 16개 기준) 적정 컨넥션 풀 크기 계산 : (8 × 2)+16=32\n따라서 이 서버의 적정 컨넥션 풀의 크기는 32이다.\n6. HikariCP 옵션 auto-commit (default: true) 커넥션 종료 또는 풀에 반환 시 트랜잭션을 자동으로 커밋할지 결정 connection-timeout (default: 30000ms) 풀에서 커넥션을 얻기 위해 기다리는 최대 시간을 설정, 초과 시 SQLException 던짐 idle-timeout (default: 600000 ms) 풀에 미사용 커넥션을 유지하는 시간을 지정, minimum-idle보다 작게 설정 시에만 적용 max-lifetime (default: 1800000 ms) 풀에서 커넥션이 유지될 수 있는 최대 수명 시간을 설정, 사용 중이지 않을 때만 제거 minimum-idle (default: maximum-pool-size) 풀에서 항상 유지할 최소 커넥션 수를 설정, 최적의 성능을 위해 maximum-pool-size와 동일하게 설정 권장 maximum-pool-size (default: 10) 풀에서 유지할 수 있는 최대 커넥션 수를 설정, 데이터베이스 부하에 따라 조정 pool-name (default: auto-generated) 풀의 이름을 지정하여 로깅이나 JMX 관리 콘솔에 표시 initialization-fail-timeout (default: 1ms) 초기 커넥션 생성 실패 시 풀 초기화를 빠르게 실패하도록 설정 validation-timeout (default: 5000ms) 커넥션 유효성 검사를 수행할 때 사용할 타임아웃 시간 지정 connection-test-query (default: none) JDBC4 드라이버 미지원 시 유효한 커넥션인지 확인하기 위한 쿼리 설정 read-only (default: false) 풀에서 커넥션을 획득할 때 읽기 전용 모드로 설정하여 최적화 지원 transaction-isolation (default: none) java.sql.Connection에 지정할 트랜잭션 격리 수준 설정 category (default: none) 커넥션에서 사용할 카테고리를 결정, 설정 없을 시 JDBC 드라이버 기본값 사용 leak-detection-threshold (default: 0ms) 커넥션 누수 감지를 위해 커넥션 풀에 반환되기 전 허용할 최대 시간 설정 Statement Cache 각 커넥션마다 PreparedStatement 캐싱, 데이터베이스 드라이버 설정을 통해 관리 driver-class-name (default: none) 특정 드라이버를 명시적으로 설정 시 사용, 지정 시 jdbc-url 반드시 설정정 registerMbeans (default: false) JMX 관리 Beans에 풀을 등록할지 여부 설정 참고\nhttps://steady-coding.tistory.com/564 https://1-7171771.tistory.com/119 https://backtony.tistory.com/58 https://github.com/brettwooldridge/HikariCP ","permalink":"http://localhost:50666/posts/125/","summary":"\u003chr\u003e\n\u003cp\u003e몇 년 전 운영 프로젝트 설정 시 자세하게 봤던 내용이지만, 트래픽이 대폭 증가하고, DBMS에 연결된 프로젝트와 모듈이 늘어남에 따라 재설정을 위해 개념을 다시 정리하게 되었다.\u003c/p\u003e\n\u003ch3 id=\"기본적인-데이터베이스-연결과정\" ke-size=\"size23\"\u003e1. 기본적인 데이터베이스 연결과정\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDB Connection 열기\u003c/strong\u003e- 데이터베이스 드라이버를 사용하여 데이터베이스 서버와의 연결\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTCP 소켓 열기\u003c/strong\u003e - 데이터베이스 전송을 위해 TCP 소켓을 생성하고 데이터베이스 서버와 통신채널을 설정\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e데이터 통신 수행\u003c/strong\u003e - 생성된 소켓을 통해 SQL 쿼리를 전송하고 데이터를 Read / Write\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDB연결 닫기\u003c/strong\u003e - 데이터 통신이 완료되면 데이터베이스와의 연결을 종료\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTCP 소켓 닫기\u003c/strong\u003e - 사용한 TCP 소켓을 닫아 통신 채널 해제\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e웹 어플리케이션은 클라이언트의 HTTP 요청이 들어오면 스레드를 생성한다. 각 요청 시 DB서버로부터 데이터를 얻기 위해서 DB에 지속적으로 접근하는 작업이 필요하다. 스프링부트를 예로 들면, DB에 직접 연결하는 경우, JDBC 드라이버는 애플리케이션 시작 시 한번 로드되고, 사용자 요청 시마다 새로운 connection 객체 생성하여 데이터베이스에 연결한 후 종료되어야 한다. 이렇게 사용자 요청 시 매번 connection 객체를 생성/연결/종료해야 한다면 굉장히 비효율적이다.\u003c/p\u003e","title":"[Spring] 스프링부트와 HikariCP를 활용한 Connection Pool 설정 및 최적화"},{"content":" jenkins.plugins.publish_over.BapPublisherException: Failed to connect and initialize SSH connection. Message: [Failed to connect SFTP channel. Message [4: Received message is too long: 458961709]]\n1. 문제상황 기존에 잘 작동하던 SFTP를 활용한 CI/CD가 갑자기 작동하지 않는다. 에러 메시지에 따르면 약 438mb의 메시지를 받은 상황인데, 파일을 실제로 업로드하는 시점이 아닌 SFTP연결 시도를 하는 순간(SSH연결을 설정하고 초기화하는 부분)에도 이렇게 큰 응답을 받는 상황이었다.\n2. 원인파악 SSH 접속시, 서버 간 구분을 주기 위해 ~/. bashrc의 설정을 통해 웰컴메시지를 출력하는 부분이 문제였다. (다음 포스트에서 진행한 서버별 웰컴 메시지 등록 부분에서 작업과정 확인 가능)\n[AWS] Bastion 서버 설정 및 서버 접속 상태 한눈에 구분하기 SFTP클라이언트는 SSH 세션이 시작될떄 특정 초기화 메시지를 받아야 한다. 웹컴메시지가 삽입되면서 클라이언트가 메세저리를 SFTP데이터로 잘못 해석될 수 있다.\n3. 메세지 크기의 원인 내가 작성한 웰컴 메시지는 고작 한 줄이고, kb이하의 단위인데 어떻게 저렇게 큰 메시지 크기를 응답받았을까? 3-1. 데이터 첫부분의 잘못된 해석 SSH 세션 시작될때, 서버로부터 잘못된 형식의 데이터를 받아서 올바르게 해석하지 못하고 잘못된 패킷크기로 계산되는 경우가 있다. 예를 들어 패킷이 0x1 B6 C8 FDD(16진수)로 시작하면, 이를 10진수로 변환한 값이 458961709가 된다. 웰컴 메시지가 이런 종류의 데이터를 포함하거나 데이터의 첫 부분을 잘못 해석한 경우 이런 식으로 계산될 수 있다.\n3-2. 버퍼오염 웰컴 메세지와 SFTP초기화 데이터가 섞이면서 클라이언트 버퍼가 오염될 수 있다. 이 경우 클라이언트는 비정상적인 크기의 메시지 읽었다고 판단한다.\n4. 해결 근본적으로는 의미없는 웰컴 메시지, 서버 첫 접속 시의 자동화된 문구 출력 자체를 없애는 것이 좋겠지만, 꼭 써야 한다면\nif [[ \u0026#34;$SSH_TTY\u0026#34; ]]; then echo \u0026#34;Welcome to the server! fi 이렇게 웰컴 메세지가 비인터렉티브 세션 (SFTP)에서는 출력되지 않도록 수정하여 해결할 수 있다. ","permalink":"http://localhost:50666/posts/124/","summary":"\u003chr\u003e\n\u003cp\u003e \u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ejenkins.plugins.publish_over.BapPublisherException: Failed to connect and initialize SSH connection. Message: [Failed to connect SFTP channel. Message [4: Received message is too long: 458961709]]\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"문제상황\" ke-size=\"size23\"\u003e1. 문제상황\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/124/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202025-01-03%20%EC%98%A4%ED%9B%84%203.32.21.png\"\u003e\u003c/p\u003e\n\u003cp\u003e기존에 잘 작동하던 SFTP를 활용한 CI/CD가 갑자기 작동하지 않는다. 에러 메시지에 따르면 약 438mb의 메시지를 받은 상황인데, 파일을 실제로 업로드하는 시점이 아닌 SFTP연결 시도를 하는 순간(SSH연결을 설정하고 초기화하는 부분)에도 이렇게 큰 응답을 받는 상황이었다.\u003c/p\u003e\n\u003ch3 id=\"원인파악\" ke-size=\"size23\"\u003e2. 원인파악\u003c/h3\u003e\n\u003cp\u003eSSH 접속시, 서버 간 구분을 주기 위해 ~/. bashrc의 설정을 통해 웰컴메시지를 출력하는 부분이 문제였다. (다음 포스트에서 진행한 서버별 웰컴 메시지 등록 부분에서 작업과정 확인 가능)\u003c/p\u003e","title":"[LINUX] SFTP 초기화 오류 : Failed to connect and initialize SSH connection. Message: [Failed to connect SFTP channel"},{"content":" 1. Bastion 서버란? Bastion 서버란 클라우드 환경, 네트워크 환경에서 보안 게이트웨이 역할을 하는 서버로, 외부 네트워크에서 private 서버로 접속할 때 보안을 강화해 주고 접근제어를 구현해 준다. 외부 사용자가 특정 IP에서만 Bastion 서버에 접속하도록 설정할 수 있으며, private 서버들은 Bastion 서버를 통해서 트래픽을 철저하게 관리할 수 있다. 이번 포스트에선 Bastion서버의 설정 방법과, 다중 서버를 Bastion 서버에서 관리할 시 접속상태를 한눈에 구분할 수 있는 방법에 대해 알아보자.\n2. Bastion 서버 설정 방법 2-1.aws 인스턴스 생성 t2 micro~t3 micro - 소규모 개발팀 t3 small - 중간 규모팀 T3.medium, M5.large - 대규모팀 (다수 개발자, 운영팀) 인스턴스 생성 후 bastion 서버로 접속하고자하는 ip를 보안그룹에 등록해 준다. (ssh : 22 port)\n2-2. 키 파일 포워딩 Bastion 서버를 구성하는 방식은 크게 두가지가 있다.\nbastion 서버 내에 실제 접속 서버들의 키파일을 보관하여, 각 서버로 접속 시에 사용 Bastion 서버가 해킹될 경우 모든 운영 서버에 접근 가능 키파일의 업로드 및 관리 어려움 로컬의 키파일을 forwarding하여 접속 시에 사용 (권장) 그중 개발자 개개인의 로컬 키파일을 forwarding 하는 사용하는 방식으로 적용해 보자.\neval \u0026#34;$(ssh-agent -s)\u0026#34; \u0026amp;\u0026amp; ssh-add \u0026#34;키파일\u0026#34; \u0026amp;\u0026amp; sudo ssh -A -i \u0026#34;키파일\u0026#34; ec2-user@\u0026#34;IP 이렇게 Bastion서버에 접속하면, Bastion 서버에서 로컬 키파일을 forwarding 받아 바로 다른 서버로 ssh 접속이 가능하다.\n2-3. 호스트 등록 bastian 서버에서 매번 ssh 커맨드와 여러 개의 ip를 직접 입력하는 것은 불편하고 위험하다.\n그럴 때 Bastion 서버에서 ~/. ssh/config 파일에 호스트를 등록하고 간편하게 사용 가능하다.\n이렇게 설정하면 (여러 개 설정 가능) ssh \u0026#34;호스트명 으로 각 서버에 바로 ssh 접속이 가능하다.\n3. 서버별 접속상태 구분하기 이렇게 bastion 서버에서 여러 서버로의 접근은 이제 가능하지만, 불편한 점이 있다. bastion에서 수십 개의 서버를 접속한다고 하면, 자동등록된 호스트명으로 접속하더라도 접속이 잘되었는지, ssh 연결이 끊어졌는지 구분이 잘 되지 않아 불편하고 사고로 이어질 수 있다.\n3-1. PS1환경변수 사용 콘솔에서 서버 간의 구분을 명확히 하기 위해 PS1 환경변수를 사용해 프롬프트를 변경하거나 표시되는 정보를 추가할 수 있다,\nBastion 서버의 `~/. bashrc` 또는 `~/. zshrc` 파일에 다음을 추가해 보자\nexport PS1=\u0026#34;[\\e[33m][Bastion: \\u@\\h \\W]\\$[\\e[0m] [Bastion:\\...]: 프롬프트에 \u0026ldquo;Bastion\u0026quot;이라는 표시를 추가. \\u: 사용자 이름. \\h: 호스트 이름. \\W: 현재 디렉터리. 변경 후 source ~/. bashrc로 적용을 하면, bastion 서버에 연결된 상태일 때는 다음과 같이 표시된다.\n필요하다면 각 서버에 PS1환경변수로 그에 맞는 헤더를 설정하면, 쉽게 구분된 상태로 서버 간 접속을 할 수 있다. ","permalink":"http://localhost:50666/posts/123/","summary":"\u003chr\u003e\n\u003ch3 id=\"bastion-서버란\" ke-size=\"size23\"\u003e1. Bastion 서버란?\u003c/h3\u003e\n\u003cp\u003eBastion 서버란 클라우드 환경, 네트워크 환경에서 보안 게이트웨이 역할을 하는 서버로, 외부 네트워크에서 private 서버로 접속할 때 보안을 강화해 주고 접근제어를 구현해 준다. 외부 사용자가 특정 IP에서만 Bastion 서버에 접속하도록 설정할 수 있으며, private 서버들은 Bastion 서버를 통해서 트래픽을 철저하게 관리할 수 있다. 이번 포스트에선 Bastion서버의 설정 방법과, 다중 서버를 Bastion 서버에서 관리할 시 접속상태를 한눈에 구분할 수 있는 방법에 대해 알아보자.\u003c/p\u003e\n\u003ch3 id=\"bastion-서버-설정-방법\" ke-size=\"size23\"\u003e2. Bastion 서버 설정 방법\u003c/h3\u003e\n\u003ch4 id=\"aws-인스턴스-생성\" ke-size=\"size20\"\u003e2-1.aws 인스턴스 생성 \u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003et2 micro~t3 micro - 소규모 개발팀\u003c/li\u003e\n\u003cli\u003et3 small  - 중간 규모팀\u003c/li\u003e\n\u003cli\u003eT3.medium, M5.large - 대규모팀 (다수 개발자, 운영팀)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/123/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-12-31%20%EC%98%A4%ED%9B%84%2012.14.14.png\"\u003e\u003c/p\u003e","title":"[AWS] Bastion 서버 설정 및 서버 접속 상태 한눈에 구분하기"},{"content":" 1. 문제 발생 Quota discoveryengine.googleapis.com/documents exceeded.\nGCP에서 MLOps를 구축 중, RAG를 위한 Discovery engine의 데이터 최대 수량이 초과되었다는 경고를 받게 되었다.\n기본 리밋은 100만건이지만 더 많은 데이터를 저장하기 위해서는 할당량 수정 요청이 필요하다.\n2. 할당량 수정 요청 2-1. IAM \u0026amp; 관리자 -\u0026gt; 할당량 및 시스템 한도 2-2. 초과된 항목 선택 할당량 수정을 원하는 항목의 맨 오른쪽 메뉴 탭에서 할당량 변경을 선택할 수 있다.\n2-3. 할당량 변경 신청 할당량 변경 신청을 요청하면, 2~3일 이내에 회신을 받을 수 있다. (내 경우 1일 만에 회신을 받았으나, GCP 계정을 만든 지 얼마 되지 않아 승인이 어렵다는 답변을 받아서, 재심사 요청한 상태이다, 최종 결과가 나오면 포스트를 업데이트할 예정)\n","permalink":"http://localhost:50666/posts/122/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/122/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-12-17%20%EC%98%A4%ED%9B%84%2010.49.42.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"문제-발생\" ke-size=\"size23\"\u003e1. 문제 발생\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eQuota discoveryengine.googleapis.com/documents exceeded.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eGCP에서 MLOps를 구축 중, RAG를 위한 Discovery engine의 데이터 최대 수량이 초과되었다는 경고를 받게 되었다.\u003c/p\u003e\n\u003cp\u003e기본 리밋은 100만건이지만 더 많은 데이터를 저장하기 위해서는 할당량 수정 요청이 필요하다.\u003c/p\u003e\n\u003ch3 id=\"할당량-수정-요청\" ke-size=\"size23\"\u003e2. 할당량 수정 요청\u003c/h3\u003e\n\u003ch4 id=\"iam-관리자---할당량-및-시스템-한도\" ke-size=\"size20\"\u003e2-1. IAM \u0026amp; 관리자 -\u0026gt; 할당량 및 시스템 한도\u003c/h4\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/122/img.png\"\u003e\u003c/p\u003e\n\u003ch4 id=\"초과된-항목-선택\" ke-size=\"size20\"\u003e2-2. 초과된 항목 선택\u003c/h4\u003e\n\u003cp\u003e할당량 수정을 원하는 항목의 맨 오른쪽 메뉴 탭에서 할당량 변경을 선택할 수 있다.\u003c/p\u003e","title":"[LLM] Quota discoveryengine.googleapis.com/documents exceeded."},{"content":" 1. Google Cloud Discovery Engine이란? Google Cloud Discovery Engine은 구글 클라우드 플랫폼에서 제공하는 검색 및 추천 서비스로, 웹사이트나 앱 내에서 사용자가 원하는 정보를 쉽고 빠르게 찾을 수 있도록 도와주는 서비스로 다음과 같은 특징을 가진다.\n고급 검색 기능 : 단순 키워드 검색이 아닌, 사용자의 의도에 맞춰 의미를 파악하고 관련 콘텐츠를 제안하는 자연어 처리(NLP) 기반 검색을 지원 개인화된 추천 : 머신러닝 기반 추천엔진을 활용, 취향과 행동 패턴에 맞춘 추천 콘텐츠 제공 확장 가능성 : 다양한 규모와 범위의 콘텐츠에 대해 빠른 검색 및 추천 성능유지 2. 문제상황 Google cloud discovery engine을 활용해 rag를 고도화 시도하던 중, 기존 데이터베이스의 단순 question, answer 필드구조를 csv형태로 업로드하던 방식을 벗어나 metadata, description 필드를 구성하고자 하였다. content필드를 만족하는 데이터 구조를 구성하기 위해 기존 데이터셋을 discovery engine이 요구하는 특정 struct_value, string_value 타입의 jsonl 포맷으로 변환하는 과정에서 여러 에러를 만나게 되었다.\n2-1. 기존 question, answer를 유지한 채 content를 변경 (실패) {\u0026#34;id\u0026#34;:\u0026#34;doc1\u0026#34;,\u0026#34;content\u0026#34;:{\u0026#34;question\u0026#34;:\u0026#34;어떤 기능에 대한 질문\u0026#34;,\u0026#34;answer\u0026#34;:\u0026#34;기능에 대한 자세한 설명\u0026#34;, \u0026#34;metadata\u0026#34;:\u0026#34;메타데이터\u0026#34;}} [invalid JSON in google.cloud.discoveryengine.v1 main.Document @ content:...] [near 1:42 (offset 41): no such field: 'question'] * 그 외 title, text 같은 여러 필드로도 시도해 보았지만, 여전히 제공하지 않는 필드 에러 발생 2-2. content를 json 객체가 아닌 문자열로 변경 (실패) {\u0026#34;id\u0026#34;:\u0026#34;doc1\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;내용\u0026#34;, \u0026#34;question\u0026#34;:\u0026#34;질문\u0026#34;, \u0026#34;answer\u0026#34;:\u0026#34;답변\u0026#34;} [invalid JSON in google.cloud.discoveryengine.v1 main.Document, near 1:1 (offset 0): unexpected character: 'j'; expected '{']\n3. 정상적인 데이터 포맷 2-1, 2-2 외에도 기존에 사용하던 document db포맷을 생각하며 다양한 시도를 해보았지만 모두 실패하였고, 역시 공식문서를 꼼꼼히 확인했어야 했다.\nstruct_value : json 객체를 저장하는 경우 string_value : 단순 문자열로 저장하는 경우 (두 가지 중 한 가지만 사용 가능하다. )\n{\u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;struct_data\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;이름입니다.\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;설명\u0026#34;, \u0026#34;feature1\u0026#34;: \u0026#34;특징1\u0026#34;, \u0026#34;features2\u0026#34;: [\u0026#34;feature2-1\u0026#34;, \u0026#34;feature2-2\u0026#34;]}} {\u0026#34;id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;struct_data\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;이름2입니다.\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;설명2.\u0026#34;, \u0026#34;feature1\u0026#34;: \u0026#34;특징1\u0026#34;, \u0026#34;features2\u0026#34;: [\u0026#34;feature2-1\u0026#34;, \u0026#34;feature2-2\u0026#34;]}} [다음과 같이 struct_data로 규격화된 데이터를 명시해 주면 정상적으로 업로드가 가능하다.] 참고\nhttps://cloud.google.com/php/docs/reference/cloud-discoveryengine/0.4.0/V1.Document.Content ","permalink":"http://localhost:50666/posts/121/","summary":"\u003chr\u003e\n\u003ch3 id=\"google-cloud-discovery-engine이란\" ke-size=\"size23\"\u003e1. Google Cloud Discovery Engine이란?\u003c/h3\u003e\n\u003cp\u003eGoogle Cloud Discovery Engine은 구글 클라우드 플랫폼에서 제공하는 검색 및 추천 서비스로, 웹사이트나 앱 내에서 사용자가 원하는 정보를 쉽고 빠르게 찾을 수 있도록 도와주는 서비스로 다음과 같은 특징을 가진다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e고급 검색 기능\u003c/strong\u003e : 단순 키워드 검색이 아닌, 사용자의 의도에 맞춰 의미를 파악하고 관련 콘텐츠를 제안하는 자연어 처리(NLP) 기반 검색을 지원\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e개인화된 추천\u003c/strong\u003e : 머신러닝 기반 추천엔진을 활용, 취향과 행동 패턴에 맞춘 추천 콘텐츠 제공\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e확장 가능성\u003c/strong\u003e : 다양한 규모와 범위의 콘텐츠에 대해 빠른 검색 및 추천 성능유지\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"문제상황\" ke-size=\"size23\"\u003e2. 문제상황\u003c/h3\u003e\n\u003cp\u003eGoogle cloud discovery engine을 활용해 rag를 고도화 시도하던 중, 기존 데이터베이스의 단순 question, answer 필드구조를 csv형태로 업로드하던 방식을 벗어나 metadata, description 필드를 구성하고자 하였다. content필드를 만족하는 데이터 구조를 구성하기 위해 기존 데이터셋을 discovery engine이 요구하는 특정 struct_value, string_value 타입의 jsonl 포맷으로 변환하는 과정에서 여러 에러를 만나게 되었다.\u003c/p\u003e","title":"[LLM] Google Cloud Discovery Engine 데이터 스토어 업로드 포맷"},{"content":" 4. HOW DEEP - 얼마나 깊게 테스트 코드를 작성해야 하는가? 4.1 테스트 깊이를 결정하는 기준 테스트 깊이를 설정할 때는 다음과 같은 기준을 고려해야 합니다:\n테스트 피라미드(Test Pyramid): 테스트 피라미드는 테스트 종류에 따른 계층 구조를 보여줍니다. 일반적으로 단위 테스트가 가장 많고, 그다음으로 통합 테스트, 시스템 또는 E2E(End-to-End) 테스트가 위치합니다. 단위 테스트(Unit Tests): 가장 많은 비중을 차지하며, 작은 코드 단위를 독립적으로 테스트합니다. 통합 테스트(Integration Tests): 여러 모듈이 상호작용하는지 테스트합니다. E2E 테스트(End-to-End Tests): 실제 사용자 관점에서 전체 시스템이 잘 작동하는지 확인합니다. 위험 기반 테스트(Risk-Based Testing): 비즈니스 중요도와 잠재적 위험 요소에 따라 테스트 우선순위를 설정합니다. 비즈니스에 중요한 기능이나 리스크가 높은 부분에 대한 테스트는 더 깊이 있게 수행합니다. 유스 케이스 기반 테스트: 핵심 사용자 흐름과 엣지 케이스를 기반으로 테스트를 작성합니다. 실제로 사용자가 자주 사용하는 기능이나 예외적인 상황에서의 동작을 검증하는 것이 중요합니다. 현실적인 제약과 팀 역량 고려: 모든 부분을 깊이 테스트하는 것은 시간과 리소스 측면에서 비효율적일 수 있습니다. 팀의 역량과 프로젝트 일정 등을 고려하여 테스트 깊이를 조정하는 것이 필요합니다. 이미지 출처: https://www.headspin.io/blog/the-testing-pyramid-simplified-for-one-and-allMasteringtheTestPyramid\n4.2 테스트 커버리지 및 품질 지표 활용 코드 커버리지: 코드 커버리지는 테스트가 소스 코드의 얼마나 많은 부분을 실행하는지를 나타내는 지표입니다. 일반적으로 라인 커버리지(Line Coverage)와 브랜치 커버리지(Branch Coverage)를 측정합니다. 라인 커버리지(Line Coverage): 테스트가 실행된 코드 라인의 비율. 브랜치 커버리지(Branch Coverage): 분기문(예: if/else)의 각 분기를 테스트했는지 확인하는 비율. 다음은 본 리포지토리 샘플 소스의 테스트 커버리지입니다. 커버리지 목표 설정: 높은 커버리지는 중요하지만, 무조건 100% 커버리지를 목표로 하는 것은 오히려 비효율적일 수 있습니다. 핵심 비즈니스 로직이나 복잡한 부분에 집중하여 테스트 깊이를 설정하는 것이 중요합니다. 4.3 오버테스팅의 문제점 유지보수 비용 증가: 불필요하게 많은 테스트는 유지보수 부담을 가중시킬 수 있습니다. 코드가 변경될 때마다 테스트도 함께 수정해야 할 수 있으며, 이는 오히려 생산성을 저해할 수 있습니다. 개발 속도 저하: 모든 기능을 테스트하려다 보면 개발 속도가 느려질 수 있습니다. 핵심적인 부분에 집중하는 것이 효율적입니다. 리소스 낭비: 지나치게 많은 테스트는 리소스를 낭비하게 만듭니다. 테스트를 효율적으로 유지하는 것이 중요합니다. 4.4 효율적인 테스트 범위 설정을 위한 체크리스트 핵심 로직에 대한 집중적인 테스트: 주요 비즈니스 로직을 검증합니다. 경계 값 및 예외 상황을 철저히 테스트합니다. 주요 기능 및 사용자 시나리오 검증: 핵심 사용자 흐름을 테스트하여 애플리케이션의 주요 사용 시나리오가 올바르게 작동하는지 확인합니다. 다양한 사용자 역할 및 권한에 따른 테스트를 수행합니다. 에러 및 예외 처리에 대한 테스트: 오류 메시지 및 예외 처리 로직을 검증합니다. 인증 및 권한 부여와 관련된 로직에 대한 테스트를 강화합니다. 외부 시스템 및 통합 부분 테스트: API 연동 및 응답 처리에 대한 테스트를 포함합니다. 데이터베이스 트랜잭션 일관성 검증 등을 고려합니다. 4.5 리팩토링과 테스트의 균형 잡기 필요한 부분에 집중하여 테스트 작성: 모든 부분을 테스트하기보다는 중요한 부분에 집중하여 테스트합니다. 불필요하거나 중복된 테스트 코드 제거: 리팩토링 과정에서 불필요하거나 중복된 테스트 코드는 제거해야 합니다. 테스트 코드의 유지보수성과 가독성 확보: 테스트 코드도 프로덕션 코드처럼 유지보수성과 가독성을 확보해야 합니다. ","permalink":"http://localhost:50666/posts/99/","summary":"\u003chr\u003e\n\u003ch2 id=\"how-deep---얼마나-깊게-테스트-코드를-작성해야-하는가\" ke-size=\"size26\"\u003e4. HOW DEEP - 얼마나 깊게 테스트 코드를 작성해야 하는가?\u003c/h2\u003e\n\u003ch3 id=\"테스트-깊이를-결정하는-기준\" ke-size=\"size23\"\u003e4.1 테스트 깊이를 결정하는 기준\u003c/h3\u003e\n\u003cp\u003e테스트 깊이를 설정할 때는 다음과 같은 기준을 고려해야 합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e테스트 피라미드(Test Pyramid)\u003c/strong\u003e: 테스트 피라미드는 테스트 종류에 따른 계층 구조를 보여줍니다. 일반적으로 단위 테스트가 가장 많고, 그다음으로 통합 테스트, 시스템 또는 E2E(End-to-End) 테스트가 위치합니다.\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e단위 테스트(Unit Tests)\u003c/strong\u003e: 가장 많은 비중을 차지하며, 작은 코드 단위를 독립적으로 테스트합니다.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e통합 테스트(Integration Tests)\u003c/strong\u003e: 여러 모듈이 상호작용하는지 테스트합니다.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eE2E 테스트(End-to-End Tests)\u003c/strong\u003e: 실제 사용자 관점에서 전체 시스템이 잘 작동하는지 확인합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e위험 기반 테스트(Risk-Based Testing)\u003c/strong\u003e: 비즈니스 중요도와 잠재적 위험 요소에 따라 테스트 우선순위를 설정합니다. 비즈니스에 중요한 기능이나 리스크가 높은 부분에 대한 테스트는 더 깊이 있게 수행합니다.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e유스 케이스 기반 테스트\u003c/strong\u003e: 핵심 사용자 흐름과 엣지 케이스를 기반으로 테스트를 작성합니다. 실제로 사용자가 자주 사용하는 기능이나 예외적인 상황에서의 동작을 검증하는 것이 중요합니다.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e현실적인 제약과 팀 역량 고려\u003c/strong\u003e: 모든 부분을 깊이 테스트하는 것은 시간과 리소스 측면에서 비효율적일 수 있습니다. 팀의 역량과 프로젝트 일정 등을 고려하여 테스트 깊이를 조정하는 것이 필요합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/99/img.png\"\u003e\u003c/p\u003e","title":"[Spring] 테스트 4 - 얼마나 깊게 테스트 코드를 작성해야 하는가?"},{"content":" 3. WHEN - 언제 테스트 코드를 적용해야 하는가? 3.1 TDD와 BDD의 개념 및 적용 시점 TDD (Test-Driven Development): TDD는 테스트를 먼저 작성하고, 그 테스트를 통과할 수 있는 최소한의 코드를 작성하며 개발을 진행하는 방법론입니다. TDD는 테스트를 통해 명확한 요구사항을 확인하고 코드 품질을 보장하는 방법으로 활용됩니다. 레드-그린-리팩터 사이클: TDD의 기본 개발 사이클은 레드 단계 (실패하는 테스트 작성) → 그린 단계 (테스트를 통과하기 위한 코드 작성) → 리팩터 단계 (코드 정리 및 최적화)로 이루어집니다. 적용 시점: 새로운 기능을 개발하거나 기존 코드를 리팩터링 할 때, TDD를 통해 코드의 안정성과 유지보수성을 높일 수 있습니다. 이미지출처: https://medium.com/pilar-2020/applying-test-driven-development-6d6d3af186cbApplying\\ Test-Driven\\ Development\nBDD (Behavior-Driven Development): BDD는 사용자의 관점에서 시스템의 동작(Behavior)을 기술하고, 그에 맞는 테스트를 작성하여 개발을 진행하는 방법론입니다. BDD는 테스트를 통해 요구사항을 명확히 하고, 기능적인 동작을 검증합니다. Given-When-Then 패턴: BDD의 테스트는 Given (어떤 상황이 주어졌을 때), When (어떤 동작이 수행되었을 때), Then (그 결과로 어떤 일이 발생해야 한다)의 패턴을 따릅니다. 적용 시점: 새로운 요구사항이 정의될 때, BDD를 통해 고객의 요구사항을 명확히 이해하고 구현할 수 있습니다. 이미지 출처: https://medium.com/@dineshrajdhanapathy/writing-human-readable-tests-a-guide-to-effective-bdd-practices-75a7ab7888bbWritingHuman-ReadableTests:AGuidetoEffectiveBDDPractices\n3.2 기존 코드베이스에 테스트 추가하기 레거시 코드베이스에 테스트 추가 전략: 기존 프로젝트에 테스트 코드를 추가할 때는 우선순위를 정하고, 주요 기능이나 자주 변경되는 코드부터 테스트를 작성하는 것이 중요합니다. 레거시 코드베이스에 테스트를 추가할 때 다음과 같은 전략을 사용할 수 있습니다. 핵심 비즈니스 로직에 집중: 테스트 작성의 우선순위는 핵심 기능, 주요 비즈니스 로직에 집중해야 합니다. 테스트 가능성 개선: 레거시 코드가 테스트하기 어렵다면, 코드의 모듈화 또는 의존성 분리(Dependency Injection) 등을 통해 테스트 가능성을 높이는 작업이 필요합니다. 리팩토링 후 테스트 작성: 테스트를 추가하기 전에, 코드가 너무 복잡하거나 결합도가 높다면 리팩토링을 먼저 진행한 후 테스트를 작성하는 것이 좋습니다. 3.3 새로운 기능 개발 시 테스트 작성 시점 프로덕션 코드보다 테스트 코드를 먼저 작성: TDD 원칙에 따라 새로운 기능을 개발할 때, 테스트를 먼저 작성하고 그 테스트를 통과할 수 있는 최소한의 프로덕션 코드를 작성하는 방식입니다. 테스트 우선 작성의 장점: 테스트를 먼저 작성함으로써, 새로운 기능에 대한 요구사항을 명확히 정의하고, 코드 작성 전에 논리적인 오류를 미리 방지할 수 있습니다. 테스트 코드 기반의 개발 흐름: 테스트를 작성하고 그 결과에 따라 프로덕션 코드를 작성함으로써, 테스트 주도 개발 흐름을 유지할 수 있습니다. 3.4 리팩토링 시 테스트의 역할 기존 기능의 안정성 확보: 리팩토링은 코드의 동작을 변경하지 않고 구조를 개선하는 작업입니다. 이 과정에서 테스트 코드는 기존 기능이 올바르게 동작하는지를 검증하는 역할을 합니다. 리팩토링 후에도 테스트가 통과하는지 확인: 리팩토링 후 기존 테스트가 모두 통과한다면, 기존 기능에 이상이 없음을 보장할 수 있습니다. 테스트가 없는 리팩토링은 위험: 리팩토링 전에 반드시 충분한 테스트 커버리지를 확보해야 하며, 그렇지 않으면 리팩토링 과정에서 의도치 않은 버그가 발생할 수 있습니다. 3.5 테스트 작성의 우선순위와 체크리스트 테스트 작성 시 고려해야 할 우선순위와 체크리스트는 다음과 같습니다.\n핵심 로직 및 비즈니스 규칙: 가장 중요한 비즈니스 로직에 대해 우선적으로 테스트를 작성해야 합니다. 주요 사용 사례, 경곗값 처리, 예외 상황 등이 여기에 포함됩니다. 에러 및 예외 처리: 예외 상황에 대한 테스트를 포함해야 합니다. 예외가 제대로 처리되고, 사용자가 오류를 이해할 수 있도록 명확한 메시지가 제공되는지 확인해야 합니다. 테스트 피라미드 접근: 단위 테스트, 통합 테스트, 시스템 테스트 순으로 우선순위를 정하고, 테스트의 깊이와 범위를 결정해야 합니다. ","permalink":"http://localhost:50666/posts/98/","summary":"\u003chr\u003e\n\u003ch1 id=\"3-when---언제-테스트-코드를-적용해야-하는가\"\u003e3. WHEN - 언제 테스트 코드를 적용해야 하는가?\u003c/h1\u003e\n\u003ch2 id=\"tdd와-bdd의-개념-및-적용-시점\" ke-size=\"size26\"\u003e3.1 TDD와 BDD의 개념 및 적용 시점\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTDD (Test-Driven Development)\u003c/strong\u003e: TDD는 테스트를 먼저 작성하고, 그 테스트를 통과할 수 있는 최소한의 코드를 작성하며 개발을 진행하는 방법론입니다. TDD는 테스트를 통해 명확한 요구사항을 확인하고 코드 품질을 보장하는 방법으로 활용됩니다.\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e레드-그린-리팩터 사이클\u003c/strong\u003e: TDD의 기본 개발 사이클은 \u003ccode\u003e레드 단계\u003c/code\u003e (실패하는 테스트 작성) → \u003ccode\u003e그린 단계\u003c/code\u003e (테스트를 통과하기 위한 코드 작성) → \u003ccode\u003e리팩터 단계\u003c/code\u003e (코드 정리 및 최적화)로 이루어집니다.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e적용 시점\u003c/strong\u003e: 새로운 기능을 개발하거나 기존 코드를 리팩터링 할 때, TDD를 통해 코드의 안정성과 유지보수성을 높일 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/98/img.png\"\u003e\n이미지출처: \u003ca href=\"https://medium.com/pilar-2020/applying-test-driven-development-6d6d3af186cbApplying\"\u003ehttps://medium.com/pilar-2020/applying-test-driven-development-6d6d3af186cbApplying\u003c/a\u003e\\ Test-Driven\\ Development\u003c/p\u003e","title":"[Spring] 테스트 3 - 언제 테스트 코드를 적용해야 하는가?"},{"content":" 2. HOW - 테스트 코드를 어떻게 작성해야 하는가? 2.1 테스트 케이스 선택 방법 첫 번째 테스트의 중요성: 구현하기 가장 쉬운 테스트부터 시작하는 것이 좋습니다. 예외적인 상황이나 가장 빠르게 개발할 수 있는 테스트 케이스를 먼저 작성하고, 점차 확장해 나갑니다. 점진적 확장: 쉬운 테스트부터 시작해 점차 복잡한 테스트로 나아가면서 시스템의 안정성을 검증합니다. 2.2 TDD (Test-Driven Development) 방법론 TDD는 테스트 주도 개발 방식으로, 테스트 코드를 먼저 작성하고 이를 기반으로 프로덕션 코드를 작성하는 방식입니다. TDD는 다음과 같은 세 단계를 따릅니다:\n레드 단계: 실패하는 테스트를 작성합니다. 이때, 아직 프로덕션 코드는 작성되지 않았기 때문에 테스트는 실패합니다. 그린 단계: 최소한의 코드로 테스트를 통과시킵니다. 테스트를 성공시키기 위한 코드만 작성하여 빠르게 테스트를 통과합니다. 리팩터 단계: 중복을 제거하고 코드 구조를 개선합니다. 테스트가 통과한 후 코드의 가독성이나 유지보수성을 높이기 위해 리팩터링을 진행합니다. 이 과정을 반복하면서 점진적으로 시스템을 구축하고 테스트의 커버리지를 높여갑니다.\n2.3 다양한 테스트 종류와 계층 구조 이해 단위 테스트: 개별 모듈이나 메서드를 테스트하는 방식으로, 가장 기본적인 테스트 방법입니다. 빠르고, 독립적으로 실행될 수 있습니다. 통합 테스트: 여러 모듈이 함께 작동하는지를 테스트하는 방식입니다. 단위 테스트보다 더 복잡한 시나리오를 검증할 수 있습니다. 시스템 테스트: 전체 시스템이 의도한 대로 작동하는지를 검증하는 테스트로, 사용자의 관점에서 테스트를 진행합니다. 2.4 JUnit5 활용 JUnit5는 자바 테스트를 위한 표준 프레임워크입니다. 다음과 같은 주요 구성 요소와 기능을 갖추고 있습니다:\n주요 어노테이션 @Test: 테스트 메서드를 나타냅니다. @BeforeAll, @BeforeEach: 각각 전체 테스트 전, 개별 테스트 전 실행될 메서드를 정의합니다. @AfterEach, @AfterAll: 각각 개별 테스트 후, 전체 테스트 후 실행될 메서드를 정의합니다. 주요 어서션 메서드 assertEquals(expected, actual): 기대값과 실제값이 일치하는지 확인합니다. assertNull(object): 객체가 null인지 검증합니다. assertThrows(): 예외가 발생하는지 검증합니다. 참조 코드:\nJUnit 기본 테스트 예시 - UserServiceImplTest\nhttps://github.com/junhkang/springboot-testing-from-zero-to-hero/blob/main/src/test/java/io/github/junhkang/springboottesting/service/impl/JpaUserServiceImplTest.java\n2.5 Mockito와 같은 Mocking 프레임워크 사용 Mocking은 외부 의존성을 모방하여 테스트하는 방법입니다. Mockito와 같은 프레임워크를 사용하여 쉽게 Mock 객체를 생성하고 테스트할 수 있습니다.\n테스트 더블의 종류 Dummy: 사용되지 않는 매개변수에 전달되는 객체입니다. Stub: 미리 정의된 결과를 반환하는 객체입니다. Mock: 동작을 검증할 수 있는 객체로, 메서드 호출 여부 등을 검증합니다. Spy: 실제 객체의 동작을 일부 모니터링하거나 수정하는 객체입니다. Fake: 실제 동작을 구현하지만, 단순하게 동작하는 테스트 객체입니다. 참조 코드: https://github.com/junhkang/springboot-testing-from-zero-to-hero/blob/main/src/test/java/io/github/junhkang/springboottesting/controller/UserControllerTest.java\nMockito를 활용한 UserControllerTest에서 Mock 객체 활용 예시\nhttps://github.com/junhkang/springboot-testing-from-zero-to-hero/blob/main/src/test/java/io/github/junhkang/springboottesting/controller/UserControllerTest.java\n2.6 다양한 테스트 어노테이션 및 도구 활용 @ParameterizedTest: 여러 파라미터를 전달하여 같은 테스트를 반복 실행합니다. https://github.com/junhkang/springboot-testing-from-zero-to-hero/blob/main/src/test/java/io/github/junhkang/springboottesting/service/impl/JpaOrderServiceImplTest.java#L103 참조코드 @ValueSource: 다양한 값을 전달합니다. @EnumSource: Enum 타입의 파라미터를 전달합니다. @MethodSource: 메서드를 통해 테스트 데이터를 제공합니다. @Nested: 테스트를 그룹화하여 계층적으로 관리할 수 있습니다. https://github.com/junhkang/springboot-testing-from-zero-to-hero/blob/main/src/test/java/io/github/junhkang/springboottesting/service/impl/JpaOrderServiceImplTest.java#L81 참조코드 Nested를 사용하면 테스트를 논리적인 그룹으로 분리할 수 있으며, 이를 통해 특정 도메인, 기능, 시나리오에 대한 테스트를 더욱 체계적으로 관리할 수 있습니다. 특히, 여러 테스트 메서드를 계층적으로 정리하여 가독성을 높이고, 테스트 목적이 더 명확해지도록 도와줍니다. @DisplayName: 테스트의 설명을 추가하여 가독성을 높일 수 있습니다. https://github.com/junhkang/springboot-testing-from-zero-to-hero/blob/main/src/test/java/io/github/junhkang/springboottesting/service/impl/JpaOrderServiceImplTest.java#L82 참조코드 DisplayName을 통해 각 테스트 메서드에 대해 직관적인 설명을 부여할 수 있으며, 이는 테스트 결과 리포트에서도 그대로 반영되어 가독성을 크게 향상합니다. 테스트 트리를 시각화할 때 각 테스트의 목적과 역할을 쉽게 이해할 수 있도록 도와줍니다. @Timeout: 테스트 실행 시간에 제한을 두어, 성능을 테스트할 수 있습니다. https://github.com/junhkang/springboot-testing-from-zero-to-hero/blob/main/src/test/java/io/github/junhkang/springboottesting/service/impl/JpaOrderServiceImplTest.java#L236 참조코드 @RepeatedTest: 동일한 테스트를 여러 번 반복 실행합니다. https://github.com/junhkang/springboot-testing-from-zero-to-hero/blob/main/src/test/java/io/github/junhkang/springboottesting/service/impl/JpaOrderServiceImplTest.java#L357 참조코드 RepeatedTest는 동일한 테스트를 여러 번 반복 실행하여, 특정 코드가 여러 실행 환경에서도 일관되게 동작하는지 확인하는 데 유용합니다. 성능 테스트나 동시성 이슈를 확인할 때 자주 사용됩니다. @SpringBootTest: 전체 스프링 컨텍스트를 로드하여 통합 테스트를 수행합니다. Testcontainers: 컨테이너 환경을 활용하여 통합 테스트를 진행할 수 있습니다. 가독성 향상된 테스트 트리 예시 @Nested와 @DisplayName을 적절히 사용하면 아래와 같이 가독성이 높은 테스트 트리를 구성할 수 있습니다:\n참조 코드: https://github.com/junhkang/springboot-testing-from-zero-to-hero/blob/main/src/test/java/io/github/junhkang/springboottesting/service/impl/JpaOrderServiceImplTest.java\nSpringBootTest와 다양한 어노테이션 활용 예시 - OrderControllerTest\nhttps://github.com/junhkang/springboot-testing-from-zero-to-hero/blob/main/src/test/java/io/github/junhkang/springboottesting/service/impl/JpaOrderServiceImplTest.java 2.7 MockMvc와 WebTestClient를 사용한 웹 레이어 테스트 웹 레이어 테스트를 위해 MockMvc와 WebTestClient를 사용합니다.\nMockMvc: Spring MVC를 모킹 하여 웹 애플리케이션의 HTTP 요청 및 응답을 테스트합니다. WebTestClient: WebFlux를 지원하는 비동기식 테스트 클라이언트로, 웹 애플리케이션의 반응형 동작을 검증할 수 있습니다. 참조 코드: https://github.com/junhkang/springboot-testing-from-zero-to-hero/blob/main/src/test/java/io/github/junhkang/springboottesting/controller/UserControllerTest.java\nMockMvc를 사용한 Web Layer 테스트 예시 - UserControllerTest\nhttps://github.com/junhkang/springboot-testing-from-zero-to-hero/blob/main/src/test/java/io/github/junhkang/springboottesting/controller/UserControllerTest.java ","permalink":"http://localhost:50666/posts/97/","summary":"\u003chr\u003e\n\u003ch1 id=\"2-how---테스트-코드를-어떻게-작성해야-하는가\"\u003e2. HOW - 테스트 코드를 어떻게 작성해야 하는가?\u003c/h1\u003e\n\u003ch2 id=\"테스트-케이스-선택-방법\" ke-size=\"size26\"\u003e2.1 테스트 케이스 선택 방법\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e첫 번째 테스트의 중요성\u003c/strong\u003e: 구현하기 가장 쉬운 테스트부터 시작하는 것이 좋습니다. 예외적인 상황이나 가장 빠르게 개발할 수 있는 테스트 케이스를 먼저 작성하고, 점차 확장해 나갑니다.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e점진적 확장\u003c/strong\u003e: 쉬운 테스트부터 시작해 점차 복잡한 테스트로 나아가면서 시스템의 안정성을 검증합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"tdd-test-driven-development-방법론\" ke-size=\"size26\"\u003e2.2 TDD (Test-Driven Development) 방법론\u003c/h2\u003e\n\u003cp\u003eTDD는 테스트 주도 개발 방식으로, 테스트 코드를 먼저 작성하고 이를 기반으로 프로덕션 코드를 작성하는 방식입니다. TDD는 다음과 같은 세 단계를 따릅니다:\u003c/p\u003e","title":"[Spring] 테스트 2 - 테스트 코드를 어떻게 작성해야 하는가?"},{"content":" 1. WHY - 왜 테스트를 작성해야 하는가? 1.1 테스트 코드의 중요성 테스트 코드는 소프트웨어 개발에서 매우 중요한 역할을 합니다. 기능을 수정하거나 새로운 기능을 추가할 때 코드가 안정적으로 작동하는지 확인할 수 있는 수단이 바로 테스트 코드입니다. 이를 통해 예상하지 못한 버그를 방지하고, 코드 품질을 높일 수 있습니다.\n1.2 테스트 코드 작성의 장점 1.2.1 안정적인 개발 환경 구축 테스트 코드는 코드의 변경이 다른 기능에 미치는 영향을 최소화하는 데 도움을 줍니다. 개발자는 자신 있게 코드를 수정하거나 리팩터링 할 수 있으며, 기존 기능이 예상대로 작동하는지 검증할 수 있습니다.\n1.2.2 버그 감소 및 코드 품질 향상 테스트 코드를 통해 코드 내 버그를 사전에 발견하고 해결할 수 있습니다. 이를 통해 운영 환경에서 발생할 수 있는 문제를 줄이고, 최종 사용자에게 더 나은 품질의 소프트웨어를 제공할 수 있습니다.\n1.2.3 리팩토링의 용이성 테스트 코드가 있는 경우, 코드의 리팩토링을 안전하게 수행할 수 있습니다. 테스트는 코드 변경 후에도 기능이 정상적으로 작동하는지 확인해주므로, 리팩토링 과정에서 발생할 수 있는 예기치 않은 문제를 방지할 수 있습니다.\n1.2.4 단일 책임 원칙(SOLID) 준수 테스트 코드를 작성하다 보면 자연스럽게 단일 책임 원칙(Single Responsibility Principle)을 준수하게 됩니다. 이는 각 클래스와 메서드가 하나의 책임만을 가지도록 하며, 유지보수가 용이한 코드를 작성하는 데 도움을 줍니다.\n1.3 테스트를 작성하지 않았을 때의 문제점 테스트 코드가 없을 경우 다음과 같은 문제들이 발생할 수 있습니다:\n디버깅 시간 증가: 코드에 문제가 발생했을 때 원인을 빠르게 파악하기 어렵습니다. 리팩토링의 두려움: 테스트가 없는 상태에서 리팩토링을 진행하면 기존 코드가 깨질 위험이 커집니다. 기능 추가 시 불안정성: 새로운 기능을 추가할 때 기존 기능이 정상적으로 동작하는지 확인할 수 없어 기능 간 충돌이 발생할 수 있습니다. 유지보수의 어려움: 시간이 지남에 따라 프로젝트의 복잡도가 증가하면, 테스트가 없는 시스템은 유지보수 비용이 급격히 증가합니다. 1.4 좋은 테스트 코드 - FIRST 원칙 좋은 테스트 코드는 다음의 FIRST 원칙을 준수해야 합니다:\nF - Fast (빠르게 실행되어야 함): 테스트는 빠르게 실행되어야 하며, 개발 중 자주 실행해도 부담이 없어야 합니다. I - Isolated (독립적으로 실행될 수 있어야 함): 각 테스트는 서로 의존하지 않고 독립적으로 실행될 수 있어야 합니다. R - Repeatable (반복 실행 가능해야 함): 테스트는 언제 실행하더라도 동일한 결과를 보장해야 합니다. S - Self-Validating (스스로 결과를 검증할 수 있어야 함): 테스트는 기대값과 실제값을 스스로 비교하여 성공 또는 실패 여부를 판단할 수 있어야 합니다. T - Timely (적시에 작성되어야 함): 테스트는 프로덕션 코드 작성 직전에 작성되어야 하며, TDD(Test-Driven Development) 방식과 잘 맞아떨어집니다. 이 원칙을 지키면, 코드의 품질과 테스트의 신뢰성을 높일 수 있습니다. 테스트 코드는 단순히 버그를 줄이는 역할을 넘어서, 개발 생산성을 높이고, 유지보수 비용을 줄이며, 리팩토링에 대한 자신감을 부여하는 중요한 도구입니다. 적시에, 그리고 충분히 테스트를 작성하는 것은 안정적이고 고품질의 소프트웨어 개발을 가능하게 합니다. 관련된 내용 및 예제 샘플은 다음 리포지토리에서 확인가능합니다.\nhttps://github.com/junhkang/springboot-testing-from-zero-to-hero/tree/main ","permalink":"http://localhost:50666/posts/96/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/96/img.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"1-why---왜-테스트를-작성해야-하는가\"\u003e1. WHY - 왜 테스트를 작성해야 하는가?\u003c/h1\u003e\n\u003ch2 id=\"테스트-코드의-중요성\" ke-size=\"size26\"\u003e1.1 테스트 코드의 중요성\u003c/h2\u003e\n\u003cp\u003e테스트 코드는 소프트웨어 개발에서 매우 중요한 역할을 합니다. 기능을 수정하거나 새로운 기능을 추가할 때 코드가 안정적으로 작동하는지 확인할 수 있는 수단이 바로 테스트 코드입니다. 이를 통해 예상하지 못한 버그를 방지하고, 코드 품질을 높일 수 있습니다.\u003c/p\u003e\n\u003ch2 id=\"테스트-코드-작성의-장점\" ke-size=\"size26\"\u003e1.2 테스트 코드 작성의 장점\u003c/h2\u003e\n\u003ch3 id=\"안정적인-개발-환경-구축\" ke-size=\"size23\"\u003e1.2.1 안정적인 개발 환경 구축\u003c/h3\u003e\n\u003cp\u003e테스트 코드는 코드의 변경이 다른 기능에 미치는 영향을 최소화하는 데 도움을 줍니다. 개발자는 자신 있게 코드를 수정하거나 리팩터링 할 수 있으며, 기존 기능이 예상대로 작동하는지 검증할 수 있습니다.\u003c/p\u003e","title":"[Spring] 테스트 1 - 왜 테스트 코드를 작성해야 하는가?"},{"content":" [Caused by: java.lang.IllegalArgumentException: 이름이 {fragment}인, 둘 이상의 fragment들이 발견되었습니다. 이는 상대적 순서배열에서 불허됩니다. 상세 정보는 서블릿 스펙 8.2.2 2c 장을 참조하십시오. 절대적 순서배열을 사용하는 것을 고려해 보십시오.] Spring MVC가 포함되어 있는 상태에서 중복된 디펜던시를 추가하면서 발생한 에러이다. 라이브러리 버전업, 혹은 신규 라이브러리 추가 시 주로 발생하는 현상으로, 메이븐 클린을 통해 메이븐 리포지토리를 정리하면 된다. 메이븐 클린(프로젝트 우클릭 \u0026gt; maven \u0026gt; maven clean) 그래도 안된다면 실제로 중복된 라이브러리를 정렬 혹은 정리가 필요하다.\nSpring framework의 경우 web.xml에 \u0026lt;display-name\u0026gt;에 \u0026lt;absolute-ordering /\u0026gt; 추가하여 절대 순서로 정렬 SpringBoot의 경우 중복된 메이븐 디펜던시 정리 ","permalink":"http://localhost:50666/posts/95/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/95/img.png\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e[Caused by: java.lang.IllegalArgumentException: 이름이 {fragment}인, 둘 이상의 fragment들이 발견되었습니다. 이는 상대적 순서배열에서 불허됩니다. 상세 정보는 서블릿 스펙 8.2.2 2c 장을 참조하십시오. 절대적 순서배열을 사용하는 것을 고려해 보십시오.]\n \u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eSpring MVC가 포함되어 있는 상태에서 중복된 디펜던시를 추가하면서 발생한 에러이다. 라이브러리 버전업, 혹은 신규 라이브러리 추가 시 주로 발생하는 현상으로, 메이븐 클린을 통해 메이븐 리포지토리를 정리하면 된다.\n \u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e메이븐 클린(프로젝트 우클릭 \u0026gt; maven \u0026gt; maven clean)\n \u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e그래도 안된다면 실제로 중복된 라이브러리를 정렬 혹은 정리가 필요하다.\u003c/p\u003e","title":"[Spring] Caused by: java.lang.IllegalArgumentException: 이름이 {fragment}인, 둘 이상의 fragment들이 발견되었습니다."},{"content":" 1. RDS에서의 쿼리 성능 분석, pg_stat_statements란? RDS에서는 외부 익스텐션 사용이 제한된다. 쿼리 통계 및 성능에 대한 지표를 알 수 있는 여러 가지 익스텐션이 있지만, 그중 pg_stat_statements가 허용된다. pg_stat_statements의 원리, 사용법, 주요 지표에 대해 알아보자. pg_stat_statements는 실행된 쿼리에 대한 통계를 수집하고 저장한다. 다음은 저장되는 주요 지표들이고, 이를 통해 쿼리 성능을 분석하고 최적화할 수 있다. 쿼리빈도 쿼리 실행시간 쿼리 리소스 사용량 (CPU, 디스크I/O, 버퍼 히트 등) 쿼리 텍스트 저장 2. 설치 설치가 되어있지않다면 익스텐션을 먼저 설치하면 된다.\nCREATE EXTENSION pg_stat_statements; 만약 설치가 안된다면, RDS 데이터베이스가 할당된 파라미터 그룹에서 \u0026quot;shared_preload_libraries\u0026quot; 파라미터에 \u0026quot;pg_stat_statements\u0026quot;추가되어 있는지 확인하자\n3. 상세 지표 SELECT * FROM pg_stat_statements; 컬럼 설명 userid 쿼리 실행 사용자ID, 어떤 쿼리를 어떤 사용자가 사용했는지 추적 가능 dbid 쿼리 실행 데이터베이스 ID, 동일 인스턴스 에 여러 데이터베이스를 사용하는 경우, 데이터베이스 실행 추적 가능 queryid 쿼리 텍스트의 해쉬 값, 동일 쿼리 식별 가능 (쿼리가 변형되어도 ID를 통해 유사쿼리 그룹화 가능) query 실제 실행된 쿼리 텍스트 calls 쿼리가 실행된 횟수 total_time 쿼리의 총 실행 시간 (ms), 모든 호출의 실행시간 합산으로 쿼리의 전체 리소스 소비량을 알 수 있음 min_time 쿼리 실행 최소 시간 (ms) max_time 쿼리 실행 최대 시간 (ms) mean_time 쿼리 실행 평균 시간 (ms) stddev_time 쿼리 실행 표준편차 ( 실행 시간의 변동성 판단 가능, 변동성이 크다면 특정 상황에서 성능문제 발생할 수 있음 ) rows 쿼리 실행 결과 행 수 shared_blks_read 공유 메모리에서 블록이 히트된 횟수, 메모리에서 쿼리가 얼마나 효율적으로 데이터를 읽었는지 파악 가능 shared_blks_dirtied 디스크에서 읽은 공유 블록의 수 (값이 높으면 디스크 I/O가 많이 발생) shard_blks_written 쿼리 실행중 수정된 공유블록의 수, (데이터 변경이 얼마나 발생했는지 확인 가능) local_blks_hit, local_blks_read, local_blks_dirtied, local_blks_written shared_ 헤더의 값들과 유사, 공유 메모리가 아닌 로컬 메모리에서의 지표 temp_blks_read 쿼리 실행 중 임시 테이블에서 읽은 블록 수 (높으면 쿼리가 임시 테이블을 많이 사용중) temp_blks_written 쿼리 실행 중 임시 테이블에 기록된 블록 수 (높다면 쿼리 최적화 필요) blk_read_time 블록을 읽는데 총 걸린 시간 (ms) 쿼리가 I/O작업에서 얼마나 많은 시간을 소비했는지 확인 가능 blk_write_time 블록을 쓰는데 걸린 총 시간 (ms) 데이터 쓰기 작업이 성능에 얼마나 영향을 미쳤는지 파악 가능 4. 어떤 지표를 봐야 하나? 4-1. 쿼리 실행시간 관련 쿼리의 실행시간은 절대적인 기준으로 판단할 수 없다. 빠를수록 좋겠지만 \u0026quot;5초 이상은 무조건 튜닝이 필요한 쿼리\u0026quot;라고 할 수 없다는 것이다. 시스템 자원, 쿼리 플래닝 및 테스트를 통해 파악된 해당 쿼리의 \u0026quot;예상된\u0026quot; 소모 시간 대비, 혹은 \u0026quot;기존\u0026quot; 소모시간 대비 분석을 해야 한다.\ntotal_time : 쿼리가 총 소모한 시간이 특정 시점부터 급격히 증가하면 확인 대상 mean_time : 쿼리의 평균 실행시간이 특정 시점부터 급격히 증가하면 확인 대상 max_time : 쿼리 최대 실행시간이 특정 상황에서 비정상적으로 오래 시간이 걸린다면 확인 대상 calls : 쿼리 호출 횟수로, 예상 수치보다 자주 호출되거나, 자주 호출되는데 실행시간이 길다면 확인 대상 4-2. I/O 문제 디스크 읽기/쓰기 작업이 과도하게 발생하는 쿼리는 성능 문제를 일으킬 수 있다.\nshared_blks_read, shared_blks_write : 디스크에 읽고, 쓰는 공유 블록의 수로, 급격히 증가한다면 읽기/쓰기 작업이 과도하게 발생한 것을 의미하므로 확인 대상 temp_blks_read : 임시 테이블에서 읽은 블록수가 크다면 임시테이블 사용이 빈번한 쿼리로 확인대상 blk_write_time : 블록을 쓰는데 소요된 총시간으로 특정 시점부터 급격히 증가한다면 확인 대상 4-3. 변동성 및 불안정성 쿼리의 성능이 일관적이지 않다면 특정 상황에만 문제가 될 수 있는 쿼리로, 성능 문제를 일으킬 수 있다.\nstddev_time : 쿼리 실행시간의 표준편차, 값이 크다면 성능이 일관되지 않다는 의미로 확인 대상 max_time : 쿼리 실행 최대시간, 특정 시점에서만 비정상적으로 실행시간 증가한다면 확인 대상 5. 주의사항 pg_stat_statements는 쿼리에 대한 추가 통계를 수집하기 때문에 성능에 약간의 오버헤드를 유발할 수 있다. 일반적으로 시스템 운영에 영향을 줄정도로 크지 않지만, 매우 트래픽이 많은 시스템은 주의가 필요하다. 통계가 계속 축적되기 때문에 설정에 따라 디스크 사용량이 증가할 수 있다. pg_stat_statements_max (기본값 : 5000)을 적절히 조정하여 관리해야 한다. pg_stat_statements를 통해 실행되는 쿼리문 자체가 저장되기에 민감 데이터가 노출되지 않도록 관리해야 한다. (사용자 권한을 제한 권고) 누적된 집계 외에 특정 시점 이후의 성능을 분석하고 싶다면 통계 초기화해야 한다. SELECT pg_stat_statements_reset(); 6. 정리 pg_stat_statements의 지표를 통해 쿼리 성능 분석이 가능하다. 특히 쿼리 실행시간, 디스크 I/O, 변동성의 관점에서 쿼리 분석을 시작하는 것이 좋다. 트래픽이 아주 많은 시스템의 경우 익스텐션 설치만으로도 오버헤드가 발생할 수 있으니 충분한 테스트가 필요하다. 모든 지표들이 객관적인 정답을 가지고 있진 않기에, 현재 운영 중인 서비스의 시스템 자원, 쿼리 플래닝 및 테스트를 통해 파악된 해당 쿼리의 \u0026quot;예상된\u0026quot; 소모 시간과 \u0026quot;기존\u0026quot; 소모시간을 지속적으로 관리하여 예외적인 상황들을 빠르게 인지하고 대응하는 것이 중요하다.\n","permalink":"http://localhost:50666/posts/94/","summary":"\u003chr\u003e\n\u003ch2 id=\"rds에서의-쿼리-성능-분석-pg_stat_statements란\" ke-size=\"size26\"\u003e1. RDS에서의 쿼리 성능 분석, pg_stat_statements란?\u003c/h2\u003e\n\u003cp\u003eRDS에서는 외부 익스텐션 사용이 제한된다. 쿼리 통계 및 성능에 대한 지표를 알 수 있는 여러 가지 익스텐션이 있지만, 그중 pg_stat_statements가 허용된다. pg_stat_statements의 원리, 사용법, 주요 지표에 대해 알아보자.\n \u003c/p\u003e\n\u003cp\u003epg_stat_statements는 실행된 쿼리에 대한 통계를 수집하고 저장한다. 다음은 저장되는 주요 지표들이고, 이를 통해 쿼리 성능을 분석하고 최적화할 수 있다. \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e쿼리빈도\u003c/li\u003e\n\u003cli\u003e쿼리 실행시간\u003c/li\u003e\n\u003cli\u003e쿼리 리소스 사용량 (CPU, 디스크I/O, 버퍼 히트 등)\u003c/li\u003e\n\u003cli\u003e쿼리 텍스트 저장\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"설치\" ke-size=\"size26\"\u003e2. 설치 \u003c/h2\u003e\n\u003cp\u003e설치가 되어있지않다면 익스텐션을 먼저 설치하면 된다.\u003c/p\u003e","title":"[PostgreSQL] RDS 쿼리 성능 분석 방법 : pg_stat_statements 설치, 고려사항 및 주요 지표"},{"content":" 1. 빌더(Builder) 패턴이란? 일반적으로 구조를 갖춘 큰 구조물을 건축, 구축하는 것을 build라고 한다\n예를 들어 빌딩을 지을 때 지반다지기, 뼈대 만들기 등의 과정을 거치며 아래에서 위로 순차적으로 만들어간다. 복잡한 구조물을 단숨에 완성하기는 어렵기에, 구성하는 각 부분을 만들고 단계를 밟아가며 만들게 된다. Builder 패턴은 이처럼 구조를 가진 복잡한 인스턴스를 조립해 가는 패턴이다. 2. 빌더 패턴의 구조 빌더패턴 예제 클래스 다이어그램\nBuilder 역 - 인스턴스를 결정, 인스턴스 각 부분을 만드는 메서드를 정의. 예제에서는 Builder 클래스가 역할을 맡음 ConcreteBuilder 역 - 인터페이스를 구현, 실제 인스턴스 생성으로 호출되는 메서드가 여기에서 정의 TextBuilder, HTMLBuilder 클래스가 역할을 맡음 Director 역- Builder의 인터페이스를 사용하여 인스턴스 생성한다. Concrete Builder역에 의존하지 않으며, ConcreteBuilder에 상관없이 작동하도록 Builder의 메서드만 활용한다. 예제에서는 Director클래스가 역할을 맡음 3. 예제 빌더 패턴을 사용하여 문서 작성하는 프로그램을 구현해보자.\n문서의 기본적인 구조는 다음과 같다.\n타이틀을 한 개 포함 문자열을 몇 개 포함 항목을 몇개 포함 해당 구현을 빌더패턴으로 구현하기 위해 다음 클래스들을 구현해 보자. (위에서 본 클래스 다이어그램을 참고)\nBuilder 클래스 - 문서를 구성하는 메서드 결정하는 추상 클래스 Director 클래스- 그 메서드를 이용해 구체적인 문서 한 개 완성 TextBuilder - 텍스트를 이용해서 문서를 제작하는 구현 클래스 HTMLBuilder - HTML을 이용해서 문서 제작하는 구현 클래스 4. 예제 클래스 분석 4-1. Builder 클래스 public abstract class Builder { public abstract void makeTitle(String title); public abstract void makeString(String str); public abstract void makeItems(String[] items); public abstract void close(); } Builder 클래스는 문서를 만드는 메서드를 선언한 추상 클래스이다. makeTitle, makeString, makeItems는 각각 제목, 문자, 항목을 문서 안에 만들며 close메서드를 통해 문서를 최종 완성한다.\n4-2. Director 클래스 public class Director { private final Builder builder; public Director(Builder builder) { this.builder = builder; } public void construct() { builder.makeTitle(\u0026#34;Greeting\u0026#34;); builder.makeString(\u0026#34;일반적인 인사\u0026#34;); builder.makeItems(new String[]{ \u0026#34;How are you?\u0026#34;, \u0026#34;Hello.\u0026#34;, \u0026#34;Hi.\u0026#34;, }); builder.makeString(\u0026#34;시간대별 인사\u0026#34;); builder.makeItems(new String[]{ \u0026#34;Good Morning.\u0026#34;, \u0026#34;Good Afternoon.\u0026#34;, \u0026#34;Good Evening.\u0026#34;, }); builder.close(); } } Director 클래스는 Builder 클래스에 선언된 메서드로 문서를 제작한다. Builder 클래스는 추상 클래스이므로 인스턴스를 만들 수 없다.\nDirector의 생성자에 실제 전달되는 것은 Builder의 하위 클래스이고, 실제 구현체는 TextBuilder, HTMLBuilder의 인스턴스이다. 하위 클래스 종류에 따라 Director 클래스가 만들 구체적인 최종 문서형 식이 지정된다. construct 메서드는 실제로 문서를 만드는 메서드로 호출되면 문서가 생성된다.\n4-3. TextBuilder 클래스 public class TextBuilder extends Builder{ private StringBuilder sb = new StringBuilder(); @Override public void makeTitle(String title) { sb.append(\u0026#34;==============================\\n\u0026#34;); sb.append(\u0026#34;[\u0026#34;).append(title).append(\u0026#34;]\\n\u0026#34;); sb.append(\u0026#34;\\n\u0026#34;); } @Override public void makeString(String str) { sb.append(\u0026#34;■\u0026#34;).append(str).append(\u0026#34;\\n\u0026#34;); sb.append(\u0026#34;\\n\u0026#34;); } @Override public void makeItems(String[] items) { for (String item : items) { sb.append(\u0026#34; ・\u0026#34;).append(item).append(\u0026#34;\\n\u0026#34;); } sb.append(\u0026#34;\\n\u0026#34;); } @Override public void close() { sb.append(\u0026#34;==============================\\n\u0026#34;); } public String getTextResult() { return sb.toString(); } } TextBuilder 클래스는 Builder 클래스의 하위 클래스로 텍스트를 사용한 문서 생성의 각 메서드를 구체화한다.\n4-4. HTMLBuilder 클래스 public class HTMLBuilder extends Builder { private String filename = \u0026#34;untitled.html\u0026#34;; private StringBuilder sb = new StringBuilder(); @Override public void makeTitle(String title) { filename = title + \u0026#34;.html\u0026#34;; sb.append(\u0026#34;\u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;\u0026#34;).append(title).append(\u0026#34;\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt;\u0026lt;body\u0026gt;\\n\u0026#34;); sb.append(\u0026#34;\u0026lt;h1\u0026gt;\u0026#34;).append(title).append(\u0026#34;\u0026lt;/h1\u0026gt;\\n\u0026#34;); } @Override public void makeString(String str) { sb.append(\u0026#34;\u0026lt;p\u0026gt;\u0026#34;).append(str).append(\u0026#34;\u0026lt;/p\u0026gt;\\n\u0026#34;); } @Override public void makeItems(String[] items) { sb.append(\u0026#34;\u0026lt;ul\u0026gt;\\n\u0026#34;); for (String item : items) { sb.append(\u0026#34;\u0026lt;li\u0026gt;\u0026#34;).append(item).append(\u0026#34;\u0026lt;/li\u0026gt;\\n\u0026#34;); } sb.append(\u0026#34;\u0026lt;/ul\u0026gt;/n/n\u0026#34;); } @Override public void close() { sb.append(\u0026#34;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\\n\u0026#34;); } public String getHTMLResult() { return sb.toString(); } } \bHTMLBuilder 클래스는 Builder 클래스의 하위 클래스로 HTML을 사용한 문서 생성의 각 메서드를 구체화한다.\n4-5. Main 클래스 public class Main { public static void main(String[] args) { if (args.length != 1) { usage(); System.exit(0); } if (args[0].equals(\u0026#34;text\u0026#34;)) { TextBuilder textBuilder = new TextBuilder(); Director director = new Director(textBuilder); director.construct(); String result = textBuilder.getTextResult(); System.out.println(result); } else if (args[0].equals(\u0026#34;html\u0026#34;)) { HTMLBuilder htmlBuilder = new HTMLBuilder(); Director director = new Director(htmlBuilder); director.construct(); String filename = htmlBuilder.getHTMLResult(); System.out.println(\u0026#34;HTML파일 \u0026#34; + filename + \u0026#34; 이 작성되었습니다.\u0026#34;); }else { usage(); System.exit(0); } } public static void usage() { System.out.println(\u0026#34;Usage: java Main text 텍스트로 문서 작성\u0026#34;); System.out.println(\u0026#34;Usage: java Main html HTML로 문서 작성\u0026#34;); } } 빌더패턴을 테스트하기 위한 클래스로 text 문서를 선택하여 생성할 경우 다음과 같은 결과를 확인할 수 있다.\n5. 정리 빌더 패턴은 복잡한 인스턴스를 효율적으로 조립해 가는 과정이다. 예제에서 확인할 수 있는 내용 중 중요한 점은 자세한 구축과정이 Director 역할에 의해 감추어진다는 것이다. 객체 지향에서는 \u0026ldquo;누가 무엇을 알고 있는가\u0026quot;가 중요하다. 빌더 패턴의 예제에서도 어느 클래스가 어떤 메서드를 사용할 수 있는지 고민해 볼 필요가 있다. 예제에선 Main 클래스의 Builder 클래스의 메서드를 직접 호출하지 않는다. Director 클래스의 construct 메서드만 호출할 뿐이고 문서가 완성된다. Director 클래스가 알고 있는 것은 Builder 클래스이고 Director 클래스는 Builder 클래스의 메서드를 활용하여 문서를 구축한다. 자신이 실제로 사용하는 구현체 클래스가 어떤 것인지 모른다. (TextBuilder, HTMLBuilder, etc... ) 그리고 이렇게 하위 클래스를 모르는 것은 쉽게 교체할 수 있기 때문에 매우 유용하게 적용될 수 있다. ","permalink":"http://localhost:50666/posts/93/","summary":"\u003chr\u003e\n\u003ch2 id=\"빌더builder-패턴이란\" ke-size=\"size26\"\u003e1. 빌더(Builder) 패턴이란?\u003c/h2\u003e\n\u003cp\u003e일반적으로 구조를 갖춘 큰 구조물을 건축, 구축하는 것을 build라고 한다\u003c/p\u003e\n\u003cp\u003e예를 들어 빌딩을 지을 때 지반다지기, 뼈대 만들기 등의 과정을 거치며 아래에서 위로 순차적으로 만들어간다. 복잡한 구조물을 단숨에 완성하기는 어렵기에, 구성하는 각 부분을 만들고 단계를 밟아가며 만들게 된다. Builder 패턴은 이처럼 구조를 가진 복잡한 인스턴스를 조립해 가는 패턴이다. \u003c/p\u003e\n\u003ch2 id=\"빌더-패턴의-구조\" ke-size=\"size26\"\u003e2. 빌더 패턴의 구조\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/93/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-07-29%20%EC%98%A4%ED%9B%84%202.34.00.png\"\u003e\u003c/p\u003e\n\u003cp\u003e빌더패턴 예제 클래스 다이어그램\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eBuilder 역\u003c/strong\u003e - 인스턴스를 결정, 인스턴스 각 부분을 만드는 메서드를 정의. 예제에서는 Builder 클래스가 역할을 맡음\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConcreteBuilder 역\u003c/strong\u003e - 인터페이스를 구현, 실제 인스턴스 생성으로 호출되는 메서드가 여기에서 정의 TextBuilder, HTMLBuilder 클래스가 역할을 맡음\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDirector 역\u003c/strong\u003e- Builder의 인터페이스를 사용하여 인스턴스 생성한다. Concrete Builder역에 의존하지 않으며, ConcreteBuilder에 상관없이 작동하도록 Builder의 메서드만 활용한다. 예제에서는 Director클래스가 역할을 맡음\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"예제\" ke-size=\"size26\"\u003e3. 예제\u003c/h2\u003e\n\u003cp\u003e빌더 패턴을 사용하여 문서 작성하는 프로그램을 구현해보자.\u003c/p\u003e","title":"[디자인패턴] 빌더(Builder) 패턴의 개념, 예제, 장단점, 활용"},{"content":" Amazon RDS and Amazon Aurora SSL/TLS 인증서를 업데이트 후 별다른 설정 변경을 하지 않았지만, 오랜만에 빌드하는 스프링부트 프로젝트의 데이터베이스 연결이 되지 않는 현상 발생하였다. [### Error querying database. Cause: org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is org.postgresql.util.PSQLException: FATAL: no pg_hba.conf entry for host \u0026ldquo;{host}\u0026rdquo;, user \u0026ldquo;{user}\u0026rdquo;, database \u0026ldquo;{database}\u0026rdquo;, no encryption ] RDS 인증서 업데이트 작업 후 SSL 요구설정이 활성화된 것으로, 해당 에러가 나며 접속이 안된다면 다음과 같이 조치할 수 있다.\n1. sslmode=require 속성을 추가 별도 인증서를 지정하지 않아도 다음과 같이 간단한 속성 추가로 문제를 해결할 수 있다.\njdbc.url=jdbc:postgresql://{url}:{port}/{database}?sslmode=require 2. SSL 인증서를 지정 AWS RDS의 SSL 인증서 번들을 다운로드 받은 후 접속 파라미터 혹은 스프링부트 시스템 속성에 추가해 주면 된다. 다음은 접속 파라미터에 추가하는 예제이다.\njdbc.url=jdbc:postgresql://{url}:{port}/{database}?ssl=true\u0026amp;sslmode=require\u0026amp;sslrootcert={인증서경로}.pem ","permalink":"http://localhost:50666/posts/92/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/92/img.png\"\u003e\n \u003c/p\u003e\n\u003cp\u003eAmazon RDS and Amazon Aurora SSL/TLS 인증서를 업데이트 후 별다른 설정 변경을 하지 않았지만, 오랜만에 빌드하는 스프링부트 프로젝트의 데이터베이스 연결이 되지 않는 현상 발생하였다.\n \u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e[### Error querying database. Cause: org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is org.postgresql.util.PSQLException: FATAL: no pg_hba.conf entry for host \u0026ldquo;{host}\u0026rdquo;, user \u0026ldquo;{user}\u0026rdquo;, database \u0026ldquo;{database}\u0026rdquo;, no encryption ]\n \u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eRDS 인증서 업데이트 작업 후 SSL 요구설정이 활성화된 것으로, 해당 에러가 나며 접속이 안된다면 다음과 같이 조치할 수 있다.\u003c/p\u003e","title":"[Spring] Error querying database. Cause: org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection, 데이터베이스 접속 에러"},{"content":" PostgreSQL에서 문자열을 다루는 다양한 방법이 있다. 그중 문자열을 원하는 방식대로 자를 수 있는 함수들에 대해 알아보자. 먼저, 가장 많이 쓰이는 SUBSTRING, SUBSTR의 기본 사용 법 및 응용, 성능에 대해 알아보자.\n1. Substring 기본 사용법 Substring과 Substr은 시작 위치(n), 길이(l)를 기준으로 문자열을 자를 수 있다.\nSELECT substring(\u0026#39;문자열\u0026#39; FROM n FOR l); -- from, for 구문은 substring만 지원 SELECT substring(\u0026#39;문자열\u0026#39;, n, l); SELECT substr(\u0026#39;문자열\u0026#39;, n, l); 구문을 활용하여 PostgreSQL Tutorial 문자열에서 첫번째 위치인 P부터 10개의 문자열만을 추출하는 예제를 확인해 보면 동일한 결과를 확인할 수 있다.\nSELECT substring(\u0026#39;PostgreSQL Tutorial\u0026#39; FROM 1 FOR 10); SELECT substring(\u0026#39;PostgreSQL Tutorial\u0026#39;, 1, 10); SELECT substr(\u0026#39;PostgreSQL Tutorial\u0026#39;, 1, 10); 2. Substring 뒤에서 뒤에서부터 (오른쪽에서 부터) 문자열을 추출하려면, 문자열의 길이를 계산한 후, 시작 위치를 해당 길이에서 빼는 방식으로 적용이 가능하다. 예제 1 - 뒤에서 3개 문자열 추출 다음 예제는 특정 문자열 'abcdef'에서 뒤에 3개의 문자열 'def'를 추출하는 예제이다. SELECT substring(\u0026#39;abcdef\u0026#39; FROM char_length(\u0026#39;abcdef\u0026#39;) - 3 + 1); -- 3은 추출하고자하는 길이 char_length('abcdef') - 3 + 1\u0026quot;는 \u0026ldquo;6 - 2 = 4\u0026quot;가 되며, 4번째부터 맨 끝까지 문자열을 추출하게 된다.\n예제 2 - 뒤에서 4개 문자열 추출 다음은 'PostgreSQL' 문자열에서 뒤에서 4번째 글자부터 맨 끝 문자를 추출하는 예제이다.\nSELECT substring(\u0026#39;PostgreSQL\u0026#39; FROM char_length(\u0026#39;PostgreSQL\u0026#39;) - 4 + 1); 3. Substring 정규식으로 추출 3-1. 기본 Substring 정규식 추출 FROM 뒤에 정규식을 추가함으로써 정규식 패턴매칭으로 문자열을 추출할 수도 있다.\nSELECT substring(\u0026#39;user@example.com\u0026#39; FROM \u0026#39;@(.+)$\u0026#39;); 다음과 같이 적용하면 @뒤에 오는 모든 문자열을 추출하게 된다.\n3-2. 그룹 Substring 정규식 추출 혹은 정규식에 매칭된 문자열이 여러 그룹이라면, 매칭되는 문자열그룹의 첫 번째 그룹만을 조회한다. 예를 들어 다음 정규식 문자열 자르기를 보면\nSELECT substring(\u0026#39;2024-07-24\u0026#39; FROM \u0026#39;([0-9]{4})-([0-9]{2})-([0-9]{2})\u0026#39;); {2024, 07, 24} 세 개의 그룹으로 정규식 매칭이 된다. 하지만 결과는 첫 번째 그룹인 2024만 조회된다. 즉, 다음 정규식 매칭 함수와 동일한 기능을 하게 된다.\nSELECT (regexp_matches(\u0026#39;2024-07-24\u0026#39;, \u0026#39;([0-9]{4})-([0-9]{2})-([0-9]{2})\u0026#39;))[1]; 4. Substring, Substr의 차이 (문법, 성능) 기본적으로 원하는 위치의 문자열을 자른다는 공통점이 있지만, SUBSTRING이 더 많은 기능을 제공한다, 특히 패턴 매칭이 가능하며, SUBSTR은 간단한 위치와 길이를 기반으로 문자를 추출할 때 사용한다. 성능 측면에서도 단순히 위치와 길이를 기반으로 문자열을 추출할 경우 동일하다, 둘 다 문자열 인덱싱을 통해 부분 문자열을 빠르게 추출한다. 다음은 두 함수의 성능을 비교하는 간단한 예제이다. EXPLAIN ANALYZE SELECT substring(\u0026#39;abcdefghijklmnopqrstuvwxyz\u0026#39; FROM 2 FOR 10) FROM generate_series(1, 1000000); EXPLAIN ANALYZE SELECT substr(\u0026#39;abcdefghijklmnopqrstuvwxyz\u0026#39;, 2, 10) FROM generate_series(1, 1000000); 성능상 큰 차이가 없음을 확인할 수 있다. ","permalink":"http://localhost:50666/posts/91/","summary":"\u003chr\u003e\n\u003cp\u003ePostgreSQL에서 문자열을 다루는 다양한 방법이 있다. 그중 문자열을 원하는 방식대로 자를 수 있는 함수들에 대해 알아보자. 먼저, 가장 많이 쓰이는 SUBSTRING, SUBSTR의 기본 사용 법 및 응용, 성능에 대해 알아보자.\u003c/p\u003e\n\u003ch2 id=\"substring-기본-사용법\" ke-size=\"size26\"\u003e1. Substring 기본 사용법\u003c/h2\u003e\n\u003cp\u003eSubstring과 Substr은 시작 위치(n), 길이(l)를 기준으로 문자열을 자를 수 있다.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eSELECT substring(\u0026#39;문자열\u0026#39; FROM n FOR l); -- from, for 구문은 substring만 지원\nSELECT substring(\u0026#39;문자열\u0026#39;, n, l);\nSELECT substr(\u0026#39;문자열\u0026#39;, n, l);\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e구문을 활용하여  PostgreSQL Tutorial 문자열에서 첫번째 위치인 P부터 10개의 문자열만을 추출하는 예제를 확인해 보면 동일한 결과를 확인할 수 있다.\u003c/p\u003e","title":"[PostgreSQL] SUBSTRING, SUBSTR, 문자열 자르기"},{"content":" 1. 개념 equals를 단순히 재정의 하는 것은 쉽지만 함정이 많다. 이번 장에서는 equals를 재정의 할 때 고려해야 하는 점과, 재정의가 완료된 후 확인해야 하는 부분들에 대해 다루고 있다.\n2. equals를 재정의하는 경우와 재정의하지 말아야 하는 경우 2-1. equals를 재정의하지 말아야 할 경우 각 인스턴스가 본질적으로 고유한 값을 표현하는 클래스: 예를 들어, Thread 클래스는 각 인스턴스가 고유한 ID를 가지므로 equals를 재정의할 필요가 없다. 논리적 동치성 검사가 필요 없는 경우: 대부분의 경우 객체 식별성만 중요하며, 논리적 동치성은 필요하지 않을 수 있다. 상위 클래스에서 재정의한 equals가 하위 클래스에 적절한 경우: 상위 클래스에서 이미 equals를 적절히 구현했고, 이를 하위 클래스에서 그대로 사용해도 무방한 경우이다. 접근이 제한된 클래스: equals를 호출할 일이 없는 private이나 package-private 클래스일 경우, 재정의할 필요가 없다. 2-2. equals를 재정의해야 할 경우 논리적 동치성 비교가 필요한 경우: 객체의 내용이 같은지를 비교해야 할 때, 상위 클래스의 equals가 이를 충족시키지 않을 때 재정의가 필요하다. 3. equals 메서드의 규약 Object 기본 명세의 equals 메서드는 다음의 규약을 준수해야 한다:\n반사성 (Reflexivity): 모든 null이 아닌 참조 값 x에 대해, x.equals(x)는 true여야 한다. 대칭성 (Symmetry): 모든 null이 아닌 참조 값 x, y에 대해, x.equals(y)가 true이면 y.equals(x)도 true여야 한다. 추이성 (Transitivity): 모든 null이 아닌 참조 값 x, y, z에 대해, x.equals(y)가 true이고 y.equals(z)도 true이면 x.equals(z)도 true여야 한다. 일관성 (Consistency): 모든 null이 아닌 참조 값 x, y에 대해, x.equals(y)를 반복해서 호출하면 항상 true 또는 항상 false를 반환해야 한다. null-아님 (Non-nullity): 모든 null이 아닌 참조 값 x에 대해, x.equals(null)은 항상 false여야 한다. 4. equals 구현 방법 == 연산자를 사용하여 입력이 자기 자신의 참조인지 확인한다. instanceof 연산자로 입력이 올바른 타입인지 확인한다. 입력을 올바른 타입으로 형변환한다. 입력 객체와 자기 자신의 대응되는 핵심 필드들이 모두 일치하는지 하나씩 검사한다. 5. equals 구현 후 점검 새로 구현된 equals 메서드가 대치성, 추이성, 일관성을 유지하는지 확인해야한다.\n6. equals를 재정의하는 예 경험했던 equals를 재정의하는 예로는 보통 값 객체(value object, VO)가 있다. 예를 들어 User 클래스에서 식별자 id가 같으면 같은 사용자로 간주할 수 있다. 이 경우, equals를 id 필드를 기준으로 재정의하여 사용자 객체의 논리적 동치성을 정확히 비교할 수 있다.\n7. 정리 꼭 필요한 경우가 아니면 equals를 재정의하지 말아야 한다. Object의 기본 equals 메서드가 대개 원하는 비교를 수행해 준다. 꼭 필요한 경우라면 앞서 정리한 다섯 가지 규약을 반드시 지켜가며 equals를 구현해야 한다. ","permalink":"http://localhost:50666/posts/90/","summary":"\u003chr\u003e\n\u003ch2 id=\"개념\" ke-size=\"size26\"\u003e1. 개념\u003c/h2\u003e\n\u003cp\u003eequals를 단순히 재정의 하는 것은 쉽지만 함정이 많다. 이번 장에서는 equals를 재정의 할 때 고려해야 하는 점과, 재정의가 완료된 후 확인해야 하는 부분들에 대해 다루고 있다.\u003c/p\u003e\n\u003ch2 id=\"equals를-재정의하는-경우와-재정의하지-말아야-하는-경우\" ke-size=\"size26\"\u003e2. equals를 재정의하는 경우와 재정의하지 말아야 하는 경우\u003c/h2\u003e\n\u003ch3 id=\"equals를-재정의하지-말아야-할-경우\" ke-size=\"size23\"\u003e2-1. equals를 재정의하지 말아야 할 경우\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e각 인스턴스가 본질적으로 고유한 값을 표현하는 클래스\u003c/strong\u003e: 예를 들어, Thread 클래스는 각 인스턴스가 고유한 ID를 가지므로 equals를 재정의할 필요가 없다.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e논리적 동치성 검사가 필요 없는 경우\u003c/strong\u003e: 대부분의 경우 객체 식별성만 중요하며, 논리적 동치성은 필요하지 않을 수 있다.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e상위 클래스에서 재정의한 equals가 하위 클래스에 적절한 경우\u003c/strong\u003e: 상위 클래스에서 이미 equals를 적절히 구현했고, 이를 하위 클래스에서 그대로 사용해도 무방한 경우이다.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e접근이 제한된 클래스\u003c/strong\u003e: equals를 호출할 일이 없는 private이나 package-private 클래스일 경우, 재정의할 필요가 없다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"equals를-재정의해야-할-경우\" ke-size=\"size23\"\u003e2-2. equals를 재정의해야 할 경우\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e논리적 동치성 비교가 필요한 경우\u003c/strong\u003e: 객체의 내용이 같은지를 비교해야 할 때, 상위 클래스의 equals가 이를 충족시키지 않을 때 재정의가 필요하다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"equals-메서드의-규약\" ke-size=\"size26\"\u003e3. equals 메서드의 규약\u003c/h2\u003e\n\u003cp\u003eObject 기본 명세의 equals 메서드는 다음의 규약을 준수해야 한다:\u003c/p\u003e","title":"[이펙티브 자바] 10. equals는 일반 규약을 지켜 재정의하라"},{"content":" GitHub 리포지토리에 보안정책에 위반되거나 민감한 정보를 실수로 업로드하는 경우가 있다. 해당 브랜치를 지우거나 커밋 히스토리를 밀어버려도 이미 머지된 Pull Request라면, 깃허브 UI의 closed request 탭에서 파일 히스토리를 확인하면 해당 내용이 그대로 남아 있어 별도의 조치가 필요하다.\n1. 민감정보 파일 완전 삭제 (리포지토리 복제 및 재생성) 1-1. 기존 리포지토리 클론 및 민감 정보 제거 git clone --mirror https://github.com/username/repo.git cd repo.git # BFG Repo-Cleaner 다운로드 curl -L -o bfg.jar https://repo1.maven.org/maven2/com/madgag/bfg/1.13.0/bfg-1.13.0.jar # 민감한 파일 깃 히스토리에서 완전 제거 java -jar bfg.jar --delete-files \u0026#39;test.properties(삭제파일명)\u0026#39; # Git Garbage Collection 수행 ( Git reflog를 만료 시키고, 쓰지 않는 객체 삭제 ) git reflog expire --expire=now --all git gc --prune=now --aggressive 1-2. 기존 리포지토리에 클린 된 히스토리 강제 푸시 git push --force --all git push --force --tags 1-3. 주의사항 git push --force는 기존 히스토리를 덮어쓰기에 원격 리포지토리를 백업 권고 모든 협업자들에게 영향이 갈 수 있어 모든 협업자에게 변경사항을 공지하고 로컬 저장소를 다시 클론 하도록 안내 권고 1-4. 효과 이 과정을 거치면 다음과 같은 효과를 볼 수 있다.\n기존 파일 히스토리에 민감한 정보가 포함된 파일을 완전히 제거 깃 히스토리의 각 객체를 정리하고 최적화하여 저장소 용량을 줄임 강제 푸시를 통해 수정된 내용을 원격 리포지토리에 적용하여 최종적으로 민감함 정보가 포함된 파일을 삭제 2. GitHub 지원팀에 문의 1번 방법을 사용하더라도 깃허브 UI를 통해 closed pull request의 파일 히스토리는 여전히 볼 수가 있다. 깃허브의 기본 작동 방식으로, 깃허브에서 pull request를 통해 추가된 변경 사항은 해당 PR이 머지되더라도, 깃허브에서 볼 수가 있다. 그러므로 민감정보 히스토리를 완전히 제거하려면 PR이 머지되기 전에 민감정보를 삭제하는 것이 중요하다. 완전히 제거하는 기능은 깃허브에서 기본적으로 제공하진 않는다. 이 경우 지원팀에 문의하여 해결하는 방법을 알아보도록 하자.\n2-1. 깃허브 지원 페이지로 이동 https://support.github.com/requesthttps://support.github.com/request\nhttps://support.github.com/request 2-2. 문의 유형 선택 \u0026quot;Remove data from a repository I own or control\u0026quot;\n2-3. \u0026quot;Remove pull requests\u0026quot; 선택 comment에 업로드한 파일이나 이미지, 그 외 데이터를 삭제하려면 그에 맞는 옵션을 선택하면 된다.\n2-4. 단일, 여러 개 PR 선택 2-5. 원하는 Pull request의 주소 입력 https://github.com/#https://github.com/#{owner}/#{repo}/pull/#{requestId} 형태\n2-6. 깃 히스토리에서 이미 민감 정보를 제거했는지 확인 1번 스탭에서 제거를 했다면 Yes 선택\n2-7. 히스토리 삭제로 문제가 해결됐는지 확인 No 선택\n2-8. 깃허브 서포트 답변 확인 과정을 완료하고 기다리면 github support(https://support.github.com/https://support.github.com/)에 서포트 답변이 추가된다. 2-9. PR 중에 정확히 민감정보를 가지고 있는 히스토리의 SHA 히스토리 제공 Pull Reqest에서 민감 정보가 들어있는 파일을 찾아... 부분을 선택한다.\n해당 파일의 히스토리 리스트에서 원하는 Pull Request의 SHA를 찾는다. (복사 버튼을 통해서 Full SHA를 제출해야 한다.)\n2-10. 깃허브 서포트에 답변을 남긴다. 2-11. 해당 SHA와 연관된 전체 Pull Request 삭제, 혹은 민감 정보 부분만 삭제를 선택 전체 Pull Request 삭제 : 민감 정보가 노출될 여지없이 연관된 Pull Request 자체를 모두 삭제한다. 위험 요소를 모두 삭제하고 싶을 때 적용하면 되지만, 관련된 comment / history가 모두 삭제되기에 큰 프로젝트에서 중요한 변경 사항을 추적하기 힘들 수 있다.\n내부 참조만 삭제 : comment / history를 남겨 민감 정보의 내부 참조만 삭제를 한다. 모든 참조가 삭제되지 않으면 데이터에는 여전히 접근 가능할 수 있기에 보안에 완전한 확신을 가질 순 없지만, Pull Request 자체는 남아있어 프로젝트 진행 상황을 추적할 수 있다. 이번 경우 \u0026ldquo;전체 Pull Request 삭제\u0026quot;를 선택하여 회신하였고, 정삭적으로 연관 Pull Request들이 모두 삭제됨을 확인할 수 있었다.\n","permalink":"http://localhost:50666/posts/89/","summary":"\u003chr\u003e\n\u003cp\u003eGitHub 리포지토리에 보안정책에 위반되거나 민감한 정보를 실수로 업로드하는 경우가 있다. 해당 브랜치를 지우거나 커밋 히스토리를 밀어버려도 이미 머지된 Pull Request라면, 깃허브 UI의 closed request 탭에서 파일 히스토리를 확인하면 해당 내용이 그대로 남아 있어 별도의 조치가 필요하다.\u003c/p\u003e\n\u003ch2 id=\"민감정보-파일-완전-삭제-리포지토리-복제-및-재생성\" ke-size=\"size26\"\u003e1. 민감정보 파일 완전 삭제 (리포지토리 복제 및 재생성)\u003c/h2\u003e\n\u003ch3 id=\"기존-리포지토리-클론-및-민감-정보-제거\" ke-size=\"size23\"\u003e1-1. 기존 리포지토리 클론 및 민감 정보 제거\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003egit clone --mirror https://github.com/username/repo.git\ncd repo.git\n\n# BFG Repo-Cleaner 다운로드\ncurl -L -o bfg.jar https://repo1.maven.org/maven2/com/madgag/bfg/1.13.0/bfg-1.13.0.jar\n\n# 민감한 파일 깃 히스토리에서 완전 제거\njava -jar bfg.jar --delete-files \u0026#39;test.properties(삭제파일명)\u0026#39;\n\n# Git Garbage Collection 수행 ( Git reflog를 만료 시키고, 쓰지 않는 객체 삭제 )\ngit reflog expire --expire=now --all\ngit gc --prune=now --aggressive\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"기존-리포지토리에-클린-된-히스토리-강제-푸시\" ke-size=\"size23\"\u003e1-2. 기존 리포지토리에 클린 된 히스토리 강제 푸시\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003egit push --force --all\ngit push --force --tags\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e \u003c/p\u003e","title":"[GitHub] 민감한 정보 완전 삭제와 PR 히스토리 정리"},{"content":" 1. 문자열에서 날짜/시간으로의 변환 PostgreSQL의 날짜형태의 칼럼을 조회할 때, 종종 정확한 날짜 형태를 사용하는 것이 아닌, 문자열, 혹은 숫자 형태로 간편하게 조회하는 경우가 있다. 예를 들어 2024/05/02 이후의 값을 조회할 때 다음 두 가지 조회 방법을 사용할 수 있다.\ndate_column \u0026gt; '20240502'\ndate_column \u0026gt; TO_DATE('20240502', 'YYYYMMDD')\n예제와 같이 PostgreSQL은 일련의 문자/숫자열을 조건에 맞는 날짜형으로 자동으로 디코딩을 해주는데, 문자열을 인식하는 상세 과정을 순서대로 알아보자.\n2. 문자열에서 날짜/시간으로의 디코딩 과정 2-1. 문자열을 토큰으로 분리하고 각 토큰을 시간, 시간대, 또는 숫자로 분류한다. 예제들에서는 정상적으로 날짜 및 시간이 변환되는지 확인하기 위해 강제로 TIMESTAMP 및 DATE로 형 변환을 하였지만, 날짜 형태의 데이터와 문자열 그대로를 비교하여도 날짜 및 시간 비교가 가능하다.\n숫자 토큰이 \u0026quot;:\u0026quot;를 포함한다면, 시간 문자열로 인식되며, 하나라도 발견되면 이후의 모든 숫자와 콜론은 시간 문자열의 일부로 취급 SELECT \u0026#39;20240202 13:45:30\u0026#39;::TIMESTAMP 13:45:30에 \u0026ldquo;:\u0026ldquo;를 포함하였으니 해당 토큰 전체를 시간 문자열로 취급하여, 13시 45분 30초로 해석된다. (hh:mm, hh:mm:ss 등의 표준규격에 맞는 경우에만)\n숫자 토큰에 하이픈(-), 슬래시(/) 또는 두개이상의 점(.)이 포함되어 있으면 날짜 문자열 취급 SELECT \u0026#39;2024-05-02\u0026#39;::DATE; SELECT \u0026#39;05/02/2024\u0026#39;::DATE; SELECT \u0026#39;02.05.2024\u0026#39;::DATE; yyyy mm dd의 표준 규격에 맞는 경우 모두 날짜 형태로 해석된다.\n이미 날짜 토큰이 확인된경우 문자열을 시간대 이름 (ex America/New_York)으로 해석 SELECT \u0026#39;2023-12-25 America/New_York\u0026#39;::TIMESTAMP WITH TIME ZONE; 2023-12-25 America/New_York 은, 앞부분에서 이미 날짜 토큰이 확인되었기에, 뒷부분은 시간대 이름으로 해석된다.\n토큰이 숫자만으로 구성되어 있으면 단일필드이거나 ISO 8601 형식의 날짜로 해석된다. SELECT \u0026#39;19990113\u0026#39;::DATE; 19990113 = 1999년 1월 13일 또는 (141516 = 14시 15분 16초)\n토큰이 +, -로 시작하면 숫자 시간대 또는 특별필드이다. +0200 : UTC보다 2시간 빠른 시간대\n-0500 : UTC보다 5시간 늦은 시간대\n+15 : 현재 날짜로부터 15일 후 날짜\n-3 : 현재 날짜로부터 3일 전 날짜\nSELECT \u0026#39;20240202 12:00 +0200\u0026#39;::TIMESTAMP WITH TIME ZONE; 2-2. 토큰이 알파벳 문자열이라면 가능한 문자열 사전을 조회한다. 토큰이 알려진 시간대 약어 중에 일치하는 게 있는지 확인 SELECT \u0026#39;20240202 12:00 PM EST\u0026#39;::TIMESTAMP WITH TIME ZONE; \u0026ldquo;12:00 PM EST\u0026rdquo; 에서 EST는 동부표준시(Eastern Standard Time)의 약어로, UTC 오프셋 매핑 딕셔너리에 포함되어 있어 사용 가능\n발견된 문자열이 없으면 내부 테이블을 검색하여 토큰을 특별 문자열 (ex, today, Thursday, January) 혹은 at, on 같은 조사와 매칭시킨다 SELECT \u0026#39;Today\u0026#39;::DATE + 1; 'Today'::date + 1 은 내일 날짜를 반환한다.\n문자열이 위 두조건에 부합하지 않는다면 에러 발생 2-3. 토큰이 숫자, 숫자 필드로만 이루어져 있을 때 6 , 8자리이며 다른 날짜 필드가 발견되기 전이라면 날짜 형태로 해석(YYYYMMDD 혹은 YYMMDD) SELECT \u0026#39;20240201\u0026#39;::DATE SELECT \u0026#39;240201\u0026#39;::DATE 20240201, 240201 모두 2024년 02월 01일로 해석된다.\n토큰이 3자리 숫자이고, 이미 연도가 발견되었다면, 그 해의 n번째 일수로 해석 SELECT \u0026#39;2024 021\u0026#39;::DATE; '2024 021'::date 은 2024년의 21번째 날로 2024년 01월 21일로 해석된다.\n네 자리 또는 여섯 자리 숫자이고 이미 날짜가 발견되었다면, 시간(HHMM 또는 HHMMSS)으로 해석 SELECT \u0026#39;20240502 123422\u0026#39;::TIMESTAMP; '20240502 123422'::timestamp 에서 앞부분 토큰에 날짜는 이미 발견되었기에, 6자리 숫자가 시간으로 해석된다. (2024년 05월 02일의 12:34:22)\n세 자리 이상의 숫자이고 아직 날짜 필드가 발견되지 않았다면, 연도로 해석 (기본적으로 yy-mm-dd 순서이며 서버의 DateStyle 설정에 따라 mm-dd-yy, dd-mm-yy 등으로 변경할 수 있다. SELECT \u0026#39;240522\u0026#39;::TIMESTAMP '240522'::timestamp는 처음 발견된 날짜형 토큰이기에 기본 설정인 yy-mm-dd에 따라 해석된다. (2024년 05월 02일)\n월/일 필드가 범위를 벗어나거나 유효하지 않은 값이라면 오류 발생 2-4. BC(기원전) 설정 bc 문자열을 통해 기원전 설정이 가능하며, BC(기원전)이 설정되어 있다면, 내부적으로는 연도를 음수로 바꾸고 1을 더한 후 저장한다. (그레고리력에는 연도 0이 없으므로 수치상 1 BC는 연도 0이 됨) SELECT \u0026#39;bc 1200201\u0026#39;::DATE BC가 지정되지 않고 연도 필드가 두 자리 숫자인 경우, 연도를 4자리로 조정한다. 해당 필드가 70보다 작으면 2000을 더하고, 그렇지 않으면 1900을 더한다. '800502'::date - 년도필드(80)가 70보다 크기에 1900을 더한 '1980년 05월 02일'을 의미 '240502'::date - 년도필드(24)가 70보다 작기에 2000을 더한 '2024년 05월 02일'을 의미 SELECT \u0026#39;800502\u0026#39;::DATE ; SELECT \u0026#39;240502\u0026#39;::DATE ; 참고 https://www.postgresql.org/docs/16/datetime-input-rules.html\n","permalink":"http://localhost:50666/posts/88/","summary":"\u003chr\u003e\n\u003ch2 id=\"문자열에서-날짜시간으로의-변환\" ke-size=\"size26\"\u003e1. 문자열에서 날짜/시간으로의 변환\u003c/h2\u003e\n\u003cp\u003ePostgreSQL의 날짜형태의 칼럼을 조회할 때, 종종 정확한 날짜 형태를 사용하는 것이 아닌, 문자열, 혹은 숫자 형태로 간편하게 조회하는 경우가 있다. 예를 들어 2024/05/02 이후의 값을 조회할 때 다음 두 가지 조회 방법을 사용할 수 있다.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003edate_column \u0026gt; '20240502'\u003cbr\u003e\ndate_column \u0026gt; TO_DATE('20240502', 'YYYYMMDD')\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e예제와 같이 PostgreSQL은 일련의 문자/숫자열을 조건에 맞는 날짜형으로 자동으로 디코딩을 해주는데,  문자열을 인식하는 상세 과정을 순서대로 알아보자.\u003c/p\u003e\n\u003ch2 id=\"문자열에서-날짜시간으로의-디코딩-과정\" ke-size=\"size26\"\u003e2. 문자열에서 날짜/시간으로의 디코딩 과정\u003c/h2\u003e\n\u003ch3 id=\"문자열을-토큰으로-분리하고-각-토큰을-시간-시간대-또는-숫자로-분류한다.\" ke-size=\"size23\"\u003e2-1. 문자열을 토큰으로 분리하고 각 토큰을 시간, 시간대, 또는 숫자로 분류한다.\u003c/h3\u003e\n\u003cp\u003e예제들에서는 정상적으로 날짜 및 시간이 변환되는지 확인하기 위해 강제로 TIMESTAMP 및 DATE로 형 변환을 하였지만, 날짜 형태의 데이터와 문자열 그대로를 비교하여도 날짜 및 시간 비교가 가능하다.\u003c/p\u003e","title":"[PostgreSQL] 문자열에서 날짜/시간 변환 및 처리 과정"},{"content":" 1. 전략(Strategy) 패턴이란? Strategy는 전략이라는 뜻으로 적을 해치우는 작전, 게임을 이기는 전략, 문제를 풀어나가는 방법 등의 의미가 있고, 특히 프로그래밍에서는 문제를 해결하는 방식인 \u0026quot;알고리즘\u0026quot;을 의미한다. 모든 프로그램은 문제를 해결하기 위해 존재하며, 특정 알고리즘으로 구현된다. Strategy 패턴에서는 이미 구현한 알고리즘을 쉽게 모두 교체할 수 있다. 즉, 스위치를 누르듯 쉽게 알고리즘을 바꿔서 같은 문제를 다른 방법으로 해결하기 쉽게 만들어주는 패턴이 Strategy 패턴이다.\n2. 전략(Strategy) 패턴의 구조 전략 패턴의 구조와 예제에서 각 요소가 어떤 역할을 하는지 간단하게 먼저 살펴보자.\nStrategy(전략) - 전략을 사용하기 위한 인터페이스를 결정 (예제의 Strategy 인터페이스) ConcreteStrategy(구체적인 전략) - 인터페이스를 실제로 구현, 예제의 WinningStrategy, ProbStrategy 클래스 Context(문맥) - Strategy를 직접사용, ConcreteStrategy의 인스턴스를 가지고 있다가 필요에 따라 사용 (예제의 Player) 3. 예제 (가위 바위 보 게임) 책에서는 컴퓨터로 가위바위보 하는 프로그램을 예제로 들고 있다. 3-1. 구조 3-1-1. 전략\nWinningStrategy - 이기면 다음에도 같은 손을 내는 방식 ProbStrategy - 직전 손에서 다음손을 확률적으로 계산하는 방식 3-1-2. 클래스, 인스턴스\nHand - 가위바위보를 내는 손 클래스 Strategy - 가위바위보 전략 인터페이스 WinningStrategy - 이기면 같은 걸 내는 전략을 타나는 클래스 ProbStrategy - 직전손에서 다음손을 확률적으로 계산하는 클래스 Player - 가위바위보를 하는 플레이어 클래스 3-2. Hand enum 가위바위보를 어떤 걸 내는지를 나타나는 Hand 클래스부터 살펴보자\npublic enum Hand { // 가위 바위 보를 나타내는 세 개의 enum 상수 ROCK(\u0026#34;바위\u0026#34;, 0), SCISSORS(\u0026#34;가위\u0026#34;, 1), PAPER(\u0026#34;보\u0026#34;, 2); // enum이 가진 필드 private String name; // 가위 바위 보 손의 이름 private int handvalue; // 가위 바위 보 손의 값 // 손의 값으로 상수를 얻기 위한 배열 private static Hand[] hands = { ROCK, SCISSORS, PAPER }; // 생성자 private Hand(String name, int handvalue) { this.name = name; this.handvalue = handvalue; } // 손의 값으로 enum 상수를 가져온다 public static Hand getHand(int handvalue) { return hands[handvalue]; } // this가 h보다 강할 때 true public boolean isStrongerThan(Hand h) { return fight(h) == 1; } // this가 h보다 약할 때 true public boolean isWeakerThan(Hand h) { return fight(h) == -1; } // 무승부는 0, this가 이기면 1, h가 이기면 -1 private int fight(Hand h) { if (this == h) { return 0; } else if ((this.handvalue + 1) % 3 == h.handvalue) { return 1; } else { return -1; } } // 가위 바위 보의 문자열 표현 @Override public String toString() { return name; } } Hand - 가위/바위/보를 내는 손 자체를 나타내는 enum으로 각 행위를 enum 상수로 나타내고 getHand 메서드를 사용하여 Hand의 인스턴스를 얻을 수 있다. 예를 들어 가위 바위 보(0,1,2)를 각각 인수로 주면 값에 대응하는 인스턴스가 반환된다. isStrongerThan, isWeakerThan - Hand형인 두 손 객체의 강약을 비교한다. fight - 실제 손의 강약 판정한다. 예제에서는 두 손의 합으로 승패를 구분하는데, 가위 바위 보 가 각각 0/1/2 이기에 합을 3으로 나눈 값을 기준으로 승패를 판별할 수 있다. 3-3. Strategy 인터페이스 가위바위보 전략을 위한 추상 메서드의 집합체이다.\npublic interface Strategy { public abstract Hand nextHand(); public abstract void study(boolean win); } nextHand - 다음에 낼 손을 얻기 위한 메서드 study - 직전에 맨손으로 이겼는지 졌는지를 학습하는 메서드 직전 nextHand 메서드 호출에서 이긴 경우 study(true), 지면 study(false)를 각각 호출한다. 이에 따라 Strategy 인터페이스를 구현하는 클래스는 자신의 내부상태를 변화시키고 \b다음에 낼 nextHand의 반환 값을 결정한다.\n3-4. WinningStrategy 클래스 Strategy 인터페이스를 구현하는 클래스 중 하나로 nextHand, study 두 메서드를 구현한다.\npublic class WinningStrategy implements Strategy { private Random random; private boolean won = false; private Hand prevHand; public WinningStrategy(int seed) { random = new Random(seed); } @Override public Hand nextHand() { if (!won) { prevHand = Hand.getHand(random.nextInt(3)); } return prevHand; } @Override public void study(boolean win) { won = win; } } 이 클래스는 이전 승부가 이겼다면 다음번에 같은 걸 다시 내는 전략을 구현한 클래스이다. 만약 승부에 졌다면 다음은 랜덤 하게 낸다. won필드에는 이전 승부의 결과를 보관하며 이기면 true, 지면 false가 된다. preHand필드에는 이전 승부에서 낸 손 보관한다.\n3-5. ProbStrategy 클래스 Strategy 인터페이스를 구현하는 또 하나의 구체적인 전략 클래스로, nextHand, study 두 메서드를 구현한다.\npublic class ProbStrategy implements Strategy { private Random random; private int prevHandValue = 0; private int currentHandValue = 0; private int[][] history = { { 1, 1, 1, }, { 1, 1, 1, }, { 1, 1, 1, }, }; public ProbStrategy(int seed) { random = new Random(seed); } @Override public Hand nextHand() { int bet = random.nextInt(getSum(currentHandValue)); int handvalue = 0; if (bet \u0026lt; history[currentHandValue][0]) { handvalue = 0; } else if (bet \u0026lt; history[currentHandValue][0] + history[currentHandValue][1]) { handvalue = 1; } else { handvalue = 2; } prevHandValue = currentHandValue; currentHandValue = handvalue; return Hand.getHand(handvalue); } private int getSum(int handvalue) { int sum = 0; for (int i = 0; i \u0026lt; 3; i++) { sum += history[handvalue][i]; } return sum; } @Override public void study(boolean win) { if (win) { history[prevHandValue][currentHandValue]++; } else { history[prevHandValue][(currentHandValue + 1) % 3]++; history[prevHandValue][(currentHandValue + 2) % 3]++; } } } 다음에 낼 손을 난수로 결정하고, 과거의 이기고 진 이력을 활용해서 확률을 바꾸는 전략을 구현한 클래스이다.\nhistory 필드는 과거 승패를 반영한 확률 계산을 위한 2차원 배열이다.\nhistory [직전에 낸 손] [이번에 낼 손] = 승리한 횟수\\\nex) history [0][1] - 바위, 바위를 냈을 때 승수 history [0][0]- 바위, 가위를 냈을 때 승수 ...\n즉, 직전에 바위를 냈다면 history [0]의 배열의 승수를 비율을 통해 서로 다른 비율로 손을 결정한다. 예를 들어 처음 바위를 냈을 때 이긴 승수가 다음과 같다면\nhistory [0][0] = 3\nhistory [0][1] = 2 history [0][2] = 5 전체 승수 10 대비 각 손을 냈을 때의 승률을 기준으로 다음 손을 결정한다. (ex. 바위를 다시 낼 확률 = 3/10)\n3-6. Player 클래스 가위바위보를 하는 사람을 나타내는 클래스로 이름과 전략으로 인스턴스가 생성된다.\npublic class Player { private String name; private Strategy strategy; private int wincount; private int losecount; private int gamecount; // 이름과 전략을 받아서 플레이어를 만든다 public Player(String name, Strategy strategy) { this.name = name; this.strategy = strategy; } // 전략에 따라 다음 손을 결정한다 public Hand nextHand() { return strategy.nextHand(); } // 승리 public void win() { strategy.study(true); wincount++; gamecount++; } // 패배 public void lose() { strategy.study(false); losecount++; gamecount++; } // 무승부 public void even() { gamecount++; } @Override public String toString() { return \u0026#34;[ + name + \u0026#34;: + gamecount + \u0026#34; games, + wincount + \u0026#34; win, + losecount + \u0026#34; lose + \u0026#34;]\u0026#34;; } } nextHand메서드는 자신의 전략을 Strategy에 위임하고 있다. 승부 결과를 다음 승부에 활용하고자 Strategy필드를 통해 study메서드를 호출하고, study 메서드로 전략의 내부상태를 변화시킨다.\n3-7. Main 이제 실제 다른 전략의 플레이들로 가위바위보를 하는 메인 클래스를 생성해 보자\npublic class Main { public static void main(String[] args) { if (args.length != 2) { System.out.println(\u0026#34;Usage: java Main randomseed1 randomseed2\u0026#34;); System.out.println(\u0026#34;Example: java Main 314 15\u0026#34;); System.exit(0); } int seed1 = Integer.parseInt(args[0]); int seed2 = Integer.parseInt(args[1]); Player player1 = new Player(\u0026#34;KIM\u0026#34;, new WinningStrategy(seed1)); Player player2 = new Player(\u0026#34;LEE\u0026#34;, new ProbStrategy(seed2)); for (int i = 0; i \u0026lt; 10000; i++) { Hand nextHand1 = player1.nextHand(); Hand nextHand2 = player2.nextHand(); if (nextHand1.isStrongerThan(nextHand2)) { System.out.println(\u0026#34;Winner:\u0026#34; + player1); player1.win(); player2.lose(); } else if (nextHand2.isStrongerThan(nextHand1)) { System.out.println(\u0026#34;Winner:\u0026#34; + player2); player1.lose(); player2.win(); } else { System.out.println(\u0026#34;Even...\u0026#34;); player1.even(); player2.even(); } } System.out.println(\u0026#34;Total result:\u0026#34;); System.out.println(player1); System.out.println(player2); } } 두 명의 플레이어를 각각 다른 Strategy 구현체로 생성한 후 서로 가위바위보를 시키는 클래스이다.\n4. Strategy(전략) 패턴의 장점 및 정리 Strategy 패턴은 알고리즘을 다른 부분과 의도적으로 분리한다. 얼핏 보면 불필요하고 복잡한 구조를 오히려 만드는 것 같지만 그렇지 않다.\n인터페이스만 정의하고 위임을 통해 약한 결합으로 알고리즘을 사용하기에 알고리즘을 용이하게 전환할 수 있고, 알고리즘을 변경해야 하는 경우 ConcreteStrategy만 수정하면 된다. 또한 전략패턴을 사용 시 프로그램을 실행 중에 ConcreteStrategy 역을 전환할 수도 있다. 전략을 캡슐화하고 이를 사용하는 콘텍스트에서 독립적으로 전략을 변경해 줄 수 있기 때문이고, 실행 중인 애플리케이션 내에서 사용자 선택이나 조건에 따라 정렬방식/암호화/난수생성 등의 전략을 바꿀 수 있기 때문에 유연성과 확장성이 올라간다. 참고 : JAVA 언어로 배우는 디자인 패턴 입문 3편\n상세 예제소스는 깃허브에서 확인가능\nhttps://github.com/junhkang/java-design-pattern/tree/master/src/main/java/com/example/javadesignpattern/strategy\n","permalink":"http://localhost:50666/posts/87/","summary":"\u003chr\u003e\n\u003ch2 id=\"전략strategy-패턴이란\" ke-size=\"size26\"\u003e1. 전략(Strategy) 패턴이란?\u003c/h2\u003e\n\u003cp\u003eStrategy는 전략이라는 뜻으로 적을 해치우는 작전, 게임을 이기는 전략, 문제를 풀어나가는 방법 등의 의미가 있고, 특히 프로그래밍에서는 문제를 해결하는 방식인 \u0026quot;알고리즘\u0026quot;을 의미한다.\n \u003c/p\u003e\n\u003cp\u003e모든 프로그램은 문제를 해결하기 위해 존재하며, 특정 알고리즘으로 구현된다. Strategy 패턴에서는 이미 구현한 알고리즘을 쉽게 모두 교체할 수 있다. 즉, 스위치를 누르듯 쉽게 알고리즘을 바꿔서 같은 문제를 다른 방법으로 해결하기 쉽게 만들어주는 패턴이 Strategy 패턴이다.\u003c/p\u003e\n\u003ch2 id=\"전략strategy-패턴의-구조\" ke-size=\"size26\"\u003e2. 전략(Strategy) 패턴의 구조\u003c/h2\u003e\n\u003cp\u003e전략 패턴의 구조와 예제에서 각 요소가 어떤 역할을 하는지 간단하게 먼저 살펴보자.\u003c/p\u003e","title":"[디자인패턴] 전략(Strategy) 패턴의 개념, 예제, 장단점, 활용"},{"content":" 1. 순번 부여하기 PostgreSQL에서는 각 데이터에 의미 있는 순번을 부여하기 위해 ROW_NUMBER(), RANK(), DENSE_RANK() 함수를 제공한다. ROW_NUMBER() OVER(PARTITION BY * ORDER BY * ) RANK() OVER(PARTITION BY * ORDER BY * ) DENSE_RANK() OVER(PARTITION BY * ORDER BY * ) 예제를 통해 자세한 사용법을 알아보자. (2. 테스트 테이블 \u0026amp; 데이터 생성 참고)\n예제에서 사용할 데이터\n1-1. ROW_NUMBER() 1-1-1. 단일 그룹 순번 부여 SELECT ROW_NUMBER() OVER (ORDER BY BRAND) AS ROWNUM, * FROM TEST_COMPLEX_GROUP; 특정 그룹에 대한 설정 없이 순번만을 지정할 경우, 전체 데이터를 기준으로 명시된 order by 순서대로 순번을 부여한다. 예제에서는 BRAND의 순서대로 정렬된 순번을 차례로 부여하였다. 1-1-2. 다중 그룹 순번 부여 보통 단일 그룹으로 순번을 부여하기보다, 부서별, 사이즈별 순번 등 다중 그룹에 대한 개별 순번을 부여할 일이 많이 있다. 그럴 경우 다음과 같이 PARTITION BY를 통해 순번 그룹을 명시해 주면 그룹별 순번을 확인할 수 있다.\nSELECT ROW_NUMBER() OVER (PARTITION BY BRAND ORDER BY SALES) AS ROWNUM, * FROM TEST_COMPLEX_GROUP; 물론 PARTITION BY에 여러 그룹을 명시함으로써 여러 그룹에 대한 순위를 얻을수도 있다.\nSELECT ROW_NUMBER() OVER (PARTITION BY BRAND, COLOR ORDER BY SALES) AS ROWNUM, * FROM TEST_COMPLEX_GROUP; 1-2. RANK() RANK()는 같은 순위의 값에 대해 같은 순위를 부여하고, 그 다음 순위는 중복된 만큼을 스킵한 다음 순번을 부여한다. (공동 3위가 두명일 경우, 다음은 5위)\nSELECT RANK() OVER (PARTITION BY BRAND ORDER BY SALES) AS ROWNUM, * FROM TEST_COMPLEX_GROUP; 1-3. DENSE_RANK() DENSE_RANK()는 같은 순위의 값에 대해 같은 순위를 부여하고, 그다음 순위는 중복순위에 상관없이 연속된 순번을 부여한다. (공동 3위가 두명일 경우, 다음은 4위)\n2. 테스트 테이블 \u0026amp; 데이터 생성 CREATE TABLE TEST_COMPLEX_GROUP ( BRAND VARCHAR(10), SIZE VARCHAR(1), COLOR VARCHAR(10), SALES INTEGER ) INSERT INTO TEST_COMPLEX_GROUP (BRAND, SIZE, COLOR, SALES) VALUES (\u0026#39;FOO\u0026#39;, \u0026#39;L\u0026#39;, \u0026#39;BLUE\u0026#39;, 10), (\u0026#39;FOO\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;BLUE\u0026#39;, 20), (\u0026#39;FOO\u0026#39;, \u0026#39;L\u0026#39;, \u0026#39;RED\u0026#39;, 30), (\u0026#39;BAR\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;RED\u0026#39;, 15), (\u0026#39;BAR\u0026#39;, \u0026#39;L\u0026#39;, \u0026#39;GREEN\u0026#39;, 5), (\u0026#39;BAR\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;GREEN\u0026#39;, 25); ","permalink":"http://localhost:50666/posts/86/","summary":"\u003chr\u003e\n\u003ch2 id=\"순번-부여하기\" ke-size=\"size26\"\u003e1. 순번 부여하기 \u003c/h2\u003e\n\u003cp\u003ePostgreSQL에서는 각 데이터에 의미 있는 순번을 부여하기 위해 ROW_NUMBER(), RANK(), DENSE_RANK() 함수를 제공한다. \u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eROW_NUMBER() OVER(PARTITION BY * ORDER BY * )\n\nRANK() OVER(PARTITION BY * ORDER BY * )\n\nDENSE_RANK() OVER(PARTITION BY * ORDER BY * )\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e \u003c/p\u003e\n\u003cp\u003e예제를 통해 자세한 사용법을 알아보자. (2. 테스트 테이블 \u0026amp; 데이터 생성 참고)\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/86/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-04-11%20%EC%98%A4%ED%9B%84%201.56.30.png\"\u003e\u003c/p\u003e\n\u003cp\u003e예제에서 사용할 데이터\u003c/p\u003e\n\u003ch3 id=\"row_number\" ke-size=\"size23\"\u003e1-1. ROW_NUMBER()\u003c/h3\u003e\n\u003ch4 id=\"단일-그룹-순번-부여\" ke-size=\"size20\"\u003e          1-1-1. 단일 그룹 순번 부여\u003c/h4\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eSELECT ROW_NUMBER() OVER (ORDER BY BRAND) AS ROWNUM, *\nFROM TEST_COMPLEX_GROUP;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/86/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-04-11%20%EC%98%A4%ED%9B%84%201.57.20.png\"\u003e\u003c/p\u003e","title":"[PostgreSQL] ROWNUM 사용과 순번 부여하기 - ROW_NUMBER(), RANK(), DENSE_RANK()"},{"content":" 1. 버전 확인이 왜 필요한가 PostgreSQL를 안정적으로 운영하기 위해서는 보안 패치와 업데이트를 최신 상태로 유지하는 것이 중요하다. 또한 특정 기능이나 호환성 요구 사항을 충족하기 위해서도 버전 정보를 알고 있어야 한다. 이를 위해 현재 버전 및 공식 문서의 버전 관리 내용을 주기적으로 확인해주어야 한다. 2. PostgreSQL 버전 확인 방법 2-1. SQL 쿼리 실행: VERSION() PostgreSQL 쿼리로 가장 간단하게 버전 정보를 얻을 수 있는 방법이다. 서버의 버전정보를 포함한 상세정보를 확인할 수 있다.\nSELECT VERSION(); 2-2. SQL 쿼리 실행: \bSERVER_VERSION 또 다른 SQL 쿼리 옵션은 SHOW SERVER_VERSION으로, 서버의 버전 번호만을 반환한다. 서버의 정확한 버전 번호만 필요한 경우 유용하다.\nSHOW SERVER_VERSION; 2-3. psql 사용 데이터베이스 서버가 아닌 PostgreSQL 클라이언트의 버전을 확인하고 싶을 때 사용 가능하다. SQL콘솔이 아닌 SQL서버에 실행하면 된다.\npsql --version 2-4. GUI에서 확인 (Datagrip, Dbeaver) Datagrip을 기준으로 DB연결 Properties의 \u0026quot;Data Sources and Drivers\u0026quot; 하단 \u0026quot;Test Connection\u0026quot; 버튼 옆에서 버전을 확인할 수 있다.\n버전별 호환성 및 기능 정의는 공식문서를 통해 확인가능하다.\nhttps://www.postgresql.org/docs/ ","permalink":"http://localhost:50666/posts/85/","summary":"\u003chr\u003e\n\u003ch2 id=\"버전-확인이-왜-필요한가\" ke-size=\"size26\"\u003e1. 버전 확인이 왜 필요한가\u003c/h2\u003e\n\u003cp\u003ePostgreSQL를 안정적으로 운영하기 위해서는 보안 패치와 업데이트를 최신 상태로 유지하는 것이 중요하다. 또한 특정 기능이나 호환성 요구 사항을 충족하기 위해서도 버전 정보를 알고 있어야 한다. 이를 위해 현재 버전 및 공식 문서의 버전 관리 내용을 주기적으로 확인해주어야 한다. \u003c/p\u003e\n\u003ch2 id=\"postgresql-버전-확인-방법\" ke-size=\"size26\"\u003e2. PostgreSQL 버전 확인 방법\u003c/h2\u003e\n\u003ch3 id=\"sql-쿼리-실행-version\" ke-size=\"size23\"\u003e2-1. SQL 쿼리 실행: VERSION()\u003c/h3\u003e\n\u003cp\u003ePostgreSQL 쿼리로 가장 간단하게 버전 정보를 얻을 수 있는 방법이다. 서버의 버전정보를 포함한 상세정보를 확인할 수 있다.\u003c/p\u003e","title":"[PostgreSQL] 버전 확인: 필요성과 4가지 방법"},{"content":" 1. SETS, CUBE, ROLLUP의 개념 및 사용법 고급 \u0026quot;GROUP BY\u0026quot;의 기능들로 PostgreSQL에서는 SETS, CUBE, ROLLUP 기능을 제공한다. 기본적인 콘셉트는 일반 GROUP BY와 동일하게 FROM / WHERE 절에서 선택된 데이터는 각각 지정된 그룹으로 GROUP BY 되고, 각 그룹에 대해 집계가 계산된 후, 결과가 반환된다.\n다음은 테스트로 사용할 테이블 정보이다. (마지막 장의 4. 테이블 \u0026amp; 데이터 생성 참고) 1-1. GROUP BY SETS의 개념 및 사용법 GROUPING SETS의 각 하위 요소(subsets)들은 하나 이상의 열 혹은 표현식을 지정할 수 있으며 조합에 맞게 집계 결과를 별도로 계산한다.\nSELECT ((brand), (size)), brand, size, sum(sales) FROM test_complex_group GROUP BY GROUPING SETS ((brand), (size), ()); 예시의 쿼리에서는 brand와 size를 독립적으로 설정했기에 brand에 대한 집계 size에 대한 집계 전체 합계 (빈 괄호는 모든 행을 단일 그룹으로 취급하여 집계) 가 이루어졌지만 (brand, size)를 결합하여 GROUPING SETS로 사용하였다면 다음과 같은 기준으로 집계가 된다.\nbrand, size를 둘 다 그룹으로 취급하여 집계 SELECT (brand, size), brand, size, sum(sales) FROM test_complex_group GROUP BY GROUPING SETS ((brand, size)); GROUPING SETS가 비어있으면 모든 ROW를 1개의 그룹으로 그룹화시키는 것과 같다. (GROUP BY가 없는 집계함수의 경우와 동일) SELECT sum(sales) FROM test_complex_group GROUP BY GROUPING SETS (()); 1-2. ROLLUP의 개념 및 사용법 ROLLUP은 지정된 열(또는 표현식)에 대해 계층적인 집계를 생성한다. 여기서 계층적이라 함은 열을 기준으로 각 열의 조합 및 그 조합의 모든 가능한 접두사 부분 집합에 대해 데이터를 그룹화하고 집계한다는 뜻이다. 공식문서에 ROLLUP을 GROUPING SETS로 변환한 내용을 확인해 보면 이해에 도움이 된다.\nROLLUP ( e1, e2, e3, ... ) -- 동일 GROUPING SETS ( ( e1, e2, e3, ... ), ... ( e1, e2 ), ( e1 ), ( ) ) 이제 테스트 테이블에서 결과를 확인해 보자.\nSELECT BRAND, SIZE, COLOR, SUM(SALES) FROM TEST_COMPLEX_GROUP GROUP BY ROLLUP (BRAND, SIZE, COLOR); 결과에서 확인할 수 있듯이, (brand, size, color) 뿐 아니라 (brand, size), brand도 GROUPING 대상이 된다. 일반적으로 계층 구조의 데이터 분석에서 많이 사용된다. (전체, 부서, 파트별 총 월급 등을 한 번에 조회할 때)\n1-3. CUBE의 개념 및 사용법 CUBE는 모든 가능한 부분집합을 GROUPING 대상으로 사용한다. ROLLUP 이 해당 요소를 접두사로 사용하는 요소만을 대상으로 하는 반면에 전체 경우의 수를 모두 조회한다. 공식 문서에 CUBE를 GROUPING SETS로 변환한 예제를 보면 이해에 도움이 된다.\nCUBE ( a, b, c ) -- 동일 GROUPING SETS ( ( a, b, c ), ( a, b ), ( a, c ), ( a ), ( b, c ), ( b ), ( c ), ( ) ) 이제 테스트 테이블에서 결과를 확인해 보자.\nSELECT BRAND, SIZE, COLOR, SUM(SALES) FROM TEST_COMPLEX_GROUP GROUP BY CUBE (BRAND, SIZE, COLOR); 2. 고급 GROUPING의 응용 2-1. SUBLISTS의 사용 CUBE, ROLLUP은 개별 표현 식나 괄호를 포함한 sublist를 포함할 수 있다. sublist를 사용할 경우 개별 GROUPING SETS를 생성하기 위한 단일 단위로만 취급된다. (sublist 내부의 각 항목별로 구분되진 않는다.)\nCUBE ( (a, b), (c, d) ) --동일 GROUPING SETS ( ( a, b, c, d ), ( a, b ), ( c, d ), ( ) ) ROLLUP ( a, (b, c), d ) -- 동일 GROUPING SETS ( ( a, b, c, d ), ( a, b, c ), ( a ), ( ) ) 2-2. GROUPING의 중첩 사용 CUBE, ROLLUP은 GROUP BY 절에 직접 사용되거나, GROUPING SETS 절 내부에 중첩되어서 사용 가능하다. 만약 하나의 GROUPING SETS 절이 다른 절 내에 중첩되는 경우, 내부 절의 모든 요소는 외부 절에 직접 작성된 것과 동일한 결과를 출력한다. GROUP BY 절에 여러 개의 GROUPING 요소가 있다면, 모든 GROUPING 요소에 대해 가능한 모든 조합이 기준으로 사용되기에 최종 GROUPING SETS 목록은 각 항목의 교차의 곱으로 생성된다.\nGROUP BY a, CUBE (b, c), GROUPING SETS ((d), (e)) -- 동일 GROUP BY GROUPING SETS ( (a, b, c, d), (a, b, c, e), (a, b, d), (a, b, e), (a, c, d), (a, c, e), (a, d), (a, e) ) 실제 테스트 데이터에서 확인해 보자.\nSELECT BRAND, SIZE, COLOR, SUM(SALES) FROM TEST_COMPLEX_GROUP group by brand, cube(size, color); 결과를 보면 brand만으로 GROUPING 된 것이 아니라, 중첩된 GROUPING 절의 교차의 곱으로 생성된다. brand : 2개 조합 cube (size, color) (size, color) : 6개 조합 size : 2개 조합 color : 3개 조합 () : 1개 조합 단순 계산으로는 (6+2+3+1) x 2 = 24의 조합이 구성되지만, 데이터 분포상 브랜드별로 8개의 조합이 가능하기에 총 16개의 그루핑 결과가 노출된다.\n2-3. DISTINCT 다중 그루핑된 항목을 여러 개 지정할 경우, 최종 그루핑 세트 목록이 중복될 수 있다.\nSELECT BRAND, SIZE, COLOR, SUM(SALES) FROM TEST_COMPLEX_GROUP group by rollup(brand, color), rollup (brand, size); 이 경우,\nrollup (brand, color) - (brand, color), brand brand, color의 모든 조합 각 brand에 대한 color의 서브 토털 전체 토털 rollup (brand, size) = (brand, size), size brand와 size의 모든 조합 각 brand에 대한 size의 서브 토털 전체 토털 를 교차로 GROUPING 하기에 다음 두 결과 셋을 교차로 GROUPING 하게 된다.\n두 조건을 교차로 GROUPING 한 후 유효한 값들만 출력된 결과는 다음과 같다.\nbrand에 대한 서브 토털이 중복되어 나타나기에 최종적으로 전체 토털도 중복되어 출력된다. 의도하지 않은 중복이 발생한 경우, 단일 ROLLUP으로 쿼리를 튜닝해도 되지만, DISTINCT절을 사용하여 중복을 명시적으로 제거할 수도 있다.\nSELECT BRAND, SIZE, COLOR, SUM(SALES) FROM TEST_COMPLEX_GROUP GROUP BY DISTINCT ROLLUP (BRAND, COLOR), ROLLUP ( BRAND, SIZE) ORDER BY BRAND, SIZE, COLOR; 결과를 중복 제거하는 것이 아닌, 그루핑 조건의 중복제거를 하는 것이기에 SELECT DISTINCT와는 다른 개념이다.\n3. 주의사항 3-1. 결과가 NULL인 집계 그룹에 포함되지 않은 다른 열의 값이 NULL인 경우, 특정 열이 그룹화되면서 그 결과로 해당 열의 값이 NULL이 되는 경우(ex. 집계함수를 사용하는 경우 특정 그룹에 해당하는 데이터가 없어 결과가 NULL인 경우) 사이의 구분이 되지 않아 같은 취급을 당하기에 주의해야 한다.\n3-2. 명시적 ROW 생성 (a, b) 형태의 표현식은 일반적으로 쿼리 내에서 ROW 생성자로 인식되어 여러 개의 값을 하나의 행으로 묶는다.\nSELECT (COLOR, SIZE) FROM TEST_COMPLEX_GROUP; 하지만 GROUP BY 절에서는 이러한 규칙이 최상위 수준의 표현식에는 적용되지 않으며 a, b를 별도의 표현식으로 해석하여 각각을 GROUPING 기준으로 사용한다. GROUPING 표현식 내에서 (a, b)를 하나의 단일 그루핑 기준으로 사용하고 싶다면 ROW(a, b)을 사용하여 명시적으로 행을 생성해야 한다.\nGROUP BY (a, b) = GROUP BY a, b!= GROUP BY ROW(a, b)\n4. 테스트 테이블\u0026amp;데이터 생성 CREATE TABLE TEST_COMPLEX_GROUP ( BRAND VARCHAR(10), SIZE VARCHAR(1), COLOR VARCHAR(10), SALES INTEGER ) INSERT INTO TEST_COMPLEX_GROUP (BRAND, SIZE, COLOR, SALES) VALUES (\u0026#39;FOO\u0026#39;, \u0026#39;L\u0026#39;, \u0026#39;BLUE\u0026#39;, 10), (\u0026#39;FOO\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;BLUE\u0026#39;, 20), (\u0026#39;FOO\u0026#39;, \u0026#39;L\u0026#39;, \u0026#39;RED\u0026#39;, 30), (\u0026#39;BAR\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;RED\u0026#39;, 15), (\u0026#39;BAR\u0026#39;, \u0026#39;L\u0026#39;, \u0026#39;GREEN\u0026#39;, 5), (\u0026#39;BAR\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;GREEN\u0026#39;, 25); 참고 https://www.postgresql.org/docs/16/queries-table-expressions.html#QUERIES-GROUPING-SETS ","permalink":"http://localhost:50666/posts/84/","summary":"\u003chr\u003e\n\u003ch2 id=\"sets-cube-rollup의-개념-및-사용법\" ke-size=\"size26\"\u003e1. SETS, CUBE, ROLLUP의 개념 및 사용법\u003c/h2\u003e\n\u003cp\u003e고급 \u0026quot;GROUP BY\u0026quot;의 기능들로 PostgreSQL에서는 \u003cstrong\u003eSETS, CUBE, ROLLUP\u003c/strong\u003e 기능을 제공한다. 기본적인 콘셉트는 일반 GROUP BY와 동일하게 FROM / WHERE 절에서 선택된 데이터는 각각 지정된 그룹으로 GROUP BY 되고, 각 그룹에 대해 집계가 계산된 후, 결과가 반환된다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e다음은 테스트로 사용할 테이블 정보이다. (마지막 장의 4. 테이블 \u0026amp; 데이터 생성 참고)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/84/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-04-09%20%EC%98%A4%ED%9B%84%202.20.01.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"group-by-sets의-개념-및-사용법\" ke-size=\"size23\"\u003e1-1. GROUP BY SETS의 개념 및 사용법\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eGROUPING SETS\u003c/strong\u003e의 각 하위 요소(subsets)들은 하나 이상의 열 혹은 표현식을 지정할 수 있으며 조합에 맞게 집계 결과를 별도로 계산한다.\u003c/p\u003e","title":"[PostgreSQL] 고급 GROUPING 전략 : SETS, CUBE, ROLLUP의 개념, 효과적인 사용법 및 주의사항"},{"content":" 1. UNION, INTERSECT, EXCEPT를 통한 쿼리 결합 UNION(결합), INTERSECT(교차), EXCEPT(차이) 구문을 통해 두 쿼리의 결과를 결합할 수 있다.\nquery1 UNION [ALL] query2 query1 INTERSECT [ALL] query2 query1 EXCEPT [ALL] query2 해당 구문들을 실행시키기 위해서는 query1, query2가 동일한 개수, 동일한 type의 칼럼을 리턴해야 한다.\n2. UNION query2의 결과를 query1에 이어 붙인다. 그냥 사용할 경우 중복을 제거하여 distinct와 같은 효과를 볼 수 있으며, UNION ALL을 사용하면 중복을 포함하여 쿼리를 합친다.\n2-1. UNION 단일 사용 1~5 번째 ROWS, 4~8번째 ROWS를 합친 후 중복 제거한 결과를 보면 다음과 같다.\n(SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5) UNION (SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5 OFFSET 3) ORDER BY ID; 2-2. UNION ALL 1~5 번째 ROWS, 4~8번째 ROWS를 합친 결과를 보면 다음과 같다. (1000004, 1000005 중복 출력)\n(SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5) UNION ALL (SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5 OFFSET 3) ORDER BY ID; 3. INTERSECT query1과 query2에 동시에 존재하는 ROWS를 반환한다. INTERSECT ALL을 사용하면 중복을 제거해 준다. 예제에선 동일 값을 중복생성하기 위해 UNION과 결합해서 사용하였다.\n3-1. INTERSECT 단일 사용 [1~5 번째 ROWS X 2, 4~8번째 ROWS X 2중 교차되는 ROWS만 중복 제거한 후 출력(1000004, ]{style=\u0026ldquo;color: #333333; text-align: start;\u0026quot;}1000005가 중복 제거된 후 출력)\n((SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5) UNION ALL (SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5)) INTERSECT ((SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5 OFFSET 3) UNION ALL (SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5 OFFSET 3)) ORDER BY ID; 3-2. INTERSECT ALL 1~5 번째 ROWS X 2, 4~8번째 ROWS X 2중 교차되는 ROWS만 중복을 포함하여 출력(1000004, 1000005가 두 번씩 중복 출력)\n((SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5) UNION ALL (SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5)) INTERSECT ALL ((SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5 OFFSET 3) UNION ALL (SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5 OFFSET 3)) ORDER BY ID; 4. EXCEPT query1에는 존재하고, query2에는 존재하지 않는 ROWS를 반환한다. EXCEPT ALL을 사용하면 중복을 제거해 준다. 위 예제와 동일하게 중복된 상황을 위해 UNION 쿼리와 결합하여 사용하였다.\n4-1. EXCEPT 단일 사용 선행쿼리에 존재하고, 후행쿼리에 존재하지 않는 ROWS를 중복을 제거한 후 반환한다.\n((SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5) UNION ALL (SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5)) EXCEPT (SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5 OFFSET 3) ORDER BY ID; 4-2. EXCEPT ALL 선행쿼리에 존재하고, 후행쿼리에 존재하지 않는 ROWS를 중복 제거 없이 모두 반환한다.\n((SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5) UNION ALL (SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5)) EXCEPT ALL (SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 5 OFFSET 3) ORDER BY ID; 주의할 점은, EXCEPT만 사용할 경우, 선행쿼리에 동일한 ROWS가 아무리 많더라도 후행쿼리에 하나라도 존재할 경우 모두 삭제되지만, EXCEPT ALL로 중복을 모두 리턴할 경우, 후행 쿼리에 존재하는 만큼만 ROWS가 삭제된다.\n(EXCEPT ALL의 경우 1000003, 1000004 ROWS가 후행 쿼리에 존재하지만, 선행쿼리에 2개씩 존재하기에 여전히 출력되고 있다.) 5. 복합 사용 세 가지 구문은 복합적으로 사용할 수 있다. 예를 들어 다음 두 개의 쿼리는 같은 결과를 리턴한다.\nquery1 UNION query2 EXCEPT query3 -- 동일 (query1 UNION query2) EXCEPT query3 해당 예제에서 확인할 수 있듯, 괄호를 통해 구문의 실행 순서를 조절할 수 있다. 괄호가 없을 때 UNION, EXCEPT는 왼쪽에서 오른쪽으로 실행되며, INTERSECT가 최우선으로 적용된다. (공통부분 추출이기에 더 엄격한 필터 취급)\nquery1 UNION query2 INTERSECT query3 -- 동일 query1 UNION (query2 INTERSECT query3) 또한 개별 쿼리를 괄호로 지정할 수 있으며, LIMIT/OFFSET 같이 특정 쿼리에만 지정하고 싶은 구문이 있을 경우 다르게 적용할 수 있다.\n(2~4 예제에서 사용 중)\n-- UNION 결과에 LIMIT 10 적용 SELECT a FROM b UNION SELECT x FROM y LIMIT 10 -- UNION 결과에 LIMIT 10 적용 (1과 동일) (SELECT a FROM b UNION SELECT x FROM y) LIMIT 10 -- 후행 쿼리 결과에만 LIMIT 10 적용 SELECT a FROM b UNION (SELECT x FROM y LIMIT 10) 참고 https://www.postgresql.org/docs/16/queries-union.html\n","permalink":"http://localhost:50666/posts/83/","summary":"\u003chr\u003e\n\u003ch2 id=\"union-intersect-except를-통한-쿼리-결합\" ke-size=\"size26\"\u003e1. UNION, INTERSECT, EXCEPT를 통한 쿼리 결합\u003c/h2\u003e\n\u003cp\u003eUNION(결합), INTERSECT(교차), EXCEPT(차이) 구문을 통해 두 쿼리의 결과를 결합할 수 있다.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003equery1 UNION [ALL] query2\nquery1 INTERSECT [ALL] query2\nquery1 EXCEPT [ALL] query2\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e해당 구문들을 실행시키기 위해서는 query1, query2가 동일한 개수, 동일한 type의 칼럼을 리턴해야 한다.\u003c/p\u003e\n\u003ch2 id=\"union\" ke-size=\"size26\"\u003e2. UNION\u003c/h2\u003e\n\u003cp\u003equery2의 결과를 query1에 이어 붙인다. 그냥 사용할 경우 중복을 제거하여 distinct와 같은 효과를 볼 수 있으며, UNION ALL을 사용하면 중복을 포함하여 쿼리를 합친다.\u003c/p\u003e\n\u003ch3 id=\"union-단일-사용\" ke-size=\"size23\"\u003e2-1. UNION 단일 사용\u003c/h3\u003e\n\u003cp\u003e1~5 번째 ROWS, 4~8번째 ROWS를 합친 후 중복 제거한 결과를 보면 다음과 같다.\u003c/p\u003e","title":"[PostgreSQL] UNION, INTERSECT, EXCEPT, SQL 쿼리 결합의 개념, 사용법 및 주의사항"},{"content":" 1. Limit과 Offset의 개념 및 사용법 Limit과 Offset은 쿼리 결과 중 특정 부분의 ROW만을 조회하게 해주는 기능이다.\nSELECT select_list FROM table_expression [ ORDER BY ... ] [ LIMIT { number | ALL } ] [ OFFSET number ] 예를 들어 ID순서로 정렬된 결과 셋에서 21번째부터 30번째의 ROWS를 반환하고 싶다면 다음과 같이 사용하면 된다.\n-- 21~30번째 열 조회 SELECT * FROM TEST_EXPLAIN ORDER BY ID LIMIT 10 OFFSET 20 LIMIT을 지정하면 해당 값만큼의 결과만 출력되며 총 결과 값이 더 적을 경우 있는 만큼만 출력한다. LIMIT ALL, LIMIT NULL은 LIMIT을 설정하지 않은 것 (전체 ROWS 리턴)과 동일하다. OFFSET은 리턴되는 ROWS의 시작점을 지정한다. OFFSET 0, OFFSET NULL은 OFFSET을 설정하지 않은 것 (첫 ROW부터 리턴)과 동일하다. OFFSET과 LIMIT을 둘 다 사용할 경우, LIMIT 카운트를 세기 전에 OFFSET만큼의 ROWS가 앞에서 생략된다. 2. 주의점 LIMIT을 사용할 경우 결과 ROWS의 순서를 유니크하게 만들어주는 ORDER BY와 함께 쓰는 것이 중요하다. 그게 아니라면 예상하지 못한 부분 집합을 조회하게 될 수 있다. 앞의 예시 쿼리에서 ORDER BY를 뺀다면, 10~20 번째를 가져오려고 해도, 어떤 순서에서의 10~20 번째인지 알 수 없다.\n-- 21~30번째 열 조회 (순서 없이) SELECT * FROM TEST_EXPLAIN LIMIT 10 OFFSET 20 다음 예시에서 볼 수 있듯이, 쿼리 옵티마이저는 쿼리 PLAN을 생성할 때 LIMIT을 고려하여 생성하기 때문에, LIMIT, OFFSET이 다를 경우 다른 PLAN이 생성되며, 서로 다른 순서의 결과를 얻을 확률이 매우 높다. 그래서 쿼리 결과에 부분 집합을 선택하기 위해 ORDER BY 없이 LIMIT / OFFSET 값을 사용하는 경우, 일관성 없는 결과를 얻게 된다. 주로 LIMIT / OFFSET은 페이징 처리를 하여 특정 순서의 일정량의 데이터를 반환할 때 사용하게 되는데, 이 경우 \u0026ldquo;유니크\u0026quot;한 정렬에 주의하여야 한다. 다음 예제를 보면, 동일 값을 다수 보유하고 있는 address 칼럼을 기준으로 정렬 후 LIMIT/OFFSET을 설정하면, 21~30번째 ROWS와 31~40번째의 ROWS가 연속됨을 보장할 수 없다.\nselect * from TEST_EXPLAIN order by address LIMIT 10 OFFSET 20; select * from TEST_EXPLAIN order by address LIMIT 10 OFFSET 30; 각각 31~40, 41~50번째 ROWS를 추출하였으나 동일한 값도 포함되어 있고, 순서도 명확하지 않다. 따라서 LIMIT / OFFSET을 통해 부분 데이터를 추출할 경우, 단순히 ORDER BY를 하는 것이 아닌, 유니크한 순서 정렬이 될 수 있도록 정렬해 주어야 한다. 또한 OFFSET으로 생략된 앞부분은 서버 내부에서 계산이 되어야 하기 때문에, 아주 큰 OFFSET을 설정하는 것은 효율적이지 못할 수 있다. ","permalink":"http://localhost:50666/posts/82/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/82/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"limit과-offset의-개념-및-사용법\" ke-size=\"size26\"\u003e1. Limit과 Offset의 개념 및 사용법\u003c/h2\u003e\n\u003cp\u003eLimit과 Offset은 쿼리 결과 중 특정 부분의 ROW만을 조회하게 해주는 기능이다.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eSELECT select_list\n    FROM table_expression\n    [ ORDER BY ... ]\n    [ LIMIT { number | ALL } ] [ OFFSET number ]\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e예를 들어 ID순서로 정렬된 결과 셋에서 21번째부터 30번째의 ROWS를 반환하고 싶다면 다음과 같이 사용하면 된다.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e-- 21~30번째 열 조회\nSELECT * FROM TEST_EXPLAIN \nORDER BY ID\nLIMIT 10 OFFSET 20\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/82/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-04-08%20%EC%98%A4%ED%9B%84%202.54.59.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLIMIT을 지정하면 해당 값만큼의 결과만 출력되며 총 결과 값이 더 적을 경우 있는 만큼만 출력한다.\u003c/li\u003e\n\u003cli\u003eLIMIT ALL, LIMIT NULL은 LIMIT을 설정하지 않은 것 (전체 ROWS 리턴)과 동일하다.\u003c/li\u003e\n\u003cli\u003eOFFSET은 리턴되는 ROWS의 시작점을 지정한다.\u003c/li\u003e\n\u003cli\u003eOFFSET 0, OFFSET NULL은 OFFSET을 설정하지 않은 것 (첫 ROW부터 리턴)과 동일하다.\u003c/li\u003e\n\u003cli\u003eOFFSET과 LIMIT을 둘 다 사용할 경우, LIMIT 카운트를 세기 전에 OFFSET만큼의 ROWS가  앞에서 생략된다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"주의점\" ke-size=\"size26\"\u003e2. 주의점\u003c/h2\u003e\n\u003cp\u003eLIMIT을 사용할 경우 결과 ROWS의 순서를 유니크하게 만들어주는 ORDER BY와 함께 쓰는 것이 중요하다. 그게 아니라면 예상하지 못한 부분 집합을 조회하게 될 수 있다. 앞의 예시 쿼리에서 ORDER BY를 뺀다면, 10~20 번째를 가져오려고 해도, 어떤 순서에서의 10~20 번째인지 알 수 없다.\u003c/p\u003e","title":"[PostgreSQL] 페이징 : OFFSET과 LIMIT의 올바른 사용과 주의점"},{"content":" 1. 개념 자바 라이브러리에는 close 메서드를 직접 호출해서 닫아줘야 하는 자원이 많다. 대표적으로 InputStream, ouputStream java.sql.connection 등이 있으며 해당 자원들은 클라이언트에서 놓치기 쉬워 예측할 수 없는 성능 문제로 이어지곤 한다. 이중 상당 수가 finalizer를 안정망으로 사용하여 문제에 대비하고 있긴 하지만, 완전히 안전하다고 할 수 없다. (해당 내용은 다음 포스트에서 확인 가능)\n[이펙티브 자바] - [이펙티브 자바] 8. finalizer와 cleaner 사용을 피하라\n흔히 사용하는 try-finally를 사용한 예외처리를 확인해보자\n2. try-finally 2-1. 자원을 1개 사용하는 try-finally 메서드 static String firstLineOfFile(String path) throws IOException { BufferdReader br = new BufferedReader(new FileReader(Path)); try { return br.readLine(); } finally { br.close(); } } 2-2. 자원을 2개 사용하는 try-finally 메서드 자원을 1개 사용하는 경우는 꽤나 괜찮아 보인다, 그렇다면 자원을 여러 개 쓰는 경우 어떻게 될까?\nstatic void copy(String src, String dst) throws IOException { InputStream in = new FileInputStream(src); try { OutputStream out = new FileOutputStream(dst); try { byte[] buf = new byte[BUFFER_SIZE]; int n; while ((n = in.read(buf)) \u0026gt;= 0) out.write(buf, 0, n); } finally { out.close(); } } finally { in.close(); } } 예제에서 볼 수 있듯, 2개 이상의 자원을 try-finally로 구현하면 너무 지저분해진다. 두 메서드 모두 올바르게 try-finally를 사용했지만 미묘한 결점이 있다. 바로 특정 예외가 숨겨질 수 있다는 점이다. 2-3. try-finally의 결점 해당 메서드들은 try, finally 구문에서 모두 예외가 발생할 수 있다. 기기에 물리적 문제가 생기면 firstLineOfFile 메서드 안에 readLine 메서드가 예외를 던질 것이고, 그 이후 close 메서드도 실패하게 된다. 이 경우 두 번째 예외(close)가 첫 번째 예외(readLine)를 집어삼킨다. 그러면 stack traces에 첫 번째 예외는 정보가 남지않아서 실제 시스템에서 디버깅을 어렵게한다. 실제 운영 상황에서 문제를 해결하기 위해서는 첫번째 예외에 대한 로그를 보고 싶은 경우가 분명 있을 것이고, 두 번째 예외 대신 첫 번째 예외를 찍도록 코드를 수정할 수는 있지만, 코드가 너무 지저분해져서 그렇게 사용하는 경우는 거의 없다.\n3. try-with-resources 이런 문제는 자바 7에서 try-with-resources로 모두 해결된다. 이 구조를 사용하면 해당 자원이 AutoCloseable인터페이스를 구현해야 한다. AutoCloseable 인터페이스를 확인해 보면 다음과 같이 close() 메서드만으로 이루어진 인터페이스이다.\n구현된 AutoClosebale (정의된 close 메서드는) 리소스를 닫을 때 호출되며 명시적으로 닫아줄 필요 없이 자동으로 자원을 닫히게 해 준다.\ntry (ResourceType resource = acquireResource()) { // 리소스를 사용하는 코드 } 예를 들어 해당 코드에서 ResourceType은 AutoCloseable을 구현한 클래스이고, try 블록이 끝나면 자동으로 close() 메서드가 호출되며 리소스가 안전하게 닫힌다. 예외가 발생하더라도 리소스가 정확히 닫힘을 보장할 수 있다. 다양한 서드파티 라이브러리들도 AutoCloseable 인터페이스를 구현/확장하고 있어서 이 기능을 사용할 수 있다. 효율적인 자원 관리를 위해 닫아야 하는 자원을 대상으로 한다면 AutoCloseable을 꼭 구현해야 한다. 이제 위의 2개 메서드에 각각 try-with-resource를 적용해 보자\n3-1. 자원을 1개 사용하는 try-with-resource 메서드 static String firstLineOfFile(String path) throws IOException { try (BufferedReader br = new BufferedReader( new FileReader(path))) { return br.readLine(); } } 3-2. 자원을 2개 사용하는 try-with-resource 메서드 static void copy(String src, String dst) throws IOException { try (InputStream in = new FileInputStream(src); OutputStream out = new FileOutputStream(dst)) { byte[] buf = new byte[BUFFER_SIZE]; int n ; while ((n = in.read(buf)) \u0026gt;= 0) out.write(buf, 0, n); } } 한눈에 봐도 가독성이 더 좋으며, 문제를 진단하기도 좋다. firstLineOfFiles 메서드를 먼저 확인해 보면, readLine, close에서 둘 다 에러가 발생할 시 기존의 try-finally 메서드와 다르게 숨겨진 예외(readLine)들도 버려지지 않고 stack trace에 숨겨졌다는(suppressed) 꼬리표를 달고 출력된다. 또한 자바 7의 Throwable에 추가된 getSuppressed 메서드를 사용하면 코드상에서도 쓸 수 있다.\n3-3. try-with-resource와 catch 메서드 보통 try-finally처럼 try-with-resources에서도 catch절을 쓸 수 있다. catch을 함께 사용하면 try 문을 더 중첩하지 않고 다수의 예외처리가 가능하다. firstLineOfFile 메서드를 수정하여 예외가 발생했을 때 예외를 던 지는 대신 기본값을 반환하도록 수정하면 다음과 같다.\nstatic String firstLineOfFile(String path, String defaultVal) { try (BufferedReader br = new BufferedReader( new FileReader(path))) { return br.readLine(); } catch (IOException e) { return defaultVal; } } 4. 정리 꼭 회수해야 하는 자원을 다룰 때는 try-finally 가 아닌, try-with-resources를 사용하자. 예외적인 경우는 없으며 코드는 간결하고 분명해지고 예외정보를 추적하기에도 훨씬 좋다. 내용 정리 및 테스트 코드\nhttps://github.com/junhkang/effective-java-summary/tree/master/src/main/java/org/example/ch01/item08\nhttps://github.com/junhkang/effective-java-summary/tree/master/src/main/java/org/example/ch01/item09\n","permalink":"http://localhost:50666/posts/81/","summary":"\u003chr\u003e\n\u003ch2 id=\"개념\" ke-size=\"size26\"\u003e1. 개념\u003c/h2\u003e\n\u003cp\u003e자바 라이브러리에는 close 메서드를 직접 호출해서 닫아줘야 하는 자원이 많다. 대표적으로 InputStream, ouputStream java.sql.connection 등이 있으며 해당 자원들은 클라이언트에서 놓치기 쉬워 예측할 수 없는 성능 문제로 이어지곤 한다. 이중 상당 수가 finalizer를 안정망으로 사용하여 문제에 대비하고 있긴 하지만, 완전히 안전하다고 할 수 없다. (해당 내용은 다음 포스트에서 확인 가능)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://junhkang.tistory.com/80\"\u003e[이펙티브 자바] - [이펙티브 자바] 8. finalizer와 cleaner 사용을 피하라\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e흔히 사용하는 try-finally를 사용한 예외처리를 확인해보자\u003c/p\u003e\n\u003ch2 id=\"try-finally\" ke-size=\"size26\"\u003e2. try-finally\u003c/h2\u003e\n\u003ch3 id=\"자원을-1개-사용하는-try-finally-메서드\" ke-size=\"size23\"\u003e2-1. 자원을 1개 사용하는 try-finally 메서드\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003estatic String firstLineOfFile(String path) throws IOException    {\n       BufferdReader br = new BufferedReader(new FileReader(Path));\n       try {\n           return br.readLine();\n       }    finally {\n           br.close();\n       }\n   }\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e \u003c/p\u003e","title":"[이펙티브 자바] 9. try-finally 보다는 try-with-resources를 사용하라"},{"content":" 1. finalizer와 cleaner란? finalizer와 cleaner는 자바의 2가지 객체 소멸자이다. finalizer는 Object.finalize() 메서드를 오버라이딩 함으로써 사용된다. 작동 여부 및 시점을 예측할 수 없고 상황에 따라 위험할 수 있어 일반적으로 불필요하며, 기능의 잘못된 동작, 낮은 성능, 이식성 문제의 원인이 되기도 한다. 기본적으로는 사용하면 안 되고, 자바 9에서는 finalizer를 deprecated API로 지정하고, java.lang.ref.Cleaner 클래스를 사용하여 구현된 cleaner를 대안으로 제시하였으나, cleaner 또한 finalizer보다는 덜 위험 하지만 여전히 예측불가하고 성능이 좋지 않아 일반적으로 불필요하다. 2. 사용 시 문제점 2-1. 예측불가능한 실행시점 finalizer, cleaner는 즉시 실행된다는 보장이 없다. 객체에 접근할 수 없게 된 후 언제 실행될지 알 수 없다는 뜻이며, 특정시점에 실행을 요하는 작업에는 사용이 불가능하다. 예를 들어 파일 닫기를 기능을 맡기면, 제때 안 닫히기 때문에 시스템이 열 수 있는 파일 개수에는 한계가 있고 큰 문제로 이어질 수 있다. finazlier, cleaner가 언제 수행될지는 전적으로 가비치 콜렉터 알고리즘에 달려있고, 가비지 콜렉터 구현마다 다르기에 finalizer, cleaner 수행시점에 의존되는 모든 프로그램의 동작 또한 예측할 수 없다. 이는 두 번째 문제인 \u0026quot;이식성 문제\u0026quot;로도 이어진다.\n2-2. 이식성 문제 가비지 컬렉션의 작동 방식은 JVM의 구현에 따라 다르기 때문에, 서로 다른 JVM에서는 Finalizer의 동작이 달라질 수 있다. 그러므로 테스트한 JVM에서는 잘 작동해도 운영시점의 시스템에서는 잘 작동 안 할 수가 있다.\n2-3. 성능 저하 클래스에 finalizer를 쓰면, 그 인스턴스의 자원회수가 멋대로 지연될 수 있다. finalizer는 객체가 가비지 컬렉션에 의해 처리될 때 실행되지만, 가비지 컬렉터의 실행시기는 JVM에 따라 달라지며, 프로그램의 실행흐름을 예측하기 힘들다. 필요한 자원이 즉시 실행되지 않고 예상 못한 시점까지 남아있을 수 있기에 메모리 누수 같은 문제로 이어질 수 있다. 또한 finalizer가 실행되는 과정에서는 추가적인 처리가 필요하기에 가비지 컬렉터는 finalizable 객체를 별도로 관리해야 하며, 이 객체들은 일반적인 가비지 컬렉션 프로세스보다 더 복잡한 과정을 거친다. 따라서 전반적인 가비지 컬렉션 성능에 영향을 미치며, 애플리케이션 응답시간에도 영향을 미치게 된다. 2-4. finalize 스레드의 우선순위 finalizer 스레드는 다른 애플리케이션 스레드보다 우선순위가 낮아서 실행이 안된 채 대기만 하는 상황이 발생할 수도 있으며, 자바 언어 명세는 어떤 스레드 어떤 순서대로가 finalizer를 수행할지 명시하지 않고 있어 명확한 해결방법이 없다. cleaner는 자신이 수행할 스레드를 제어할 수 있다는 면에서 조금 낫지만 여전히 백그라운드에서 수행되며 가비지 컬렉터의 통제하에 있으니 즉각 수행을 보장할 수 없다는 점은 동일하다.\n2-5. 수행 여부의 불확실성 finalizer와 cleaner는 실행 시점뿐 아니라 수행 여부도 보장하지 않는다. 접근할 수 없는 일부 객체에 포함된 종료 작업을 수행하지 못한 채로 프로그램이 중단될 수 있다는 뜻이다. JVM이 종료 과정에서 남아있는 모든 finalizer 스레드를 실행하려고 시도하지만, System.exit(), 운영체제의 강제 종료 등 시스템이 갑자기 종료되면 실행을 보장할 수 없다. 그래서 프로그램의 생애주기와 상관없는 중요한 자원이나, 상태를 영구적으로 수정하는 작업에서는 절대 Finalizer / Cleaner에 의존하면 안 된다. 데이터베이스 같은 공유자원의 영구락을 해제하는 작업을 Finalizer와 Cleaner에 맡겨놓으면 분산시스템 전체가 서서히 멈추게 될 것이다. 가비지 컬렉션과 Finalization을 유도할 수 있는 System.gc, System.runFinalization 메서드가 있지만 실행 가능성을 높여줄 뿐, 실행을 보장하진 않기에 자원 해제와 관련된 중요한 작업은 명시적으로 관리하고, finalizer, cleaner에 의존하지 않는 방향으로 설계하는 것이 좋다. [자원의 회수를 완전히 보장해 주는 기능이 존재하기는 하지만 사용이 권장되지는 않는다. ]{style=\u0026ldquo;color: #333333; text-align: start;\u0026rdquo;}System.runFinalizersOnExit, Runtime.runFinalizersOnExit를 사용하면 자바 프로그램이 종료될 때 모든 객체에 대해 finalizer를 실행하도록 강제하는 기능을 제공한다. 이론적으로는 프로그램 종료 시 모든 리소스가 정리되도록 보장하는 듯 보이지만, 이 메서드들은 사용이 권장되지 않는다. 이 메서드들은 내부적으로 Thread.stop을 사용할 수 있는데, 이 경우 스레드를 강제로 종료시키지만, 락이 비정상적으로 해제되거나 중요한 데이터가 변경될 위험이 있다.\n2-6. 예외처리 finalizer도중 예외가 발생하면 이후 작업이 남아도 그 순간 종료된다. 처리 안된 예외 때문에 해당 객체는 마무리가 덜 된 상태로 남을 수 있다. 다른 스레드가 이처럼 훼손된 객체를 사용하려고 하면 어떻게 동작할지 예측불가하다. 보통 미처리 예외가 스레드를 중단하면, 스택 트레이스 내역을 출력하지만, finalizer에서 발생하면 경고조차 출력 안 하지 않는다. (Cleaner를 사용하는 라이브러리는 자신의 스레드를 통제하기 때문에 이런 문제는 발생하지 않는다)\n2-7. 보안문제 finalizer 공격에 노출되어 심각한 보안문제도 일으킬 수 있다. 공격자는 악의적인 목적으로 설계된 하위 클래스를 생성하고 finalizer를 오버라이드 하여 악의적인 코드를 포함할 수 있다. 생성자나 직렬화 과정 (readObject, readResolve)에서 예외가 발생하면 이 생성되다만 객체에서 악의적인 하위클래스의 finalizer가 수행될 수 있게 한다. 원래 생성되다가 만 객체는 가비지 컬렉션 대상이 되지만 finalizer가 아직 실행되지 않았기에, 가비지 컬렉션에 의해 수집되기 전에 finalizer가 실행된다. 이때 하위 클래스의 finalizer 내부에서 실행되는 코드는 정적 필드에 자신의 참조를 할당하여 가비지 컬렉터가 수집하지 못하게 할 수 있다. 이렇게 \u0026quot;살아남은\u0026quot; 객체가 만들어지고 원래 의도하지 않았던 여러 동작을 수행하게 할 수 있다. 예를 들어 보안이 강화된 영역에서의 작업수행, 민감정보 노출, 시스템 리소스 남용 등이 가능하다. 3. 대안 객체 생성을 막으려면 생성자에서 예외를 던지는 것만으로 충분하지만 finalizer가 있다면 그렇지 않다. finalizer가 존재하는 경우 객체 생성 중 예외가 발생해도, 가비지 컬렉션 대상이 되기 전에 finalizer가 실행될 수 있기 때문이다. final 클래스들은 그 누구도 하위클래스를 만들 수 없으니 (상속할 수 없으니) 이 공격에서 자연스럽게 보호된다. final이 아닌 크래스를 finalizer공격으로부터 방어하려면 아무 일도 하지 않는 finalizer메서드를 만들고 final로 선언하면 된다.\nprotected final void finalize() throws Throwable { // 아무 작업도 수행하지 않음 } 이렇게 선언하면 클래스가 final로 선언되지 않아 상속이 가능한 경우에도, 하위 클래스에서 finalize() 메서드를 오버라이드 할 수 없게 만들어 공격으로부터 보호될 수 있다,. 그렇다면 파일이나 스레드 등 종료해야 할 자원을 담고 있는 객체의 클래스에서 finalizer / cleaner의 대안은 무엇일까? AutoCloseable을 구현하여, 클라이언트에서 인스턴스를 다 쓰면 close를 호출하는 방식을 사용하면 된다. AutoCloseable 인터페이스는 단 하나의 메서드 'close()'를 선언하고, 이 인터페이스를 구현하는 클래스는 close() 메서드 내에서 필요한 자원 정리 작업 (파일 닫기, 네트워크 연결해제 등)을 수행한다. 각 인스턴스는 자신이 닫혔는지를 추적하는 것이 좋다. close 메서드에서 이 객체는 더 이상 유효하지 않음을 필드에 기록하고 다른 메서드는 이 필드를 검사해서 객체가 닫힌 후에 호출했다면 IllegalstateException을 던짐으로써 이미 닫힌 자원을 잘못 사용하는 것을 방지할 수 있다. public class Resource implements AutoCloseable { private boolean closed = false; public void doSomething() { if (closed) { throw new IllegalStateException(\u0026#34;Resource is closed\u0026#34;); } } @Override public void close() { closed = true; // 그 외 자원 정리 작업 수행, 예: 파일 닫기, 네트워크 연결 해제 } } 4. 적절한 사용 그러면 이렇게 단점이 많은 Finalizer와 Cleaner는 왜 존재하는걸가? 물론 여러 단점과 함께 사용될 때 주의가 필요하지만, 특정상황에서는 유용할 수도 있다.\n4-1. 안정망 자원소유자가 close메서드를 호출하지 않는 것에 대비한 안정망 역할을 한다. 즉 클라이언트 코드가 자원회수를 제대로 하지 않았을 때, 자원이 늦게라도 해제될 수 있도록 한다. 완벽한 해결책은 아니지만 자원 누수를 일부 방지할 수 있는 마지막 수단이 될 수 있다. 다만 안정망 역할의 finalizer와 cleaner를 구현할 때는 그만한 가치가 있는지 자원의 중요성, 자원의 해제 필요성, 프로그램의 전반적인 안정성 들을 충분히 고려 후 사용해야 한다. 자바 라이브러리의 일부 클래스는 사용자의 실수를 대비하여 안정망 역할의 finalizer를 제공한다. FileInputStream, FileOutputStream, ThreadPoolExecutor 등의 라이브러리는 내부적으로 자원을 회수하는 로직을 포함하고 있어, 이 객체들이 사용되지 않고 가비지 컬렉션 대상이 될 때 자원을 정리할 수 있다. 4-2. 네이티브 피어 자원의 관리 네이티브 페어(native pear)와 연결된 객체에 대한 자원관리를 해준다. 일반 자바 객체에서 네이티브 메서드를 통해 기능을 위임한 네이티브 객체를 말하며 네이티브 피어는 자바 객체가 아니니 GC가 존재를 알 수 없다. 네이티브 피워와 연결된 자원은 자바 플랫폼의 자동 메모리관리 영역 밖에 있으므로 finalizer / cleaner 가 나서서 처리하기에 적당한 작업이다. 다만 성능저하를 감당할 수 있고, 네이티브 피어가 중요한 자원을 가지고 있지 않을 때만 해당된다. 네이티브 피어가 중요한 시스템 자원을 많이 사용하거나 대용량 메모리할당, 파일, 네티워크 연결 등 자원 해제를 신속하게 수행해야 하는 경우에는 앞서 말한 close를 사용해야 한다.\n4-3. Cleaner의 사용 예 Cleaner는 사용하기 좀 더 까다롭다. Cleaner를 안정망으로 사용하는 AutoCloseable클래스인 Room 클래스를 살펴보자\npublic class Room implements AutoClosable { private static final Cleaner cleaner = Cleaner.create(); private static class State implements Runnable { int numJunkPiles; State(int numJunkPiles) { this.numJunkPiles = numJunkPiles } @Override public void run() { System.out.println(\u0026#34;방 청소\u0026#34;); numJunkPiles = 0; } } private final State state; private final Cleaner.Cleanable cleanable; public Room(int numJunkPiles) { state = new State(numJunkPiles); cleanable = cleaner.register(this, state); } @Override public void close() { cleanable.clean(); } } Cleaner를 사용하여 객체와 연결된 정리작업을 Cleaner에 등록할 수 있고, 객체가 가비지 컬렉션 되어 메모리에서 제거될 때 해당 작업이 실행된다. 또한 공개 API에 노출되지 않기에 정리작업을 내부적으로만 관리할 수 있고 직접 호출할 수 없다. static으로 선언된 중첩클래스인 State는 cleaner가 방을 청소할 때 수거할 자원들(자바 객체에 의해 관리되어야 하는 자원)을 담고 있다. 예제에서는 방안의 쓰레기 수를 뜻하는 numJunkPiles 필드가 수거할 자원에 해당하지만 실제 활용에서는, 이 필드는 네이티브 피어와 같은 더 복잡한 자원을 관리하는 데 사용될 수 있기에, 가리키는 포인터를 담은 final long 변수로 대체될 수도 있다. State는 Runnable 인터페이스를 구현하고 Cleaner에 의해 자원이 해제될 때 수행될 정리 작업들을 정의한다. 이 안의 run() 메서드는 실제 자원 해제 로직이 구현(예제에서는 쓰레기 수를 0으로 설정, 방청소 문구를 print 하는 작업)되며 cleanable에 의해 한번만 호출된다.\nRoom 객체는 생성자에서 Cleaner를 사용하여 Room자신과 State 객체를 등록하고, 이 과정에서 Cleaner는 Cleanable 인스턴스를 생성한다. run 메서드가 호출되는 2가지 상황을 살펴보자\nclose 메서드의 명시적 호출 - 보통 Room의 close 메서드를 호출할 때. close 메서드에서 Cleanable의 clean을 호출하면 이 메서드 안에 run을 호출한다. 가비지 컬렉터에 의한 자동 호출 - 가비지 컬렉터가 Room을 회수할 때까지 클라이언트가 close를 호출 안 하면, cleaner가 State의 run 메서드를 호출해 준다. (즉시 실행을 보장하지 않는다.) State인스턴스는 절대 Room인스턴스를 참조하면 안 된다. Room 인스턴스를 참조할 경우 순환참조가 생겨 가비지 컬렉터가 Room 인스턴스를 회수할 수 없다. (State가 정적 중첩 클래스인 이유이다.) 정적이 아닌 중첩 클래스는 자동으로 바깥 객체의 참조를 갖게 된다. 이와 비슷하게 람다 역시 바깥 객체의 참조를 갖기 쉬우니 사용하지 않는 것이 좋다.\n5. 올바른 사용 예제 앞서 이야기한 대로 Room의 cleaner는 단지 안정망으로만 쓰였다. 클라이언트가 모든 Room생성을 try with resources블록으로 감쌌다면 자동 청소는 전혀 필요하지 않다. 다음 잘 짜인 클라이언트 코드의 예를 확인해 보자\npublic static void main(String[] args) { try (Room myRoom = new Room(7)) { System.out.println(\u0026#34;안녕~\u0026#34;); } } Room 객체는 try-with-resources 문내에서 생성된다. Room 객체가 AutoCloseable 인터페이스를 구현하고 있기 때문에 가능하며, try 블록이 종료된 후 \u0026quot;안녕~\u0026quot;을 출력한 후 Room 객체의 close() 메서드가 자동으로 호출된다. 따라서 \u0026quot;안녕\u0026quot;을 출력한 후 \u0026quot;방 청소\u0026quot;가 확정적으로 출력된다. 반면에 아래 코드를 확인해 보면\npublic static void main(String[] args) { new Room(99); System.out.println(\u0026#34;아무렴\u0026#34;); } 이 코드는 Room 객체가 생성된 후 어떠한 변수도 할당되지 않고, close() 메서드가 명시적으로 호출되지 않는다. 이경우 Room 객체는 가비지 컬렉션 대상이 되며 Cleaner에 의해 정리될 수 있다. 하지만 Cleaner가 실제로 언제 작업을 수행할지는 가비지의 실행 타이밍에 의존되어 있기 때문에 \u0026quot;아무렴\u0026quot; 후에 \u0026quot;방청소\u0026quot;의 출력 시간을 확정할 수 없으며, JVM에 따라 작업 자체가 이루어지지 않을 수 있다. (특히 System.exit()'을 통해 종료될 경우)\njava.lang.ref.Cleaner명세의 System.exit 종료시에 대한 설명\n6. 정리 Cleaner는 안정망 역할이나 중요하지 않은 네이티브 자원회수용으로만 사용하고, 이 경우에도 불확실성과 성능저하를 주의해서 사용해야 한다. 자원의 해제가 명확히 필요한 경우 AutoCloseable 인터페이스를 구현하여 사용하고 try-with-resources의 사용을 통해 클라이언트 코드에서 자원을 적극적으로 관리해 주도록 하자. 내용 정리 및 테스트 코드\nhttps://github.com/junhkang/effective-java-summary/tree/master/src/main/java/org/example/ch01/item08\n","permalink":"http://localhost:50666/posts/80/","summary":"\u003chr\u003e\n\u003ch2 id=\"finalizer와-cleaner란\" ke-size=\"size26\"\u003e1. finalizer와 cleaner란?\u003c/h2\u003e\n\u003cp\u003efinalizer와 cleaner는 자바의 2가지 객체 소멸자이다. finalizer는 \u003cstrong\u003eObject.finalize()\u003c/strong\u003e 메서드를 오버라이딩 함으로써 사용된다. 작동 여부 및 시점을 예측할 수 없고 상황에 따라 위험할 수 있어 일반적으로 불필요하며, 기능의 잘못된 동작, 낮은 성능, 이식성 문제의 원인이 되기도 한다. 기본적으로는 사용하면 안 되고, 자바 9에서는 finalizer를 deprecated API로 지정하고, \u003cstrong\u003ejava.lang.ref.Cleaner\u003c/strong\u003e 클래스를 사용하여 구현된 cleaner를 대안으로 제시하였으나, cleaner 또한 finalizer보다는 덜 위험 하지만 여전히 예측불가하고 성능이 좋지 않아 일반적으로 불필요하다. \u003c/p\u003e","title":"[이펙티브 자바] 8. finalizer 와 cleaner 사용을 피하라"},{"content":" 1. Visibility Map(가시성 맵)란? Visibility Map은 트랜잭션에서 데이터에 접근할 때 어떤 데이터가 가시적인지(모든 트랜잭션에서 읽을 수 있는지), 안정적인지 (동결된 튜플인지) 판별하는데 도움을 준다. 데이터 접근 시 불필요한 I/O작업을 줄여주고, 데이터베이스가 어떤 페이지를 직접 접근할 수 있는지를 빠르게 판단함으로써 시스템의 효율적을 올려주는 역할을 한다.\n2. Visibility Map(가시성 맵)의 데이터 관리 Visibility Map\b은 데이터를 주요 데이터와는 별도의 파일(fork)에 _vm 접미사를 붙여 관리한다. 예를 들어 예를 들어 employees 테이블이 있다고 하면 테이블의 Visibility Map\b은 별도의 포크에 저장된다. 이 포크의 이름은 파일 노드 번호에 _vm 접미사를 붙여 구성되며, 예를 들어 파일 노드번호가 12345인 경우 VM 파일은 12345_vm으로 저장된다. 데이터에는 해당 테이블의 page가 모든 트랜잭션에 보이는지, 동결된 튜플만을 포함하는지 등의 정보를 저장한다. 데이터베이스가 employees 테이블을 조회할 때, 가시성 맵을 먼저 확인한다. 만약 쿼리가 접근하려는 pages가 모든 트랜잭션에게 보이는 상태라고 확인되면, 시스템은 데이터에 더 빠르게 접근한다. 불필요한 버전검사나 락을 안 해도 되기에 성능이 향상된다.\n3. Visibility Map(가시성 맵)의 원리 Visiblity Map은 힙 pages당 2개의 비트를 별도로 저장한다. 첫 번째 비트가 설정되어 있으면, 해당 페이지가 모두 visible(가시적) 한 상태이고, 이는 vacuum이 필요한 튜플을 포함하지 않는다는 뜻이다. 이는 인덱스 영역의 tuple만을 사용하여 index-only-scan으로 쿼리를 조회할 때도 사용된다. index-only-scan은 해당 포스트에서 확인 가능하다.\n[PostgreSQL] Index-Only 스캔과 Covering 인덱스, Index-only스캔의 효율적인 사용\n두 번째 bit가 설정되어 있다면 모든 pages의 튜플이 frozen(동결된) 상태라는 뜻이다. 이 상태에선 일반적인 vacuum은 물론 anti-wraparound vacuum도 동작시킬 필요가 없다. anti-wraparound-vacuum - 전체 데이터베이스를 검사하여 트랜잭션 ID가 안전한 범위 내에 있는지 확인하여, 필요에 따라 조정하며 트랜잭션 ID의 오버플로우를 방지한다. 트랜잭션 ID에 대한 상세 내용은 해당 포스트에서 확인 가능하다.\n[PostgreSQL] 트랜잭션(Transaction)의 작동원리\nVisiblity Map의 2가지 비트는 최대한 보수적으로 해석된다. 1, 2 번째 비트가 설정되어 있을 경우에는 무조건 참이지만, 비트가 설정되지 않을 경우에는 참 일수도 거짓일 수도 있다.\n4. Visibility Map(가시성 맵)의 생명주기 Visiblity Map의 비트는 vacuum에 의해서만 설정된다. 데이터베이스 내의 pages에 vacuum 작업이 수행되면 관련 Visiblity Map의 비트가 설정이 되고, 해당 pages가 모든 트랜잭션에서 완전히 가시적임을 표시하며, 더 이상 vacuum 안 해도 됨을 나타낸다. 그 후에 pages의 데이터가 하나라도 수정(update, insert, delete 등) 될 경우, VM의 비트는 초기화된다. 데이터의 상태가 변경되었기에 vacuum 작업 대상에 포함시켜야 함을 나타낸다.\n5. Visibility Map(가시성 맵) 정보 확인 pg_visibility 함수를 사용해서 vm에 저장된 정보를 확인할 수 있다.\npg_visibility_map(relation regclass, blkno bigint, all_visible OUT boolean, all_frozen OUT boolean) returns record- 해당 테이블, 해당 블록의 모든 VM의 visible, frozen 비트 조회\npg_visibility(relation regclass, blkno bigint, all_visible OUT boolean, all_frozen OUT boolean, pd_all_visible OUT boolean) returns record - 해당 테이블, 해당 블록의 모든 VM의 visible, frozen 비트 조회 + PD_ALL_VISIBLE 비트 pg_visibility_map(relation regclass, blkno OUT bigint, all_visible OUT boolean, all_frozen OUT boolean) returns setof record - 해당 테이블의 모든 블록의 VM의 visible, frozen 비트 조회\npg_visibility(relation regclass, blkno OUT bigint, all_visible OUT boolean, all_frozen OUT boolean, pd_all_visible OUT boolean) returns setof record - 해당 테이블의 모든 블록의 VM의 visible, frozen 비트 조회 + PD_ALL_VISIBLE 비트\npg_visibility_map_summary(relation regclass, all_visible OUT bigint, all_frozen OUT bigint) returns record - VM에 연관 있는 테이블의 visible 페이지 수량, frozen 페이지 수량 확인\npg_check_frozen(relation regclass, t_ctid OUT tid) returns setof tid - VM에 frozen으로 마킹되어 있는 pages 중 non-frozen 튜플의 TID, 존재해서는 안 되는 경우로, 뭔가 조회가 된다면 VM에 문제가 있는 것\npg_check_visible(relation regclass, t_ctid OUT tid) returns setof tid - VM에 visible으로 마킹되어 있는 pages 중 non-visible 튜플의 TID, 존재해서는 안 되는 경우로, 뭔가 조회가 된다면 VM에 문제가 있는 것\npg_truncate_visibility_map(relation regclass) returns void - 해당 테이블의 VM을 truncate 한다. VM에 문제가 있는 경우 강제로 재설정이 필요할 때 사용. 해당 테이블의 첫 번째 vacuum이 실행될 때 재생성되며, 그전까지는 모든 VM이 모두 0 값으로 유지 참고\nhttps://www.postgresql.org/docs/16/storage-vm.html\nhttps://www.postgresql.org/docs/16/pgvisibility.html\n","permalink":"http://localhost:50666/posts/79/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/79/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"visibility-map가시성-맵란\" ke-size=\"size26\"\u003e1. Visibility Map(가시성 맵)란?\u003c/h2\u003e\n\u003cp\u003eVisibility Map은 트랜잭션에서 데이터에 접근할 때 어떤 데이터가 가시적인지(\u003cstrong\u003e모든 트랜잭션에서 읽을 수 있는지\u003c/strong\u003e), 안정적인지 (\u003cstrong\u003e동결된 튜플인지\u003c/strong\u003e) 판별하는데 도움을 준다. 데이터 접근 시 불필요한 I/O작업을 줄여주고, 데이터베이스가 어떤 페이지를 직접 접근할 수 있는지를 빠르게 판단함으로써 시스템의 효율적을 올려주는 역할을 한다.\u003c/p\u003e\n\u003ch2 id=\"visibility-map가시성-맵의-데이터-관리\" ke-size=\"size26\"\u003e2. Visibility Map(가시성 맵)의 데이터 관리\u003c/h2\u003e\n\u003cp\u003eVisibility Map\b은 데이터를 주요 데이터와는 별도의 파일(fork)에 _vm 접미사를 붙여 관리한다. 예를 들어 예를 들어 employees 테이블이 있다고 하면 테이블의 Visibility Map\b은 별도의 포크에 저장된다. 이 포크의 이름은 파일 노드 번호에 _vm 접미사를 붙여 구성되며, 예를 들어 파일 노드번호가 12345인 경우 VM 파일은 12345_vm으로 저장된다. 데이터에는 해당 테이블의 page가 모든 트랜잭션에 보이는지, 동결된 튜플만을 포함하는지 등의 정보를 저장한다. 데이터베이스가 employees 테이블을 조회할 때, 가시성 맵을 먼저 확인한다. 만약 쿼리가 접근하려는 pages가 모든 트랜잭션에게 보이는 상태라고 확인되면, 시스템은 데이터에 더 빠르게 접근한다. 불필요한 버전검사나 락을 안 해도 되기에 성능이 향상된다.\u003c/p\u003e","title":"[PostgreSQL] Visibility Map(가시성 맵)의 개념, 원리, 생명주기 및 정보 확인 방법"},{"content":" 1. 메모리 관리 자바에선 가비지 컬렉터가 다 쓴 객체를 알아서 회수해 가기에 편리하고 효율적으로 메모리를 관리할 수 있다. 하지만 메모리 관리에 신경 쓰지 않아도 된다는 말은 절대 아니다. 메모리를 적절하게 관리하지 못하면 메모리 누수가 발생하고 심하면 프로그램이 종료될 수 있다. 메모리를 적절하게 관리하지 못하는 경우의 예제를 살펴보자. 다음은 스택을 간단하게 구현한 자바 코드이다.\npublic class Stack { private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { elements = new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public Object pop() { if (size == 0) throw new EmptyStackException(); return elements[--size]; } private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); } 이대로 사용하여도 기능상으로는 전혀 문제가 없을 것이고, 어떤 테스트도 훌륭하게 통과하겠지만, 이 스택을 사용하는 프로그램을 오래 실행시킬 경우 가비지 컬렉션과 메모리 사용량이 늘어나 결국 성능이 저하될 것이다. 정확히 문제가 되는 부분은 pop 메서드이다. 스택이 커졌다가 줄어들었을때, 스택에서 꺼내진 객체들을 가비지 컬렉터가 회수하지 않는다. 사용하지 않는 객체라고 하더라도 스택이 객체들의 다 쓴(앞으로 사용되지 않을) 참조를 여전히 가지고 있기 때문이다. 이처럼 가비지 컬렉션을 지원하는 언어에서 메모리 누수를 찾기는 아주 까다롭다. 객체 참조 하나를 살려두면 가비지 컬렉터는 그 객체뿐 아니라 그 객체가 참조하는 모든 객체 (줄줄이 참조하는 객체들)를 모두 회수하지 못한다. 그렇기에 몇 개의 객체라도, 물려있는 많은 객체들을 회수하지 못하게 할 수 있고 성능에 잠재적으로 안 좋은 영향을 줄 수 있다.\n2. 객체 참조 해제 객체의 참조를 해제하는 방법은 간단하다. 해당 참조를 다썼을때 null 처리를 하여 참조해제를 하면 된다. 위 예제의 스택에서 각 원소의 참조가 더 이상 필요 없어지는 지점은 스택에서 꺼내질 때(pop)이다. 사용이 종료될 때 참조 해제를 추가한 customPop메서드를 확인해 보자.\npublic Object customPop() { if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; return result; } 다음과 같이 다시 사용될 일 없는 참조를 null 처리하면 되고, 이 경우 실수로 사용하게 경우도 NullPointException으로 사전에 핸들링 가능하다. 다만 책에서는 모든 미사용 객체를 찾아서 Null 처리하는 것은 필요 없고 바람직하지 않다고 설명한다, 성능대비 프로그램을 필요이상으로 지저분하게 할 뿐이라 예외적인 경우에만 Null 처리로 객체의 참조를 해제하는 것을 권장한다. 책에서 설명하는 다 쓴 참조를 해제하는 가장 좋은 방법은 그 참조를 담은 변수를 범위(scope) 밖으로 밀어내는 것이다. 만약 변수의 범위를 최소가 되게 정의했다면 자연스럽게 객체가 참조해제 처리될 것이다. (Scope를 조절한다는 것은 변수가 선언된 블록(메서드, 조건문, 반복문 등)을 벗어나게 하면서 자동으로 가비지컬렉션의 대상이 되게 하는 것을 의미한다.)\npublic void scope() { { // SCOPE A 시작 int a = 10; // \u0026#39;a\u0026#39; 변수는 이 블록 안에서만 유효 System.out.println(a); // 블록 A 끝 - 여기를 벗어나는 순간 \u0026#39;a\u0026#39;는 더 이상 접근할 수 없음 } //System.out.println(a); // 여기서 \u0026#39;a\u0026#39;를 사용하려고 하면 scope에서 벗어나기에 컴파일 에러 발생 { // 블록 B 시작 - 이 블록에서는 \u0026#39;a\u0026#39;를 새로 선언할 수 있지만, 위의 \u0026#39;a\u0026#39;와는 전혀 다른 변수임 String a = \u0026#34;Hello\u0026#34;; System.out.println(a); // 블록 B 끝 - 여기를 벗어나는 순간 새로 선언된 \u0026#39;a\u0026#39;도 접근할 수 없게 됨 } } 그럼 Null처리를 해야 하는 예외적은 상황은 언제일까? 위 예제의 Stack 클래스는 왜 메모리 누수에 취약할지를 생각해 보면, 바로 스택이 자기 메모리를 직접 관리하기 때문이다. 예제의 스택은 elements 배열의 자체 저장소 풀을 만들어 원소를 관리하기에 가비지 컬렉터가 알 수 없는 행위들이 일어난다. 즉 자기 메모리를 직접 관리하는 클래스라면 메모리 누수를 항상 주의해야 하고 원소를 다 쓴 즉시 참조한 객체들을 모두 null 처리해줘야 한다.\n3. 메모리 누수 3-1. 캐싱 캐시 역시 메모리 누수를 일으키는 주범이다. 객체 참조를 캐시에 넣고 객체를 다 쓴 뒤에도 객체 참조를 캐시에 보관하고 있을 때, 캐시가 제거되지 않으면 메모리를 계속 점유하여 메모리 누수가 발생할 수 있다. 만약 캐시외부에서 키를 참조하는 동안만 앤트리가 살아있는 캐시가 필요한 상황이면 WeakHashMap 사용해서 캐시를 만드는 것이 좋다.\nWeakHashMap - java.util의 Map 인터페이스 구현체 중 하나로, 약한 참조로 저장되어 가비지 컬렉터가 해당 키에 다른 참조가 없을 때 언제든지 회수를 진행한다. 즉, 저장된 앤트리는 키에 대한 강한 참조가 캐시 외부에서 사라지면 자동으로 제거될 수 있다.\npublic void weakHashMap() { WeakHashMap\u0026lt;Object, String\u0026gt; cache = new WeakHashMap\u0026lt;\u0026gt;(); Object key = new Object(); // 이 객체는 키로 사용됨 cache.put(key, \u0026#34;Value\u0026#34;); // 키와 값 쌍을 캐시에 저장 key = null; // 이제 \u0026#39;key\u0026#39; 객체에 대한 강한 참조가 없음 } 보통 캐시를 생성 시에 캐시의 유효기간을 정확히 정의하기 힘들기에 시간이 갈수록 앤트리의 가치를 낮추는 방식을 흔히 사용한다. 그렇기에 주기적으로 안 쓰는 앤트리를 제거해 주어야 한다. 백그라운드 스레드를 활용하여 캐시에 새 엔트리를 추가할 때마다 부수작업으로 진행하기도 하고 LinkedHashMap을 사용할 경우 removeEldestEntry 메서드를 써서 처리하기도 한다. 만약 더 복잡한 캐시를 만들고 싶다면 java.lang.ref 패키지를 활용하여 직접 생성도 할 수 있다. java.lang.ref 패키지의 Reference 유형을 확인해 보면 다음과 같다.\nSoftReference : 메모리가 부족한 시점까지 GC에 의해 회수되지 않지만 메모리가 부족하면 회수된다. 메모리에 민감하지 않은 캐시에 적합 WeakReference : 강한 참조가 없을 때 언제든지 GC에 회수될 수 있다 (WeakHashMap에 사용됨) PhantomReference : GC가 해당 객체를 처리하기 직전까지는 프로그램 코드에서 직접 참조할 수 없다. 보통 리소스를 안전하게 해제하거나 객체가 가비지컬렉션 되기 전에 특별한 작업을 수행할 때 사용된다. 3-2. 리스너(listener)와 콜백(callback) 메모리 누수의 세 번째는 리스터, 콜백이다. 클라이언트가 콜백을 등록만 하고 명확히 해지하지 않는다면 콜백은 쌓여만 갈 것이다. 이럴 때 WeakHashMap 같은 약한 참조를 사용하여 콜백을 저장하면 가비지 컬렉터가 즉시 수거해 가기에 메모리 누수를 방지할 수 있다..\n4. 정리 [ 메모리 누수는 겉으로 잘 드러나지 않지 않아서 철저한 코드리뷰나 힙 프로파일러 같은 디버깅 도구를 동원해야 하기에 예방법을 알아두는 것이 좋다]{style=\u0026ldquo;font-family: \u0026lsquo;Noto Serif KR\u0026rsquo;;\u0026rdquo;} 메모리를 직접 관리하지 않아도 어느 정도 가비지 컬렉터가 메모리를 관리해 주지만, 분명 한계인 부분이 있다. 문제없이 돌아가는 프로그램도 메모리 누수 현상이 숨어 있을 수 있고, 오래 실행할 경우 치명적인 문제로 이어질 수 있음을 인지하고 객체의 참조를 해제하는 올바른 예방법을 고려하며 개발하는 것이 중요하다. 예제 코드 https://github.com/junhkang/effective-java-summary/tree/master/src/main/java/org/example/ch01/item07/codes\n","permalink":"http://localhost:50666/posts/78/","summary":"\u003chr\u003e\n\u003cp\u003e \u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/78/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"메모리-관리\" ke-size=\"size26\"\u003e1. 메모리 관리\u003c/h2\u003e\n\u003cp\u003e자바에선 가비지 컬렉터가 다 쓴 객체를 알아서 회수해 가기에 편리하고 효율적으로 메모리를 관리할 수 있다. 하지만 메모리 관리에 신경 쓰지 않아도 된다는 말은 절대 아니다. 메모리를 적절하게 관리하지 못하면 메모리 누수가 발생하고 심하면 프로그램이 종료될 수 있다.\n \u003c/p\u003e\n\u003cp\u003e메모리를 적절하게 관리하지 못하는 경우의 예제를 살펴보자. 다음은 스택을 간단하게 구현한 자바 코드이다.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-arduino\" data-lang=\"arduino\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003epublic\u003c/span\u003e \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003eclass \u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eStack\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eprivate\u003c/span\u003e Object[] elements;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eprivate\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003esize\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eprivate\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estatic\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efinal\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e DEFAULT_INITIAL_CAPACITY \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e16\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003epublic\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eStack\u003c/span\u003e() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        elements \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enew\u003c/span\u003e Object[DEFAULT_INITIAL_CAPACITY];\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003epublic\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003epush\u003c/span\u003e(Object e)  {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        ensureCapacity();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        elements[\u003cspan style=\"color:#a6e22e\"\u003esize\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e++\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003epublic\u003c/span\u003e Object \u003cspan style=\"color:#a6e22e\"\u003epop\u003c/span\u003e() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (\u003cspan style=\"color:#a6e22e\"\u003esize\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003ethrow\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enew\u003c/span\u003e EmptyStackException();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e elements[\u003cspan style=\"color:#f92672\"\u003e--\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003esize\u003c/span\u003e];\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eprivate\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eensureCapacity\u003c/span\u003e()   {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (elements.\u003cspan style=\"color:#a6e22e\"\u003elength\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003esize\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            elements \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Arrays.copyOf(elements, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003esize\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e이대로 사용하여도 기능상으로는 전혀 문제가 없을 것이고, 어떤 테스트도 훌륭하게 통과하겠지만, 이 스택을 사용하는 프로그램을 오래 실행시킬 경우 가비지 컬렉션과 메모리 사용량이 늘어나 결국 성능이 저하될 것이다.\n \u003c/p\u003e","title":"[이펙티브 자바] 7. 다 쓴 객체 참조를 해제하라"},{"content":" 1. 객체의 재사용 똑같은 객체를 매번 새로 생성하는 것보다 하나를 생성 후 재사용하는 것이 훨씬 효율적이다. 특히 불변 객체는 언제든 재사용이 가능하다. 다음은 객체 생성 시 사용하면 안 되는 극단적인 예이다.\nString s = new String(\u0026#34;bikini\u0026#34;); 보기만 해도 불편한 이 생성방식은 실행될 때마다 String 객체를 새로 생성한다. 이후에 기능적으로는 동일하게 사용되지만 큰 반복문이나 자주 호출되는 메서드 안에 있다면 쓸모없는 인스턴스가 여러 개 생성될 것이다. 개선된 객체 생성 방식을 확인해 보자.\nString s = \u0026#34;bikini\u0026#34;; 이제 익숙한 String 객체 선언 방식이 되었다. 새로운 인스턴스를 매번 만드는 대신 하나의 String 인스턴스 사용하는 방식으로, 이 방식을 사용하면 같은 가상 머신 안에서 이와 같은 문자열 리터럴을 사용하는 모든 코드가 같은 객체를 재사용함이 보장된다. 생성자 대신 정적 팩토리를 제공하는 불변 클래스에서도 정적 팩터리 메서드를 사용해서 불필요한 객체 생성을 피할 수 있다. 예를들어 Boolean 생성자 대신 Boolean.valueOf 팩토리 메서드를 사용하면 호출될 때마다 새로운 객체가 생성되는 것을 방지할 수 있다.\n2. 객체의 반복 시 캐싱 사용 불변객체뿐 아니라 가변객체도 사용 중에 변경되지 않는다는 것을 알 수 있다면 재사용이 가능하다. 특히 생성 비용이 비싼 객체의 생성이 반복해서 필요할 경우 캐싱하여 사용 권장한다. 주어진 문자열이 유효한 로마 숫자인지를 확인하는 메서드 예제를 확인해 보자.\nstatic boolean isRomanNumeral(String s) { return s.matches(\u0026#34;^(\u0026gt;=.)M*(C[MD]|D?C{0,3})\u0026#34; + \u0026#34;(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})$\u0026#34;); } 기능상으로는 문제가 없는 String.matches를 사용한 정규표현식으로 문자열 형태를 확인하는 가장 쉬운 방법 중 하나이다. 하지만 빈번한 호출이 있을 경우 성능 측면에서는 적합하지 않다. Pattern 인스턴스는 한번 사용하고 버려 저서 바로 GC대상이 되며 Pattern은 입력받은 정규표현식에 해당하는 유한 상태머신을 만들기 때문에 인스턴스 생성비용이 높다\n유한 상태머신 [Finite State Machine (FS)] - 단순히 객체를 생성하는 것이 아니라 정규표현식에 일치하는 문자열을 찾기 위해 문자열을 상태에 따라 순차적으로 처리하는 논리 구조를 생성하기에 계산적으로 복잡하고 리소스를 많이 사용한다. 성능을 개선하려면 Pattern 인스턴스를 클래스 초기화 과정에서 생성하여 캐싱하고 isRomanNumeral이 호출될 때마다 재사용하는 방법을 적용하면 된다.\npublic class RomanNumerals { private static final Pattern ROMAN = Pattern.compile( \u0026#34;^(\u0026gt;=.)M*(C[MD]|D?C{0,3}) + \u0026#34;(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})$\u0026#34;); static boolean isRomanNumeral(String s) { return ROMAN.matcher(s).matches(); } } isRomanNumeral이 자주 호출될 때 성능개선 효과를 볼 수 있으며 ROMAN 필드를 final로 꺼내면서 의미가 더 명확해진다. 8글자 기준으로 100,000번 반복 호출 시 거의 10배의 시간 효율을 확인할 수 있었다. (테스트 코드는 깃허브에서 확인 가능)\n반면 isRomanNumeral 방식의 클래스가 초기화된 후 이 메서드를 한 번도 호출하지 않으면 ROMAN필드는 쓸데없이 초기화된 것이다.\n이 경우 isRomanNumeral 메서드가 처음 호출될 때 필드를 초기화하는 지연 초기화로 불필요한 초기화를 없앨 순 있지만 성능개선에 비해 코드복잡도가 올라가서 추천하지 않은 방식이다.\n3. 불필요한 객체 생성 (keySet) 객체가 불변이면 재사용하는 것이 항상 안전하지만, 불변인지 명확하지 않은 경우도 있다. Map 인터페이스의 keySet 메서드를 확인해 보자. Map인터페이스의 KeySet메서드는 Map 객체 안의 키를 Set뷰로 반환한다. 뷰를 통해 원본 'Map'에서 키를 제거하는 등, 반환된 Set인스턴스가 가변이더라도 모든 반환된 'Set' 인스턴스는 동일한 'Map' 인스턴스를 대변하기 때문에 기능적으로 정확히 일치한다. 반환된 'Set'을 통해 원본 'Map'의 키를 수정하거나 제거하면, 이변경 사항이 'Map'에 그대로 반영되어 참조하는 모든 뷰에 영향을 미치기에 반환한 객체 중 하나를 수정하면 다른 모든 객체가 따라서 바뀌게 된다. 따라서 'keySet' 메서드를 여러 번 호출하여 여러 개의 'Set' 뷰를 생성해도 기능상 문제는 없지만, 실질적으로 볼 수 있는 효과가 없다. 모든 'Set' 뷰는 원본 'Map'을 참조하기에 여러뷰를 생성하는 것보다 단일 뷰를 재사용하는 것이 효율적이다.\n4. 오토박싱 불필요한 객체를 만들어내는 또 다른 방식으로는 오토박싱이 있다. 자바에서 기본 타입과 박싱 된 기본타입을 섞어 쓸 때 자동으로 상호변환해 주는 기능이다. 다만 오토박싱은 기본 타입과 박싱 된 타입의 구분을 흐리게 해 주지만 아얘 구분을 없애는 것은 아니다. 다음 예제는 모든 양의 정수의 총합을 구하는 메서드로 int는 충분히 크지 않아 long을 사용 중이다. 기능 상으로는 동일하지만 성능에서는 오토박싱을 사용함으로써 성능에서 큰 차이를 보이고 있다.\n3-1. 오토박싱 사용 private static long sumLong() { Long sum = 0L; for (long i = 0; i \u0026lt;= Integer.MAX_VALUE; i++) { sum += i; } return sum; } 3-2. 기본 타입사용 private static long sumlong() { Long sum = 0L; for (long i = 0; i \u0026lt;= Integer.MAX_VALUE; i++) { sum += i; } return sum; } 오토박싱을 사용한 예제를 보면 sum 변수를 Long으로 선언하여 불필요한 Long인스턴스가 2^31개나 생성된다. 실제로 두 메서드의 실행 시간을 비교해 보면 아주 큰 차이가 남을 확인할 수 있다.\n그래서 박싱 된 기본 타입보다는 기본 타입을 사용하고, 의도치 않은 오토박싱이 숨어들지 않도록 주의해야 한다. 4. 주의 객체의 재사용이 효율적임을 강조했지만, 이펙티브 자바에서는 무조건적인 재사용이 효율적이지 않은 상황도 공유하고 있다. 프로그램의 명확성, 간결성, 기능을 위한 객체 생성은 일반적으로 좋지만 단순히 재사용성을 높이기 위해 객체 pool을 생성하는 것은 효율적이지 못하다. 데이터베이스 연결 같은 경우 생성 비용이 워낙 비싸기에 객체 pool(집합)을 생성하여 재사용하는 것이 효율적이지만, 일반적으로 자체 객체 풀은 객체를 풀에서 반환하는 추가로직이 필요하기에 코드의 명확성과 간결성이 떨어진다. 또한 사용하지 않는 개체들이 메모리에 남아있게 되어 메모리 사용량이 증가하게 된다. 또한 JVM GC가 상당히 최적화되어 있어 가벼운 객체를 효율적으로 관리하고 메모리를 회수하는데 최적화되어있기에, 가벼운 객체의 경우는 직접풀을 관리하는 것보다 새로 생성하는 것이 효율적일 수 있다.\n5. 정리 불변이 보장된 객체의 경우 재사용을 항상 고려\n무거운 객체의 반복 생성 시 캐싱을 사용\n불필요한 객체 생성을 주의\n의도치 않은 오토박싱이 숨어들지 않도록 주의\n데이터베이스 연결 같이 생성 비용이 아주 큰 경우를 제외하고는 객체 풀을 사용하여 객체를 재사용하는 것보다 새로 생성하는 것이 효율적 예제 및 성능 테스트 코드\nhttps://github.com/junhkang/effective-java-summary/tree/master/src/main/java/org/example/ch01/item06/codes\n","permalink":"http://localhost:50666/posts/77/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/77/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"객체의-재사용\" style=\"color: #000000;\" ke-size=\"size26\"\u003e1. 객체의 재사용\u003c/h2\u003e\n\u003cp\u003e똑같은 객체를 매번 새로 생성하는 것보다 하나를 생성 후 재사용하는 것이 훨씬 효율적이다. 특히 불변 객체는 언제든 재사용이 가능하다. 다음은 객체 생성 시 사용하면 안 되는 극단적인 예이다.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eString s = new String(\u0026#34;bikini\u0026#34;);\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e보기만 해도 불편한 이 생성방식은 실행될 때마다 String 객체를 새로 생성한다. 이후에 기능적으로는 동일하게 사용되지만 큰 반복문이나 자주 호출되는 메서드 안에 있다면 쓸모없는 인스턴스가 여러 개 생성될 것이다. 개선된 객체 생성 방식을 확인해 보자.\u003c/p\u003e","title":"[이펙티브 자바] 6. 불필요한 객체 생성을 피하라"},{"content":" 1. 개념 한 클래스 내에서 여러 개의 자원에 의존하여 사용되는 경우에 의존 객체 주입을 통해 유연성과 테스트 용이성을 개선하는 내용이다. 스프링의 의존성 주입 개념을 생각해 본다면 이미 당연하게 사용하고 있는 경우가 많을 것이지만, 의존 객체 주입의 장점을 다시 한번 생각해 볼 수 있는 내용이다.\nIoC(제어의 역전) \u0026amp; DI(의존성 주입)의 개념\n이펙티브 자바 책에서는 \u0026quot;맞춤법 검사기 (SpellChecker)\u0026quot; 클레스에서 \u0026quot;사전 (Dictionary)\u0026quot; 자원을 사용하는 예제를 들고 있다. 맞춤법 검사기 (SpellChecker)는 사전(dictionary) 자원에 의존하는 상황을 정적 유틸리티, 싱글턴, 의존객체 주입의 차이를 비교하고 있다.\n2. 예제 2-1. 정적 유틸리티 정적 유틸리티를 잘못 사용한 예 - 유연하지 않고 테스트하기 어렵다.\npublic class SpellCheckerStatic { private static final Lexicon dictionary = new Lexicon(); private SpellCheckerStatic() { } // 객체 생성 방지 public static boolean isValid(String word) { return dictionary.isValid(word); } public static List\u0026lt;String\u0026gt; suggestions(String typo) { return dictionary.suggestions(typo); } } 2-2. 싱글턴 싱글턴을 잘못 사용한 예 - 유연하지 않고 테스트하기 어렵다. public class SpellCheckerSingleton { private static final Lexicon dictionary = new Lexicon(); private SpellCheckerSingleton() { } public static SpellCheckerSingleton INSTANCE = new SpellCheckerSingleton(); public static boolean isValid(String word) { return dictionary.isValid(word); } public static List\u0026lt;String\u0026gt; suggestions(String typo) { return dictionary.suggestions(typo); } } 두방식 모두 dictionary 자원을 한 가지만 사용한다. 그렇기에 다양한 언어의 사전을 사용하거나 특수 언어용 사전을 별도로 쓰는 경우의 확장성을 생각해 보면 좋지 않다. 일반적으로 클래스에서 여러 자원을 참조할 경우 흔히 발생하는 상황이다. 위의 Spellchecker클래스에서 여러 사전을 유연하게 쓸 수 있도록 수정하려면 대표적으로 다음 두 가지 방식이 있을 것이다. 1. 단순히 dictionary 필드에서 final 제한을 제거하고 다른 사전으로 교체하는 메서드를 추가한다.\n어색하고, 오류를 내기 쉬우며 멀티 스레드 환경에서 사용 불가능하다. 사용하는 자원 객체에 따라 이후 동작이 달라지는 클래스에서는 정적 유틸리티 클래스나 싱글턴 방식이 적합하지 않다. 2. 인스턴스를 생성하는 시점에 필요에 맞는 dictionary를 넘겨주는 방식으로 변경한다. (의존객체 주입)\n의존 객체 주입의 한 형태로 맞춤법 검사기를 생성할 때 의존 객체인 사전을 주입해주면 된다. SpellChecker 클래스가 여러 인스턴스를 지원해야 하며 클라이언트가 원하는 자원을 사용해야 한다.\n2-3. 의존객체 주입 의존객체 주입은 유연성과 테스트 용이성을 높여준다.\npublic class SpellCheckerInjection { private static final Lexicon dictionary; public SpellCheckerInjection(Lexicon disctionary) { this.dictionary = Objects.requireNonNull(disctionary); } public static boolean isValid(String word) { return dictionary.isValid(word); } public static List\u0026lt;String\u0026gt; suggestions(String typo) { return dictionary.suggestions(typo); } } 예시에서는 dictionary라는 하나의 자원만 사용하지만 여러 개의 자원을 참조하는 경우가 대부분이다. 그런 상황에서 의존객체 주입을 사용했을 때의 장점을 생각해 보자.\n몇 개의 자원에 의존하던 관계없이 실행된다. 불변성을 보장하여 여러 클라이언트가 의존객체들을 안심하고 공유할 수 있다. 테스트가 용이하다. 이에 변형으로 생성자에 자원 팩토리 자체를 넘겨주는 방식도 별도로 소개하고 있다. 호출될 때마다 특정 타입의 인스턴스를 반복해서 만들어주는 객체를 말한다.(팩토리 메서드 패턴) 책에서는 자바 8의 Supplier \u0026lt;T\u0026gt; 인터페이스를 팩토리를 표현한 완벽한 예제로 소개하고 있다.\n팩토리의 타입 매개변수를 제한하며, 이 방식을 사용해 클라이언트는 자신이 명시한 타입의 하위 타입이라면 무엇이든 생성할 수 있는 팩토리를 넘길 수 있다. 다음 코드는 클라이언트가 제공한 팩토리가 생성한 타일들로 구성된 Mosaic를 만드는 샘플 메서드이다.\nMosaic create(Supplier\u0026lt;\u0026gt; extends Tile\u0026gt; tileFactory) { Tile tile = tileFactory.get(); return new Mosaic(tile); } 3. 정리 클래스가 내부에서 하나 이상의 자원에 의존할 때 클래스 동작에 영향을 주는 자원이 있다면 싱글턴, 정적 유틸 클래스는 사용하지 않는 것이 좋고 이 자원들을 클래스가 새로 생성해서도 안된다. 필요한 자원들을 생성자에 넘겨주는 생성자 주입 방식으로 구현하면 클래스의 유연성, 재사용성, 테스트 용이성을 매우 개선해 준다. 책의 예제 소스와 상세 내용은 다음 repo에서 확인 가능하다. https://github.com/junhkang/effective-java-summaryhttps://github.com/junhkang/effective-java-summary\nhttps://github.com/junhkang/effective-java-summary\n","permalink":"http://localhost:50666/posts/76/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/76/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"개념\" ke-size=\"size26\"\u003e1. 개념\u003c/h2\u003e\n\u003cp\u003e한 클래스 내에서 여러 개의 자원에 의존하여 사용되는 경우에 의존 객체 주입을 통해 유연성과 테스트 용이성을 개선하는 내용이다. 스프링의 의존성 주입 개념을 생각해 본다면 이미 당연하게 사용하고 있는 경우가 많을 것이지만, 의존 객체 주입의 장점을 다시 한번 생각해 볼 수 있는 내용이다.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://junhkang.tistory.com/42\"\u003eIoC(제어의 역전) \u0026amp; DI(의존성 주입)의 개념\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e이펙티브 자바 책에서는 \u0026quot;맞춤법 검사기 (SpellChecker)\u0026quot; 클레스에서 \u0026quot;사전 (Dictionary)\u0026quot; 자원을 사용하는 예제를 들고 있다. 맞춤법 검사기 (SpellChecker)는 사전(dictionary) 자원에 의존하는 상황을 정적 유틸리티, 싱글턴, 의존객체 주입의 차이를 비교하고 있다.\u003c/p\u003e","title":"[이펙티브 자바] 5. 자원을 직접 명시하지 말고 의존 객체 주입을 사용하라"},{"content":" 1. TOAST (The Oversized-Attribute Storage Technique)란? 데이터베이스의 대용량 속성을 효율적으로 저장하고 관리하기 위한 기법으로, 데이터를 효율적으로 처리하고, 저장공간을 최적화하며 데이터 접근시간을 개선하기 위해 사용된다. PostgreSQL의 각 page영역은 일반적으로 8kb의 고정된 크기로 되어있고 각 tuple이 여러 페이지에 나뉘어 존재할 수 없다. (매우 큰 값을 바로 저장할 수 없다.) 이 한계를 극복하기 위해서, 큰 필드 값은 압축되어 저장되거나 여러 개의 물리적 ROWS로 분할되어 저장된다. 이 과정은 보통 개발자가 별도의 처리로직을 구현할 필요 없이 데이터베이스 백앤드에서 자동으로 이루어진다. 이 기법을 TOAST (The Oversized-Attribute Storage Technique)라고 하며 PostgreSQL에서 큰 데이터 값을 메모리 내에서 효율적으로 처리하는 데에 사용된다.\n2. TOAST의 원리 TOAST는 특정 데이터 타입만 지원한다. 애초에 큰 데이터 필드값을 생성할 수 없는 고정된 타입에는 이러한 오버헤드를 부여할 필요가 없기 때문이다. 그러므로 TOAST 기능은 데이터 타입이 가변길이인 속성만을 지원한다. 일반적으로 저장된 값의 첫 번째 4바이트 단어가 전체 길이에 대한 정보를 포함한다. 예를 들어 총 100바이트의 공간을 차지하는 텍스트 데이터는 맨 앞 4바이트에 100이라는 값이 저장된다. TOAST 기능을 통해 처리되는 값들을 TOASTed 값이라고 불리며, 이 값들은 TOAST 헤더를 포함한 특별한 표현식으로 구성되어 있다. PostgreSQL은 TOAST 헤더 값을 수정하거나 재해석하여, 큰 데이터를 효율적으로 관리하고 접근할 수 있도록 해주고 데이터를 필요에 따라 압축하거나 필요한 부분을 선택적으로 로드하여 성능과 저장공간을 최적화할 수 있게 도와준다. TOAST 처리된 데이터를 가져오기 위해서는 DETOAST 과정으로 데이터를 복원해야 한다. DETOAST - 데이터를 원래의 비압축 상태로 복구하거나, 외부 저장소에서 읽어오는 과정이다, 데이터를 안전하게 읽고 처리할 수 있도록 데이터의 원본 구조와 내용을 복원하는 작업으로 \u0026lsquo;PG_DETOAST_DATUM\u0026rsquo; 함수로 명시적으로 실행 가능하다.\nTOAST는 헤더의 맨 앞 4바이트 중 2비트를 특별한 용도로 할당하여 TOAST처리에 관련된 추가정보를 저장한다. TOAST 가능한 데이터 유형의 값은 1GB (2^30 -1 바이트)로 제한한다. 별도로 할당된 2비트가 모두 0일 때 해당 데이터는 일반적인 토스트 처리가 되지 않은 데이터를 의미하고, 나머지 비트들은 전체 데이터의 크기를 바이트단위로 나타낸다. 데이터가 TOAST 처리가 될 경우 압축여부를 함께 저장하며, 만약 데이터가 아주 작다면 일반적인 4바이트 헤더 대신 1 바이트 헤더만을 가지게 된다. 1바이트 헤더를 사용할 경우 데이터의 길이, 데이터가 압축 같은 특별한 처리를 거쳤는지를 포함한다. 4바이트 대신 1바이트를 사용하면 127바이트보다 작은 값들을 효율적으로 저장하게 해 준다.\n1바이트 = 8비트로, TOAST처리 상태를 나타내기 위한 1비트를 할당 후, 나머지 비트는 데이터의 실제 길이를 나타내는 데 사용한다. 이 경우 최대 127바이트까지의 데이터 길이만을 표현할 수 있다. (나머지 7비트를 사용해서 표현할 수 있는 길이 2^7 -1 = 127)\n1바이트 헤더의 경우 별도 정렬이 없지만, 4바이트 헤더를 가진 값들은 4바이트 경계에 맞춰 정렬된다. 경계에 맞춘 정렬 - 데이터를 메모리상의 특정 위치에 저장할 때, 그 위치가 특정 크기의 배수가 되도록 정렬한다는 의미로 4바이트 경계에 정렬은 데이터가 4의 배수가 되는 메모리 주소에 저장함을 의미한다. CPU가 데이터에 더 효율적으로 접근 가능하게 하지만 지정된 위치에 데이터를 저장해야 하기에 공백 (패딩)이 생길 수 있어 데이터가 차지하지 않는 메모리 공간이 생길 수 있다. 1바이트 헤더에는 데이터의 메타정보가 저장되어 있고, 특정 패턴을 보일 때 추가적인 정보를 제공한다. 예를 들어 헤더에서 나머지 비트가 모두 0일 경우(자신을 포함한 길이를 나타내는 영역이기에 이론상 0이 될 수 없다.), 이 데이터는 실제 값이 아닌 외부 데이터를 가리키는 포인터라는 특별한 상황을 의미한다. 또한 최상위, 최하위 비트가 설정되지 않았다면, 데이터는 압축된 상태이며, 사용되기 전에 압축해제가 되어야 한다. 이 경우 4바이트 헤더의 나머지 부분은 원데이터가 아니라 압축된 데이터의 총용량을 의미한다. 외부 저장 데이터에 대해서도 압축이 가능하지만, 헤더만으로는 압축여부를 판단할 수 없고, TOAST 포인터에서 확인가능하다. 3. TOAST의 관리와 활용 PostgreSQL에서는 인 라인 또는 아웃라인 압축 데이터에 사용되는 압축 기법을 칼럼별로 선택할 수 있다. CREATE TABLE 혹은 ALTER TABLE 명령어를 COMPRESSION과 함께 사용하면 된다. CREATE TABLE example ( column_name data_type COMPRESSION compression_method, ... ); ALTER TABLE example ALTER COLUMN column_name SET COMPRESSION compression_method; PGLZ - 기본 알고리즘으로 일반적으로 안정적인 압축 비율 제공한다. 다양한 유형의 데이터에 대해 괜찮은 성능을 보이며, 매우 큰 데이터에 대해 상대적으로 다른 방식보다 느리다. LZ4 - 높은 압축 속도를 제공하며, 13 버전 이후에서만 사용가능하다. 압축속도는 빠르지만 압축률은 낮기에 성능이 중요한 환경에서 큰 데이터를 다룰 때 유용하게 사용 가능하다. 옵션을 지정하지 않으면 default_toast_compression 옵션의 설정값을 사용한다. 해당 명령어를 통해 기본 설정을 확인/변경할 수 있다.\nSHOW default_toast_compression; SET default_toast_compression = \u0026#39;압축방식\u0026#39;; 다만 SET을 통한 변경은 해당 세션에서만 적용되기에, 영구 수정은 설정파일을 직접 수정해야 한다. 4. TOAST 포인터 TOAST 포인터 데이터는 메인 테이블에 저장되며, 실제 데이터가 TOAST 테이블에 저장된 위치를 가리키는 역할을 한다. TOAST 포인터 데이터에는 여러 가지 유형이 있으며, 보통은 메인테이블 외부의 TOAST 테이블을 가리킨다. 이 디스크상의 포인터 데이터는 TOAST management code (access/common/toast_internals.c 파일)에 의해 관리되며, tuple이 그 자체로 저장되기에 너무 큰 사이즈일 때 생성된다. TOAST 포인터는 메모리 내의 다른 위체 존재하는 외부 데이터를 가리키는 포인터를 포함할 수도 있다. 메모리상 저장된 외부데이터는 보통 휘발성 데이터이고 디스크에 저장되지 않는다. 이러한 데이터는 메모리에 중간연산 데이터를 저장하여 캐싱하여 사용하고, 데이터 접근시간이 대폭 감소하기에 큰 데이터 값을 복사하거나 불필요한 반복처리를 피할 때 매우 유용하다. 테이블의 컬럼중 하나라도 TOAST가 가능하다면, 테이블은 TOAST테이블과 연관되며 테이블의 OID는 TOAST테이블의 pg_class.reltoastrelid 항목에 저장된다. 디스크 상의 TOAST 된 값은 TOAST테이블에 유지된다.\n외부 테이블에 저장되는 데이터는 TOAST_MAX_CHUNK_SIZE (byte) 단위로 분리되어 저장된다. 각 분리 단위를 chunk라고 하며, 한 page 에는 4개의 chunk가 들어갈 수 있다. 1 page = 8KB (8,192 byte), 1 chunk = 2048 byte\n각 chunk는 TOAST 테이블에 별도의 ROW로 저장된다. 모든 TOAST 테이블은 다음 항목들로 이루어진다.\nchunk_id - TOAST된 특정 값을 식별하기 위한 OID chunk_seq - 각 chunk의 순번 chunk_data - chunk의 실제 데이터 chunk_id와 chunk_seq에 UNIQUE 인덱스가 걸려있어 값을 빠르게 찾을 수 있고 기존 테이블에서 TOAST 된 데이터를 찾기 위해서는 TOAST 테이블의 OID와 chunk_id를 저장해야 한다.\n좀 더 효율적으로 데이터를 찾기 위해서, 포인터 데이터는 데이터의 압축 전 원 데이터사이즈와 압축 후 데이터 사이즈, 압축 방식을 저장한다. 전체 구성을 보면 TOAST 포인터 데이터는 18바이트로 구성되며 다음 항목들을 저장한다.\nVarlena header - 위에서 설명한 4바이트의 길이정보, 압축정보 TOAST 테이블 OID - 4바이트의 TOAST에 저장된 테이블을 식별하는 고유 식별자 chunk_id - 실제 데이터 chunk를 식별하는 고유 식별자 4바이트 위치정보 - 데이터 청크의 실제 위치를 가리키는 추가정보가 필요하다면 사용, 나머지 바이트 TOAST는 TOAST_TUPLE_THRESHOLD bytes보다 큰 값이 저장될 때 실행된다. TOAST management code는 임계치보다 용량이 줄어들 때까지, 혹은 압축가능한 최대치로 압축을 하거나, 외부 테이블로 값을 옮긴다. 다만 UPDATE가 실행되는 동안, 변경이 없는 필드의 값은 그대로 유지된다. 그래서 TOAST 영역에 저장된 데이터의 변경이 이루어지지 않는 한 TOAST비용은 발생하지 않는다.\n5. TOAST 저장 방식 TOAST관리 코드는 디스크에 TOAST 가능한 칼럼을 판단할 때 다음 4가지를 전략을 사용한다.\nPLAIN - 압축/외부 저장소를 사용하지 않는다. TOAST 불가능한 칼럼 대상으로만 적용 가능하다. EXTENDED - 압축/외부저장소를 둘 다 사용한다. TOAST가능한 데이터의 기본 옵션으로 압축이 먼저 시도되고 그래도 여전히 데이터가 너무 크다면 외부 저장소를 사용한다. EXTERNAL - 압축은 허용하지 않고, 외부저장소를 허용한다. text, bytea 칼럼의 substring속도를 향상해 준다 (저장소 공간은 더 사용하지만, 압축을 해제할 필요 없이 필요한 부분만 바로 찾을 수 있다.) MAIN - 압축은 허용하고, 외부저장소는 허용하지 않는다. (외부저장소를 허용하지 않는 옵션이지만, 압축 후에도 page에 사이즈를 맞출 수 있는 다른 방법이 없다면 최후의 수단으로 외부 저장소를 사용한다.) 압축 외부저장소 PLAIN X X EXTENDED O O EXTERNAL X O MAIN O X (선택적 사용) 각 TOAST 가능한 칼럼에 대해 TOAST 저장 전략을 각각 설정도 가능하다.\nALTER TABLE my_table ALTER COLUMN my_column SET STORAGE EXTENDED; 6. TOAST 적용 시점 TOAST_TUPLE_TARGET으로 테이블이 TOAST를 고려하는 시점을 지정할 수 있다. 예를 들어 TOAST_TUPLE_TARGET이 2048로 설정되면 테이블의 행 크기가 2048byte에 도달했을 때 TOAST처리를 고려한다. ALTER TABLE ... SET (toast_tuple_target = N) TOAST 시스템은 page사이즈보다 큰 데이터를 강제로 저장하는 방식에 비해 훨씬 효율적이다. 일반적으로 쿼리가 상대적으로 작은 키 값에 대한 비교를 할 때, 대부분의 작업은 메인 ROW값을 통해 실행된다. TOAST 된 큰 값은 연산 후 추출될 때만 꺼내질 것이다. 그래서 메인테이블은 훨씬 더 작아지고, 메인테이블의 행들이 공유 버퍼 캐시에 더 많이 저장될 수 있다.\n7. 인메모리 토스트 저장소 TOAST포인터는 디스크상에 없는 현재 프로세스상의 데이터를 가리킬 수도 있다. 물론 해당 포인터 데이터는 휘발성이지만 효율적이다. 7-1. 간접 데이터를 가리키는 포인터 메모리상에서 간접적으로 varlena포인터를 가리킨다. 초기에는 개념을 증명하기 위해서 만들어졌지만, 현재는 디코딩 간 모든 실제 데이터를 튜플에 포함시키지 않고 효율적으로 데이터를 처리하여 1GB를 초과하는 튜플을 생성하는 것을 방지하기 위해 사용된다. 포인터가 참조하는 데이터에 대한 관리를 유저가 직접(데이터가 변경될 경우 메모리상의 참조 데이터도 명시적으로 변경해주어야 함) 해주어야 하기에 사용이 제한적이다.\n7-2. 확장된 데이터를 가리키는 포인터 확장된 데이터를 가리키는 포인터는 복잡한 데이터 유형에 유용하다. 예를 들어, 일반적이 varlena 표현식은 다음과 같은 값들을 포함한다.\n차원 정보 - 배열의 차원수와 각 차원의 크기정보 데이터의 길이 - 데이터가 차지하는 바이트 수 NULL 비트맵 - 배열에 NULL이 포함되어 있는 경우, 어떤 요소가 null인지를 아려주는 비트맵 해당 해더들 뒤에 실제 데이터 요소들이 순차적으로 저장된다. 순차적으로 저장되기에, 요소 타입 자체가 가변길이인 경우 N번째 요소를 찾기 위해서는 모든 선행요소들을 순차적으로 스캔해야만 한다. 데이터를 가능한 적은 공간에 저장하기 위해 모든 요소를 연속적으로 배치하는 이 방식은, 저장공간을 효율적으로 사용하기에 디스크 저장소에는 적합하지만, 특정 요소에 접근하거나 배열을 활용한 계산을 수행할 때는 모든 선행 요소를 순차적으로 검사해야 하는 단점이 있다. 배열의 각 요소가 어디에서 시작하는지를 미리 식별해서 **\u0026ldquo;확장된 데이터\u0026rdquo;**를 별도로 저장함으로써 특정요소에 직접, 더 빠르게 접근이 가능하다. 메모리상에서 이러한 별도의 확장된 저장영역을 TOAST 포인터가 가리킬 수 있게 함으로써 효율적인 데이터 처리를 가능하게 한다. 확장 데이터를 가리키는 포인터는 read-write, read-only 포인터로 나뉜다.\nread-write 포인터 - 함수가 참조하는 값을 메모리에서 직접 수정 가능하다. 원데이터에 변경이 필요한 경우 추가적인 복사 없이 해당 값을 바로 업데이트 가능 read-only 포인터 - 참조된 값을 변경할 수 없다. 값을 수정하려면 먼저 해당 값을 복사한 후 복사된 데이터에 대해 변경을 수행해야 한다. 원본데이터의 무결성을 보장하기 위한 조치이다. 이렇게 구분함으로써 데이터를 읽기만 하는 경우에는 복사본을 생성하지 않고, 데이터를 수정하는 경우에만 복사본을 생성하기에 메모리 사용량이 줄어들고 성능향상에 도움이 된다.\n메모리 상의 TOAST 포인터는 데이터가 메모리에 로드되어 있을 때 사용되며, 데이터 처리과정에서 임시적으로만 사용된다. TOAST management code는 이러한 메모리상의 임시 포인터 데이터가 그대로 디스크에 영구적으로 잘못 저장되지 않도록 보장하기 위해 다음과 같은 처리를 진행하며, 전처리를 완료한 후 적절한 형태로 디스크에 데이터를 저장한다.\n메모리 내 TOAST 포인터 확장 : 데이터가 디스크에 저장되기 전에 메모리 내에서 사용되던 toast 포인터는 일반 varlena 표현식으로 치환한다. 이 과정에서 실제 데이터가 인라인 형태로 확장되며 테이블의 일부로 저장될 수 있도록 준비된다. 디스크상 TOAST 포인터로 변환 : 데이터를 테이블에 저장할 때 해당 튜플의 크기가 너무 크면 자동으로 데이터를 디스크에 별도로 저장하고, 원래 테이블에는 데이터를 가리키는 TOAST 포인터만을 저장한다. 참고\nhttps://www.postgresql.org/docs/16/storage-toast.html#STORAGE-TOAST-INMEMORY\n","permalink":"http://localhost:50666/posts/75/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/75/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"toast-the-oversized-attribute-storage-technique란\" ke-size=\"size26\"\u003e1. TOAST (The Oversized-Attribute Storage Technique)란?\u003c/h2\u003e\n\u003cp\u003e데이터베이스의 대용량 속성을 효율적으로 저장하고 관리하기 위한 기법으로, 데이터를 효율적으로 처리하고, 저장공간을 최적화하며 데이터 접근시간을 개선하기 위해 사용된다. PostgreSQL의 각 page영역은 일반적으로 8kb의 고정된 크기로 되어있고 각 tuple이 여러 페이지에 나뉘어 존재할 수 없다. (매우 큰 값을 바로 저장할 수 없다.) 이 한계를 극복하기 위해서, 큰 필드 값은 압축되어 저장되거나 여러 개의 물리적 ROWS로 분할되어 저장된다. 이 과정은 보통 개발자가 별도의 처리로직을 구현할 필요 없이 데이터베이스 백앤드에서 자동으로 이루어진다. 이 기법을 TOAST (The Oversized-Attribute Storage Technique)라고 하며 PostgreSQL에서 큰 데이터 값을 메모리 내에서 효율적으로 처리하는 데에 사용된다.\u003c/p\u003e","title":"[PostgreSQL] TOAST (The Oversized-Attribute Storage Technique)의 개념, PostgreSQL의 대용량 속성 저장 기법"},{"content":"안녕하세요. 산업의 역군의 Jun Kang입니다. AI와 빅데이터를 활용하여 건설업계의 디지털 혁신을 이끌고, 데이터 기반의 의사결정을 통해 더 공정한 기회를 제공하고자 합니다.\n개발적으로는 빅데이터 처리, 데이터베이스 성능 최적화, 효율적인 개발 실천에 깊은 관심을 가지고 있습니다. 새로운 기술을 배우고 적용하는 데 열정을 가지고 있으며, 현재는 산업의 역군 개발팀의 건강하고 지속 가능한 성장 문화를 구축하는 데 힘쓰고 있습니다.\n","permalink":"http://localhost:50666/about/","summary":"\u003cp\u003e안녕하세요. \u003ca href=\"https://www.sankun.com\"\u003e산업의 역군\u003c/a\u003e의 \u003cstrong\u003eJun Kang\u003c/strong\u003e입니다.\n\u003cstrong\u003eAI와 빅데이터\u003c/strong\u003e를 활용하여 건설업계의 디지털 혁신을 이끌고, 데이터 기반의 의사결정을 통해 더 공정한 기회를 제공하고자 합니다.\u003c/p\u003e\n\u003cp\u003e개발적으로는 빅데이터 처리, 데이터베이스 성능 최적화, 효율적인 개발 실천에 깊은 관심을 가지고 있습니다. 새로운 기술을 배우고 적용하는 데 열정을 가지고 있으며, 현재는 \u003cstrong\u003e산업의 역군\u003c/strong\u003e 개발팀의 건강하고 지속 가능한 성장 문화를 구축하는 데 힘쓰고 있습니다.\u003c/p\u003e","title":"Jun Kang"},{"content":" 1. Disk 영역 PostgreSQL의 저장 영역은 크게 Heap, Index, Toast 3개로 나뉜다. 각 테이블의 대부분의 데이터를 메인 heap 영역에 저장한고, 테이블 칼럼 중 매우 큰 데이터를 받을 수 있는 칼럼은 TOAST 영역에 별도 저장한다. TOAST 테이블에는 실제 데이터를 저장하고, 본래 테이블에는 해당 데이터를 가리키는 포인터만 남게 된다. TOAST 테이블의 유효한 인덱스는 1개뿐이고, 기준 테이블에는 더 많은 인덱스가 존재할 수 있다. 테이블과 인덱스는 각각의 디스크파일에 저장 되며 각 파일이 1G가 넘으면 별도 파일로 분리된다.\nHeap - 실제 메인 데이터가 저장되는 곳 Index - 데이터에 빠르게 접근할 수 있도록 돕는 색인정보, 검색속도를 높이고 데이터의 무결성을 유지하는데 도움 Toast - 테이블의 한 행이 페이지 크기 (일반적으로 8KB)를 초과할 경우 별도 분리된 공간인 TOAST에 저장하여 데이터베이스의 효율성 유지 2. Disk 용량 모니터링 현재 산군 서비스는 AWS, Datadog의 모니터링을 사용하고 있지만 공식문서에 따르면 디스크용량을 모니터링할 수 있는 방법은 3가지가 있다.\nSQL 함수를 사용 oid2name 시스템 카탈로그의 수동 검사 2-1. SQL 기본 함수 사용 SQL 기본 함수를 사용하여 실시간으로 조회하는 것이 가장 쉽고 보편적인 방법이다. 쿼리를 사용하여 필요한 항목, 조건에 따라 조회하는 방식이다. (예제의 PG_SIZE_PRETTY -\u0026gt; 가독성 좋은 단위로 변경)\n2-1-1. 테이블의 디스크 사용량 확인 PG_RELATION_SIZE = 지정한 테이블의 크기를 바이트 단위로 반환\nSELECT pg_size_pretty(pg_relation_size(\u0026#39;테이블명\u0026#39;)); 2-2-2. 전체 데이터베이스 디스크 확인 PG_DATABASE_SIZE = 전체 데이터베이스의 크기를 바이트 단위로 반환\nSELECT pg_size_pretty(pg_database_size(\u0026#39;데이터베이스명\u0026#39;)); 2-2-3. 인덱스 디스트 사용량 확인 PG_INDEX_SIZE = 특정 테이블에 연결된 인덱스 크기를 반환\nSELECT pg_size_pretty(pg_indexes_size(\u0026#39;테이블명\u0026#39;)); 2-2-4. 테이블스페이스 크기확인 PG_TABLESPACE_SIZE = 테이블 스페이스의 디스크 사용량을 바이트단위로 반환\nSELECT pg_size_pretty(pg_tablespace_size(\u0026#39;테이블스페이스명\u0026#39;)); 2-2. oid2 name PostgreSQL에 사용되는 파일구조를 검사하는 유틸리티 프로그램으로, 대상 데이터베이스에 연결하여 OID, 파일노드 및 테이블 정보를 추출한다. 테이블 OID와 파일 노드 간의 차이를 반드시 이해하고 사용해야 한다. 2-3. 시스템 카탈로그에서 직접 확인 (Psql) 데이터베이스의 메타데이터 (테이블, 뷰, 인덱스 등)을 직접 조회하고 분석\n최근 vacuum 되거나 재집계된 데이터베이스를 대상으로 psql을 통해 디스크 사용량을 확인하기 위한 쿼리를 실행시킬 수 있다.\n2-3-1. 테이블 용량 확인 SELECT pg_relation_filepath(oid), relpages FROM pg_class WHERE relname = \u0026#39;테이블명\u0026#39;; 각 \u0026quot;page\u0026quot;는 일반적으로 8kb이며 relpage는 VACUUM, ANALYZE, CREATE INDEX와 같은 몇 가지 DDL 명령문에 의해서만 업데이트된다. 파일 경로 이름은 테이블의 디스크 파일을 직접 확인하고 싶을 때 필요한 정보이다. 예제의 테이블은 인덱스와 파티션이 포함되어 있는 1300만 ROW에 대한 정보로 926,896페이지 용량의(926,896 * 8kb로 약 7.07GB) 테이블이다.\n2-3-2. TOAST 용량확인 TOAST 테이블을 사용하는 경우 다음과 같다.\nSELECT relname, relpages FROM pg_class, (SELECT reltoastrelid FROM pg_class WHERE relname = \u0026#39;테이블명\u0026#39;) AS ss WHERE oid = ss.reltoastrelid OR oid = (SELECT indexrelid FROM pg_index WHERE indrelid = ss.reltoastrelid) ORDER BY relname; 2-3-3. 인덱스 용량 확인 SELECT c2.relname, c2.relpages FROM pg_class c, pg_class c2, pg_index i WHERE c.relname = \u0026#39;테이블명\u0026#39; AND c.oid = i.indrelid AND c2.oid = i.indexrelid ORDER BY c2.relname; 2-3-4. 최대 용량 테이블, 인덱스 찾기 각 영역의 페이지 용량으로 정렬하면, 최대 용량의 테이블 및 인덱스를 조회할 수 있다.\nSELECT relname, relpages FROM pg_class ORDER BY relpages DESC; 참고 https://www.postgresql.org/docs/16/disk-usage.html\n","permalink":"http://localhost:50666/posts/74/","summary":"\u003chr\u003e\n\u003ch2 id=\"disk-영역\" ke-size=\"size26\"\u003e1. Disk 영역\u003c/h2\u003e\n\u003cp\u003ePostgreSQL의 저장 영역은 크게 Heap, Index, Toast 3개로 나뉜다. 각 테이블의 대부분의 데이터를 메인 heap 영역에 저장한고, 테이블 칼럼 중 매우 큰 데이터를 받을 수 있는 칼럼은 TOAST 영역에 별도 저장한다. TOAST 테이블에는 실제 데이터를 저장하고, 본래 테이블에는 해당 데이터를 가리키는 포인터만 남게 된다. TOAST 테이블의 유효한 인덱스는 1개뿐이고, 기준 테이블에는 더 많은 인덱스가 존재할 수 있다. 테이블과 인덱스는 각각의 디스크파일에 저장 되며 각 파일이 1G가 넘으면 별도 파일로 분리된다.\u003c/p\u003e","title":"[PostgreSQL] DISK(디스크) 사용량 모니터링"},{"content":" 1. 데코레이터(Decorator) 패턴이란? 중심이 되는 객체가 있고, 장식이 되는 기능을 하나씩 추가하여 목적에 더 맞는 객체로 만들어가는 디자인 패턴을 Decorator 패턴이라고 한다. decorator 란 \u0026quot;장식하는 사람\u0026quot;이라는 뜻이다. 예제를 통해 상세 개념을 확인해 보자. (Java언어로 배우는 디자인 패턴 입문, 3편의 예제를 그대로 사용하였다.) 적용할 예제는, 문자열 주위에 \u0026quot;장식틀\u0026quot;을 붙여 표현하는 것이다. 예를 들어 Hello World라는 기본 문구에 장식틀을 붙여 중첩 장식을 한 후 출력하는 예제이다.\n+=============+ |Hello, world| +=============+ 사용될 클래스는 각각 다음과 같다.\nDisplay - 문자열 표시용 추상 \b클래스 StringDisplay - 단일 행으로 구성된 문자열 표시용 클래스 Border - 장식틀 추상 클래스 SideBorder - 좌우 장식틀 클래스 FullBorder - 상하좌우 장식틀 클래스 예제 클래스 다이어그램\n각 클래스의 역할\nComponent - 기능을 추가할 때 핵심이 되는 객체 (Display) ConcreteComponent - Component인터페이스를 구현하는 구현체 (StringDisplay) Decorator - Component와 동일한 인터페이스를 가지고 장식할 대상이 되는 Component도 포함(Border) ConcreteDecorator - 구체적인 Decorator (SideBorder, FullBorder) 2. 예제 2-1. Display 클래스 여러 행으로 이루어진 문자열을 표시하는 추상 클래스이다. 각 메서드의 역할을 살펴보면 다음과 같다.\ngetColumns - 가로 문자수 가져오기 getRows - 세로 문자수 가져오기 getRowText - 지정한 행의 문자열 가져오기 show - 모든 행을 표시하기 (getRow를 가져와서 for문을 돌며 getRowText 메서드로 표시할 문자열을 가져온다) public abstract class Display { public abstract int getColumns(); // 가로 문자 수를 얻는다 public abstract int getRows(); // 세로 행수를 얻는다 public abstract String getRowText(int row); // row행째 문자열을 얻는다 // 모든 행을 표시한다 public void show() { for (int i = 0; i \u0026lt; getRows(); i++) { System.out.println(getRowText(i)); } } } 2-2. StringDisplay 클래스 Display 추상 클래스만 봐서는 이해하기 어려우므로 하위 클래스인 StringDisplay 클래스를 살펴보자. StringDisplay클래스는 장식이 들어갈 중심이 될 객체 역할이며, Display클래스에서 선언된 추상 메서드를 구현한다.\ngetColumns - string.length()로 반환되는 문자열의 길이 getRows - 1을 반환 getRowText - 0번째 행의 값을 취할 때만 string 필드 반환 public class StringDisplay extends Display { private String string; // 표시 문자열 public StringDisplay(String string) { this.string = string; } @Override public int getColumns() { return string.length(); } @Override public int getRows() { return 1; // 행수는 1 } @Override public String getRowText(int row) { if (row != 0) { throw new IndexOutOfBoundsException(); } return string; } } 2-3. Border 클래스 문자열을 표시하는 Display클래스의 하위 클래스로 정의되어 있다. 상속에 의해 내용물과 동일한 메서드를 가지게 된다. getColumns, getRows, getRowText, show 메서드를 그대로 상속받으며 인터페이스(API) 관점에서 보면 장식품(Boder)이 내용물(Display)을 동일시할 수 있다는 뜻이다.\npublic abstract class Border extends Display { protected Display display; // 이 장식틀이 감싸는 \u0026#39;내용물\u0026#39; protected Border(Display display) { // 인스턴스 생성 시 \u0026#39;내용물\u0026#39;을 인수로 지정 this.display = display; } } Border는 Display 형의 display 필드를 가지고 있으면서 Border도 Display의 하위 클래스 이므로 display필드의 내용물은 또 다른 장식(Border클래스의 하위 클래스) 일 수도 있다. 그리고 그 장식 또한 display필드를 가지고 있을 수 있다.\n2-4. SideBorder 클래스 SideBorder클래스는 구체적인 장식의 일종으로 Border클래스의 하위 클래스이다. SideBorder 클래스는 문자열 좌우에 정해진 문자로 장식한다.\nborderChar - 어떤 문자로 장식할지 지정 getColumns - 표시문자의 가로 문자수 (내용물의 문자수 + 좌우 장식 문자수) getRows - display.getRows()를 그대로 사용 getRowsText - 기존 문자열 양쪽에 borderChar를 붙인 후 반환 public class SideBorder extends Border { private char borderChar; // 장식 문자 // 내용물이 될 Display와 장식 문자를 지정 public SideBorder(Display display, char ch) { super(display); this.borderChar = ch; } @Override public int getColumns() { // 문자 수는 내용물의 양쪽에 장식 문자만큼 더한 것 return 1 + display.getColumns() + 1; } @Override public int getRows() { // 행수는 내용물의 행수와 같다 return display.getRows(); } @Override public String getRowText(int row) { // 지정 행의 내용은 내용물의 지정 행 양쪽에 장식 문자를 붙인 것 return borderChar + display.getRowText(row) + borderChar; } } 2-5. FullBorder 클래스 FullBorder 클래스는 SideBorder 클래스와 동일하게 Border 하위 클래스이다. 예제에서는 SiderBorder이 좌우로만 문자를 장식했다면, 상하좌우를 모두 장식할 수 있으며, 장식 문자는 따로 지정할 수 없게 되어있다.\ngetRowText - Row가 0일 경우 상단 장식추가, Row숫자보다 1클경우 하단 장식 추가 makeLine - 지정한 문자가 연속하는 문자열을 만드는 보조 메서드 public class FullBorder extends Border { public FullBorder(Display display) { super(display); } @Override public int getColumns() { // 문자 수는 내용물 양쪽에 좌우 장식 문자만큼 더한 것 return 1 + display.getColumns() + 1; } @Override public int getRows() { // 행수는 내용물의 행수에 상하 장식 문자만큼 더한 것 return 1 + display.getRows() + 1; } @Override public String getRowText(int row) { if (row == 0) { // 상단 테두리 return \u0026#34;+\u0026#34; + makeLine(\u0026#39;-\u0026#39;, display.getColumns()) + \u0026#34;+\u0026#34;; } else if (row == display.getRows() + 1) { // 하단 테두리 return \u0026#34;+\u0026#34; + makeLine(\u0026#39;-\u0026#39;, display.getColumns()) + \u0026#34;+\u0026#34;; } else { // 기타 return \u0026#34;|\u0026#34; + display.getRowText(row - 1) + \u0026#34;|\u0026#34;; } } // 문자 ch로 count 수만큼 연속한 문자열을 만든다 private String makeLine(char ch, int count) { StringBuilder line = new StringBuilder(); for (int i = 0; i \u0026lt; count; i++) { line.append(ch); } return line.toString(); } } 2-6. Main 이제 샘플 코드를 실행시킬 메인 클래스를 보자\npublic static void main(String[] args) { Display b1 = new StringDisplay(\u0026#34;Hello, world.\u0026#34;); Display b2 = new SideBorder(b1, \u0026#39;#\u0026#39;); Display b3 = new FullBorder(b2); b1.show(); b2.show(); b3.show(); Display b4 = new SideBorder( new FullBorder( new FullBorder( new SideBorder( new FullBorder( new StringDisplay(\u0026#34;Hello, world.\u0026#34;) ), \u0026#39;*\u0026#39; ) ) ), \u0026#39;/\u0026#39; ); b4.show(); 예제에서 각 인스턴스의 역할을 보면\nb1 - \u0026quot;Hellow, world\u0026quot;를 장식 없이 기본으로 표현 b2 - b1에 '#'으로 좌우 장식만 추가 b3 - b2에 상하좌우 전체 장식틀 추가 b4 - \u0026ldquo;Hello, world\u0026quot;에 여러 겹 장식틀 추가 실행 결과를 확인해 보면 다음과 같다.\n// b1.show() Hello, world. // b2.show() #Hello, world.# // b3.show() +---------------+ |#Hello, world.#| +---------------+ // b4.show() /+-------------------+/ /|+-----------------+|/ /||*+-------------+*||/ /||*|Hello, world.|*||/ /||*+-------------+*||/ /|+-----------------+|/ /+-------------------+/ b2, b3는 생성될 때 b1, b2를 각각 참조하는데, 서로의 관계를 확인해 보면 b1의 장식이 b2, b2의 장식이 b3인 관계가 성립한다.\n3. 정리 Decorator 패턴은 장식틀과 내용물을 동일시하는 디자인 패턴이다. 장식틀을 사용해서 특징을 추가하며 감싸더라도 내부 API는 가려지지 않는다는 특징이 있고(외부에서 getColumns, getRows 등의 메서드를 볼 수 있다.) 장식틀을 중첩적으로 추가하여 중심이 되는 객체를 계속해서 변경할 수 있다. 내용물과 장식틀이 동일시된다는 점에서 Composite 패턴과 유사한 점이 있지만, Decorator 패턴은 바깥 테두리를 추가함으로써 기능을 추가 나가는 것이 주된 목적이라는 점에서 다르다.\n4. 장점 내용 변경 없이 기능 추가 가능 - 핵심 객체에 대한 변경 없이, 장식틀을 추가하여 기능을 추가할 수 있다. 단순한 구성으로 다양한 변경 가능 - 구체적인 장식틀 (ConcreteDecorator)를 많이 준비하면 장식틀끼리 조합하여 새로운 객체를 계쏙 만들어 낼 수 있기 때문이다. 단순한 구성의 장식틀의 조합으로 다양한 기능을 추가할 수 있다. - 참고 : JAVA 언어로 배우는 디자인 패턴 입문 3편\n- 상세 예제소스는 깃허브에서 확인가능\nhttps://github.com/junhkang/java-design-pattern/tree/master/src/main/java/com/example/javadesignpattern/decorator\n","permalink":"http://localhost:50666/posts/73/","summary":"\u003chr\u003e\n\u003ch2 id=\"데코레이터decorator-패턴이란\" ke-size=\"size26\"\u003e1. 데코레이터(Decorator) 패턴이란?\u003c/h2\u003e\n\u003cp\u003e중심이 되는 객체가 있고, 장식이 되는 기능을 하나씩 추가하여 목적에 더 맞는 객체로 만들어가는 디자인 패턴을 Decorator 패턴이라고 한다. decorator 란 \u0026quot;장식하는 사람\u0026quot;이라는 뜻이다. 예제를 통해 상세 개념을 확인해 보자. (Java언어로 배우는 디자인 패턴 입문, 3편의 예제를 그대로 사용하였다.) 적용할 예제는, 문자열 주위에 \u0026quot;장식틀\u0026quot;을 붙여 표현하는 것이다. 예를 들어 Hello World라는 기본 문구에 장식틀을 붙여 중첩 장식을 한 후 출력하는 예제이다.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-asciidoc\" data-lang=\"asciidoc\"\u003e+=============+\n|Hello, world|\n+=============+\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e사용될 클래스는 각각 다음과 같다.\u003c/p\u003e","title":"[디자인패턴] 데코레이터(Decorator) 패턴의 개념, 예제, 장단점, 활용"},{"content":" 1. PostreSQL의 물리적 한계치 물론 사용 가능한 disk 용량, 성능 이슈 등 실직적인 제한이 먼저 적용되겠지만, 모든 자원이 충분하다고 가정할 때 물리적인 limit이다.\n항목 최대치 데이터베이스 사이즈 무제한 데이터베이스 수량 4,294,950,911 데이터베이스 당 Relations(테이블, 뷰, 인덱스 등의 테이블 객체)수량 1,431,650,303 Relations 사이즈 32TB 테이블 당 ROW 수량 4,294,967,295 pages영역의 크기에 해당하는 ROW 테이블 당 COL 수량 1,600 결과셋의 COL 수량 1,664 COL 사이즈 1GB 테이블 당 인덱스 수량 무제한 인덱스 당 컬럼 32 파티션 키 32 식별자 키 (ex, 테이블, ROW, COL 등의 명칭) 63 bytes function의 매개변수 100 쿼리파라미터 65,535 테이블당 ROW 수량은 4,294,967,295개의 pages 영역에 저장 가능한 ROWS로 제한되어 있는데, 4,294,967,295는 2^32 - 1로 32비트 시스템에서 사용가능한 최대 정수이다. 데이터베이스에서 최대로 관리할 수 있는 pages의 수며, 각 페이지에는 여러 튜플이 저장될 수 있다. 테이블 당 인덱스의 수량은 이론상은 \u0026ldquo;무제한\u0026quot;이제만, 실제로는 데이터베이스가 관리할 수 있는 최대 Relations (테이블, 뷰, 인덱스 등의 테이블 객체)에 의해 제한된다. 위 표의 인덱스 당 칼럼 수, 파티션 키 수량, 식별자 키, 함수 매개변수 최대 수량은 기본값이며 설정값을 변경하여 증가시킬 수 있다. 테이블 당 최대 칼럼 수는 1600개이지만, 저장되는 튜플이 8192바이트의 힙 페이지에 fit 해야 한다는 조건 때문에 더 줄어들 수 있다. 예를 들어 튜플 헤더를 제외하고, 1600개의 int칼럼 투플 - 6400 bytes로 힙페이지에 정상 저장 가능 (6400 \u0026lt; 8192) 1600의 bigint칼럼 투플 - 12800 bytes로 heap page를 초과 (12800 \u0026lt; 8192) text, varchar, char같이 길이 변경이 가능한 필드의 경우 값이 크면 TOAST 테이블 영역이라 불리는 주 저장공간 외부영역에 값을 저장하고, 본래 테이블에는 해당 데이터를 가리키는 포인터만 남게 된다. 테이블에서 삭제된 칼럼들도 최대 칼럼 개수에 포함된다. 삭제된 칼럼에 대해 새로 생성된 ROW도 내부적으로는 null 표시되지만, 추적을 위해 여전히 공간을 차지하여 최대 개수에 영향을 준다. 2. 결론 운영 단계에서 1억 개 이상의 테이블을 생성하거나 1000개가 넘는 칼럼의 테이블을 생성하는 일은 없을 것이고, 이러한 물리적 제약보다 자원의 한계 (용량 및 성능이슈)를 먼저 만날 것이기에 정확한 수치를 정확히 외울 필요는 없겠지만, 삭제된 칼럼들과 그 이후 생성된 ROW들이 내부적으로는 추적을 위해 해당 컬럼을 NULL로 저장하며, 이 과정에서 사용되는 NULL비트맵이 공간을 차지하기에 최대 카운트에 영향을 준다는 것 운영 시에 유의해야 할 내용이다.\n","permalink":"http://localhost:50666/posts/72/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/72/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"postresql의-물리적-한계치\" ke-size=\"size26\"\u003e1. PostreSQL의 물리적 한계치\u003c/h2\u003e\n\u003cp\u003e물론 사용 가능한 disk 용량, 성능 이슈 등 실직적인 제한이 먼저 적용되겠지만, 모든 자원이 충분하다고 가정할 때 물리적인 limit이다.\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e항목\u003c/th\u003e\n          \u003cth\u003e최대치\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e데이터베이스 사이즈\u003c/td\u003e\n          \u003ctd\u003e무제한\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e데이터베이스 수량\u003c/td\u003e\n          \u003ctd\u003e4,294,950,911\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e데이터베이스 당 Relations(테이블, 뷰, 인덱스 등의 테이블 객체)수량 \u003c/td\u003e\n          \u003ctd\u003e1,431,650,303\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eRelations 사이즈\u003c/td\u003e\n          \u003ctd\u003e32TB \u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e테이블 당 ROW 수량\u003c/td\u003e\n          \u003ctd\u003e4,294,967,295 pages영역의 크기에 해당하는 ROW\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e테이블 당 COL 수량\u003c/td\u003e\n          \u003ctd\u003e1,600\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e결과셋의 COL 수량\u003c/td\u003e\n          \u003ctd\u003e1,664\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCOL 사이즈\u003c/td\u003e\n          \u003ctd\u003e1GB\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e테이블 당 인덱스 수량\u003c/td\u003e\n          \u003ctd\u003e무제한\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e인덱스 당 컬럼\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e파티션 키\u003c/td\u003e\n          \u003ctd\u003e32\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e식별자 키 (ex, 테이블, ROW, COL 등의 명칭)\u003c/td\u003e\n          \u003ctd\u003e63 bytes\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003efunction의 매개변수\u003c/td\u003e\n          \u003ctd\u003e100\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e쿼리파라미터\u003c/td\u003e\n          \u003ctd\u003e65,535\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e \u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cul\u003e\n\u003cli\u003e테이블당 ROW 수량은 4,294,967,295개의 pages 영역에 저장 가능한 ROWS로 제한되어 있는데, 4,294,967,295는 2^32 - 1로 32비트 시스템에서 사용가능한 최대 정수이다.  데이터베이스에서 최대로 관리할 수 있는 pages의 수며, 각 페이지에는 여러 튜플이 저장될 수 있다.\u003c/li\u003e\n\u003cli\u003e테이블 당 인덱스의 수량은 이론상은 \u0026ldquo;무제한\u0026quot;이제만, 실제로는 데이터베이스가 관리할 수 있는 최대 Relations (테이블, 뷰, 인덱스 등의 테이블 객체)에 의해 제한된다. \u003c/li\u003e\n\u003cli\u003e위 표의 인덱스 당 칼럼 수, 파티션 키 수량, 식별자 키, 함수 매개변수 최대 수량은 기본값이며 설정값을 변경하여 증가시킬 수 있다.\u003c/li\u003e\n\u003cli\u003e테이블 당 최대 칼럼 수는 1600개이지만, 저장되는 튜플이 8192바이트의 힙 페이지에 fit 해야 한다는 조건 때문에 더 줄어들 수 있다. 예를 들어 튜플 헤더를 제외하고,\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e1600개의 int칼럼 투플 -\u003c/strong\u003e 6400 bytes로 힙페이지에 정상 저장 가능 (6400 \u0026lt; 8192)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e1600의 bigint칼럼 투플 -\u003c/strong\u003e 12800 bytes로 heap page를 초과 (12800 \u0026lt; 8192)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003etext, varchar, char같이 길이 변경이 가능한 필드의 경우 값이 크면 TOAST 테이블 영역이라 불리는 주 저장공간 외부영역에 값을 저장하고, 본래 테이블에는 해당 데이터를 가리키는 포인터만 남게 된다. \u003c/li\u003e\n\u003cli\u003e테이블에서 삭제된 칼럼들도 최대 칼럼 개수에 포함된다. \u003c/li\u003e\n\u003cli\u003e삭제된 칼럼에 대해 새로 생성된 ROW도 내부적으로는 null 표시되지만, 추적을 위해 여전히 공간을 차지하여 최대 개수에 영향을 준다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"결론\" ke-size=\"size26\"\u003e2. 결론\u003c/h2\u003e\n\u003cp\u003e운영 단계에서 1억 개 이상의 테이블을 생성하거나 1000개가 넘는 칼럼의 테이블을 생성하는 일은 없을 것이고, 이러한 물리적 제약보다 자원의 한계 (용량 및 성능이슈)를 먼저 만날 것이기에 정확한 수치를 정확히 외울 필요는 없겠지만, 삭제된 칼럼들과 그 이후 생성된 ROW들이 내부적으로는 추적을 위해 해당 컬럼을 NULL로 저장하며, 이 과정에서 사용되는 NULL비트맵이 공간을 차지하기에 최대 카운트에 영향을 준다는 것 운영 시에 유의해야 할 내용이다.\u003c/p\u003e","title":"[PostgreSQL] PostgreSQL의 물리적 한계치"},{"content":" 1. Planner / Optimizer (플래너 / 옵티마이저) 란? Planner / Optimizer는 쿼리의 최적화된 실행 계획을 만들어낸다. 한 개의 SQL 쿼리도 결과는 같지만, 다양한 방법과 순서로 실행될 수 있다. Planner / Optimizer (이후는 Planner로 명칭)는 실행 가능한 각각의 플랜을 검사하여 궁극적으로 가장 빠르게 실행 \u0026quot;기대\u0026quot;되는 플랜을 선택한다. 1-1. Genetic Query Optimize 가끔 쿼리의 실행 가능한 방식들을 검사하는 데만도 굉장히 큰 시간과 메모리가 소모된다. (특히, 많은 양의 join을 포함한 쿼리를 실행할 때 발생) 합리적인 쿼리 플랜(최고일 필요는 없다)을 합리적인 시간 내에 찾기 위해, PostgreSQL은 join수량이 임계치를 초과하면 Genetic Query Optimizer를 사용한다. Genetic Query Optimize는 최적의 조인 순서를 찾기 위해, 일부 조인 순서를 시도 후 적합성 함수를 통해 조인 순서의 적합성을 평가하여 최적의 plan을 선택한다. 메모리와 시간을 절약할 수 있지만, 항상 최선의 답을 찾는 것은 아니기에 효율성을 보장하지 못한다. 플래너의 검색 과정은 paths라고 불리는 데이터 구조를 활용한다. 이는 Planner가 결정을 내리는데 필요한 정보만을 포함하는 간소화된 표현이다. 최적의 paths가 결정되면, 실행기에 전달하기 위한 plan tree가 구축된다. 이 plan tree는 실행기가 쿼리를 실행할 수 있을 만큼 충분한 세부 정보를 가진 실행 계획을 나타낸다. 2. 실행가능한 Plan 조회 Planner는 쿼리에서 사용되는 각각의 테이블에 대한 스캔 계획을 생성하면서 시작한다. 가능한 Plan들은 각 테이블의 인덱스에 의해 결정된다. 먼저, 어떤 조건, 구조던지 테이블에 대한 순차적 스캔 (sequential scan)은 언제나 가능하기에, 순차적 스캔 플랜은 무조건 실행된다. 그다음으로는, 테이블에 인덱스가 정의(ex. B-tree index) 되어 있고, 조건절에 상수 연산자를 포함(ex, attribute \u0026gt; 50000)하고 있다고 가정해 보자. 만약 \b조건절의 칼럼이 B-tree인덱스 키와 일치하고, 연산자가 인덱스가 지원하는 비교연산자 중 하나(\u0026lt;,\u0026gt;,= 등)인 경우, B-tree 인덱스를 사용하여 테이블을 스캔하는 또 다른 계획이 실행된다. 추가적인 인덱스가 존재하고, 쿼리의 조건절이 인덱스 키와 일치하는 경우, 추가적인 플랜들이 고려된다. 쿼리의 ORDER BY 절이나 병합 조인에 유용하게 쓰일 수 있는 정렬순서를 가진 인덱스에 대해서도 인덱스 스캔 플랜이 생성된다. 쿼리가 2개 이상의 테이블을 조인할 경우, 1개의 테이블을 스캔하는 모든 실행계획을 찾은 이후에, 관계를 조인하기 위한 plan이 계획된다. 다음은 테이블을 조인할 때 데이터에 접근하는 3가지 방식이다.\nDriving table - JOIN 할 때 먼저 접근되어 PATH를 주도하는 테이블 (아래 설명에서는 선행 테이블) Driven table - 플래너에 의해 Driving 테이블이 결정된 후 나머지 테이블 (아래 설명에서는 후행 테이블) 2-1. Nested loop join 후행 테이블이 선행테이블의 모든 열에 대해 스캔을 한다. 간단한 방식이지만 시간이 많이 든다. (하지만 후행 테이블이 인덱스 스캔에 의해 스캔될 수 있다면 매우 좋은 전략이 될 수 있다. 각 ROW를 스캔할 때 선행테이블의 각 키를 후행 테이블의 인덱스 키로 사용할 수 있기 때문)\n2-2. Merge join 각 테이블이 조인이 시작되기 전에 조인 속성에 따라 각각 정렬된다. 그 후 두 테이블을 병렬로 스캔하고 일치하는 row들이 결합되어 JOIN ROWS를 생성한다. 각 테이블이 1번만 스캔되면 되기에 매력적인 방식이다. 머지 조인의 경우, 조인이 실행되기 전 정렬 과정에서 두 테이블은 조인 키 기준으로 정렬되어있어야 하는데, 다음 두 가지 방법으로 이루어질 수 있다.\n명시적인 정렬 단계 - 데이터 베이스가 직접 데이터를 조인 키에 따라 정렬하는 작업 수행 인덱스를 사용한 스캔 - 이미 조인 키에 대해 인덱스가 구성되어 있을 경우, 인덱스를 사용하여 데이터를 정렬된 순서대로 접근하고 스캔한다. 추가적인 정렬과정 없이도 데이터를 정렬된 상태로 처리할 수 있게 해 준다. 2-3. Hash join 후행 테이블이 먼저 스캔되고 join 되는 칼럼을 hash key로 사용한 hash table 형태로 저장된다. 선행 테이블을 스캔하며 join 칼럼을 hash key로 사용하여 후행 테이블에서 생성된 해쉬 테이블 내에서 해당하는 값을 빠르게 찾은 후 두 테이블 간의 일치하는 ROWS를 결합한다. 두 테이블 간의 조인을 효율적으로 수행할 수 있게 해 주며 대량 데이터 처리에 유용하다. (각 조인 방식에 대한 개념은 다시 정리할 예정) 쿼리가 두 개 이상의 테이블을 포함할 경우, 최종 결과는 한 번에 연산되는 것이 아니라, 각 조인 단계에서 2개의 테이블을 조인하며, 단계별로 조인된 결과를 트리 구조를 이루며 최종 결과를 단계적으로 구축해 간다. Planner는 가능한 서로 다른 Join 순서를 확인하여 가장 빠른 경우의 수를 찾는다. 쿼리가 임계치 보다 적은 수의 테이블을 참조한다면, 최선의 조인순서를 찾기 위해 거의 모든 경우를 탐색한다. Planner는 조건 절에 직접적인 조인 조건이 존재할 경우 해당 조건의 테이블 간의 조인을 우선적으로 고려한다. 반면에 서로 직접적인 조인 조건이 없는 테이블 간에는 선택지가 없을 경우에만 수행된다. 이러한 순서로 join 조합을 찾으며 거의 모든 plan을 비교하여 가장 코스트가 낮은 것으로 예상되는 실행계획을 선택한다. 반면에 조인되는 테이블 수가 임계치를 넘을 경우, 위에서 설명한 Genetic Query Optimize 방식을 사용한다. Paths에 의해 완성된 Plan tree에는 각 테이블들의 순서와 인덱스 스캔 정보, 조인 방식 (nested-loop, merge, hash 조인 노드), 정렬이나 집계함수 노드 같은 보조 단계들로 구성되어 있고 각 plan node 들은 다음과 같은 2가지 기능을 가지고 있다.\nselection - 특정 조건에 부합하지 않는 rows들을 삭제 projection - 지정된 칼럼 값에서 파생된 새로운 칼럼의 연산 (ex, 칼럼 A, B의 합, 칼럼 C에 10배를 한 값 등) Planner는 WHERE 조건절과 부합하는 selection조건과 결과 값과 일치하는 projection을 연결하여 최적의 노드를 plan tree에서 찾게 해 준다. 참고\nhttps://www.postgresql.org/docs/16/planner-optimizer.html\n","permalink":"http://localhost:50666/posts/71/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/71/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"planner-optimizer-플래너-옵티마이저-란\" ke-size=\"size26\"\u003e1. Planner / Optimizer (플래너 / 옵티마이저) 란?\u003c/h2\u003e\n\u003cp\u003ePlanner / Optimizer는 쿼리의 최적화된 실행 계획을 만들어낸다. 한 개의 SQL 쿼리도 결과는 같지만, 다양한 방법과 순서로 실행될 수 있다. Planner / Optimizer (이후는 Planner로 명칭)는 실행 가능한 각각의 플랜을 검사하여 궁극적으로 가장 빠르게 실행 \u0026quot;기대\u0026quot;되는 플랜을 선택한다. \u003c/p\u003e\n\u003ch3 id=\"genetic-query-optimize\" ke-size=\"size23\"\u003e1-1. Genetic Query Optimize\u003c/h3\u003e\n\u003cp\u003e가끔 쿼리의 실행 가능한 방식들을 검사하는 데만도 굉장히 큰 시간과 메모리가 소모된다. (특히, 많은 양의 join을 포함한 쿼리를 실행할 때 발생) 합리적인 쿼리 플랜(최고일 필요는 없다)을 합리적인 시간 내에 찾기 위해, PostgreSQL은 join수량이 임계치를 초과하면 Genetic Query Optimizer를 사용한다. Genetic Query Optimize는 최적의 조인 순서를 찾기 위해, 일부 조인 순서를 시도 후 적합성 함수를 통해 조인 순서의 적합성을 평가하여 최적의 plan을 선택한다. 메모리와 시간을 절약할 수 있지만, 항상 최선의 답을 찾는 것은 아니기에 효율성을 보장하지 못한다.\n \u003c/p\u003e","title":"[PostgreSQL] Planner, Optimizer (플래너, 옵티마이저)란? Planner, Optimizer (플래너, 옵티마이저)의 개념과 작동방식"},{"content":" 1. Index-Only Scans PostgreSQL의 모든 인덱스는 \u0026quot;보조(Secondary)\u0026quot; 인덱스이다. 각 인덱스는 테이블의 메인 데이터 영역(테이블의 heap 영역)과 분리되어서 저장된다. 그렇기 때문에 일반적인 인덱스 스캔에서 각 ROW를 찾기 위해서는, index와 heap 영역 모두에 접근하여 데이터를 탐색해야 한다. 보통 WHERE 절 조건에 부합하는 데이터들은\n인덱스 영역 - 서로 가까이 존재하여 정렬된 순서로 빠르게 접근할 수 있다. (인덱스 테이블은 정렬된 상태로 생성) heap 영역 - 특별한 규칙 없이 어디에서든 분포할 수 있기에 heap 영역을 스캔할 때는 무작위로 접근하게 되어 속도가 느리다. 이 퍼포먼스 문제를 해결하기 위해 PostgreSQL은 힙 영역에 대한 접근 없이 인덱스 내에서만 데이터를 조회하는 Index-only 스캔을 지원한다. 기본 개념은 말 그대로 heap 영역의 참조 없이 index 항목에서 바로 값을 반환하는 것으로 매우 효율적으로 보이지만 몇 가지 제한사항이 있다.\n칼럼에 적용된 인덱스 유형이 Index-only 스캔을 지원해야 한다. (B-tree인덱스는 언제나 지원하고, GiST, SP-GiST는 특정 연산자에 한해서만 지원, 나머지 3가지 인덱스는 지원하지 않는다.) 인덱스가 각 항목에 대해 원래 데이터 값을 물리적으로 온전히 저장하거나 재구성할 수 있어야 한다. 예를 들어 GIN인덱스가 Index-only 스캔을 지원하지 않는 이유는 각 인덱스가 실제 데이터의 물리적인 값이 아닌 일부 특징 (ex. 최대, 최솟값)만을 가지고 있는 유형의 인덱스 이기 때문이다. 실행되는 쿼리가 인덱스로 설정된 칼럼만을 조건절에 참조해야 한다. 예를 들어 x, y칼럼에 인덱스가 설정되어 있고, z 칼럼에 인덱스가 설정되어 있지 않다면, 다음 쿼리는 index-only 스캔을 사용할 수 있다. SELECT x, y FROM tab WHERE x = \u0026#39;key\u0026#39;; SELECT x FROM tab WHERE x = \u0026#39;key\u0026#39; AND y \u0026lt; 42; 반면에 다음 쿼리는 인덱스가 적용되지 않은 z 칼럼이 조건절, 혹은 target에 포함되어 있기에 index-only 스캔 사용이 불가능하다.\nSELECT x, z FROM tab WHERE x = \u0026#39;key\u0026#39;; SELECT x FROM tab WHERE x = \u0026#39;key\u0026#39; AND z \u0026lt; 42; 이 조건들에 부합하면, 쿼리의 결과에 해당하는 데이터가 인덱스 영역 내에 모두 존재한다는 것이기에 Index-only 스캔이 가능하다. 2. Visiblity 위의 조건에 부합하여 Index-only 스캔이 가능하더라도, 효율적인 스캔을 위해 PostgreSQL에서는 테이블 검색에 대한 추가 요구 사항이 있다. 바로 검색 결과의 각 ROW가 쿼리의 MVCC 스냅숏에 \u0026quot;보이는지(visible)\u0026quot;이다. MVCC는 PostgreSQL에서 동시성을 지원하는 원칙이며 (상세 내용은 다음 링크 참고 -\n[PostgreSQL] MVCC (Multi-Version Concurrency Control))\nVisiblity - 해당 쿼리가 실행되는 시점에서 데이터베이스의 상태, 혹은 스냅숏에 따라 해당 쿼리에 의해 결과 데이터가 조회, 혹은 조작될 수 있는지의 의미이다. (다른 트랜잭션에 의해 삭제되거나 수정되지 않은 유효한 데이터가 반환될 수 있는지를 의미한다.)\nPostgreSQL은 테이블 heap의 각 페이지에 대해 모든 ROWS가 현재, 미래의 모든 트랜잭션에서 볼 수 있을 만큼 오래되었는지 여부를 추적한하여 이 정보를 테이블의 Visiblity map에 bit 형태로 저장한다. 하지만 설명을 보면 Visiblity 정보는 인덱스 영역이 아닌 heap영역에만 저장된다. 그렇다면 Visiblity를 조회하기 위해서는 heap영역을 무조건 조회해야 하는 것이 아닌가?라는 의문이 들것이다. 테이블 ROWS가 최근에 변경된 경우에는 인덱스 테이블이 업데이트되기 전일테니 당연히 매번 heap 영역을 조회해야 하지만, 변경이 잦지 않은 데이터의 경우 이 문제를 해결할 수 있는 방법이 있다. Index-only 스캔은 인덱스 내에서 데이터를 찾은 후 그에 해당하는 Visiblity map bit만을 확인하여 heap영역의 해당 page를 찾는다.\n데이터가 확인되면 - 물리적인 데이터를 찾을 수 있기에 추가적인 작업은 필요 없다. 데이터가 확인되지 않는다면 - 데이터의 Visiblity를 확인을 위해 heap 영역을 접근해야 하기에 표준 인덱스 스캔보다 성능이 향상되지 않는다. 성공적으로 visiblity map bit를 찾더라도 이 방식은 Visiblity map을 조회해야 한다. 하지만 visiblity map 은 heap 보다 4배 적기 때문에 물리적 I/O가 훨씬 적게 든다. 그리고 대부분의 상황에 Visiblity map은 메모리에 캐시 된 상태로 유지되기에 효율적인 조회가 가능하다. 요약하자면, Index-only 스캔은 기본적인 요구조건을 만족하면 사용은 가능하지만, heap 테이블의 각 pages의 대부분이 visiblity map을 가지고 있을 때 효율을 볼 수 있다. 3. Covering index Index-only 스캔을 효율적으로 사용하기 위해서는, 자주 사용하는 특정 쿼리의 ROW들을 포함하도록 특별히 설계된 인덱스인 커버링 인덱스(COVERING INDEX)를 생성하면 된다. 일반적으로 쿼리는 검색의 키로 사용되는 칼럼(WHERE 절의 칼럼) 외에도 훨씬 더 많은 칼럼을 조회(SELECT 하는 칼럼)하기에, PostgreSQL은 검색의 key로 사용되는 인덱스가 아닌 단순히 payload(부가 데이터) 일뿐인 인덱스를 생성할 수 있게 해 준다. (실제 검색되는 결과 칼럼을 조회하는 것이 아니기에 조회되는 칼럼에만 검색 키를 추가하고, 결과 칼럼은 부가적인 정보로만 가볍게 추가) 다음과 같이 INCLUDE와 검색되는 칼럼의 같이 부여하면 사용 가능하다. SELECT y FROM tab WHERE x = \u0026#39;key\u0026#39;; 일반적으로 위와 같은 쿼리를 실행시킨다고 했을 때, 기본적인 속도 개선의 방법은 x 칼럼에만 인덱스를 실행시키는 것이다. 하지만 다음과 같은 인덱스를 생성한다면, 해당 쿼리를 index-only 스캔방식으로 사용 가능하다.\nCREATE INDEX tab_x_y ON tab(x) INCLUDE (y); y의 결괏값들을 heap 영역에 접근하지 않고 가져올 수 있기 때문이다. Covering Index인 y는 검색 키는 아니기에, 인덱스가 처리할 수 있는 데이터 유형일 필요는 없다. 인덱스 테이블에 저장되어 있을 뿐이며 검색을 하는 데 사용되거나 내부 메커니즘에서 인덱스로 해석되지 않는다. CREATE UNIQUE INDEX tab_x_y ON tab(x) INCLUDE (y); 그리고 다음과 같이 유니크 인덱스로 설정한다면, 해당 유니크 특성은 x, y가 아닌 칼럼 x에만 부여된다. (INCLUDE 절은 UK, PK 제약조건에도 작성할 수 있다.) 인덱스영역을 차지하지만, 인덱스로 여겨지지 않고, 키에도 영향을 안주며 payload 형태로 저장만 한다면, 모든 쿼리를 등록하면 어떻게 되는 건가라는 의문이 드는데, Covering Index 사용 시 주의사항을 확인해 보자. 키가 아닌 칼럼(payload)을 인덱스, 특히 사이즈가 큰 칼럼에 대한 인덱스를 설정하는 것은 최대한 보수적으로 작업하는 것이 좋다. 인덱스 tuple이 인덱스 유형이 허락하는 최대 사이즈를 초과하면, 데이터 insert가 실패할 수 있다. 키가 아닌 칼럼(Payload)이 인덱스 테이블의 데이터를 복제하여 인덱스 크기를 증가시킬 수 있고 이는 검색 속도를 늦추는 현상은 언제든지 발생할 수 있다. 그리고 Index-only 스캔 시, 테이블 변경속도가 충분히 느려서 heap에 접근할 필요 없이 인덱스만으로 스캔할 가능성이 있지 않다면, 인덱스에 payload를 포함시키는 것은 의미가 없다는 것을 기억해야 한다. 4. Covering Index VS 일반 Index PostgreSQL이 INCLUDE를 지원하기 전에, 종종 페이로드 칼럼을 일반 인덱스 칼럼으로 작성하여 COVERING 인덱스를 생성하였다.\nCREATE INDEX tab_x_y ON tab(x, y); y칼럼을 조건절에 사용할 의도가 없었음에도 불구하고, \b타깃 칼럼(y)이 실제 조회 키 (x) 뒤에 존재한다면 index-only 스캔은 잘 작동하지만 타깃 칼럼(y)을 선행 인덱스로 생성하는 것을 효율적이지 않다. (비효율적이지만 Covering 인덱스와 같은 효과를 볼 수 있다.) COVERING 인덱스가 실제로 B-Tree 인덱스 구조에서 어떻게 작동하는\b 과정을 살펴보자\nSuffix Truncation(접미사 축소) - B-tree 인덱스 상위 레벨에서 non-key 칼럼(payload)을 제거하는 최적화 과정. 실제 검색에 필요한 키 칼럼 외에는 제거한다.\nNon-key 칼럼(payload) - 인덱스 스캔을 직접적으로 가이드하지 않고, 검색용으로 사용되지 않음 (결과와 무관) 축소 과정 - 상위 레벨에서 payload를 제거한 후, 남아있는 키칼럼의 prefix로도 최하 B-tree 튜플이 설명이 가능하다면 (조회할 수 있다면) 뒤따르는 키 칼럼을 조회할 필요가 없기에 후속 키 칼럼을 제거. 필수 정보만을 유지하며 인덱스를 통한 검색을 최적화** Covering Index, Include -** Include 없이 Covering 인덱스를 사용하는 경우, B-tree 인덱스의 최상위 레벨에서 Payload 칼럼을 저장하지 않는다. 상위 레벨에선 키 칼럼만 저장하고 있다. 쿼리가 인덱스에 포함되지 않은 칼럼 데이터를 요구하는 경우 추가적인 테이블 접근이 필요하다.\n커버링 인덱스와 payload가 B-tree 인덱스에서 어떻게 작동하는지를 확인한 후 Include 사용 여부에 따른 차이점을 확인해 보면 다음과 같다.\n일반 Index - Include 없이 Covering 인덱스를 사용하는 경우, B-tree 인덱스의 최상위 레벨에서 Payload 칼럼을 저장하지 않는다. 상위 레벨에선 키 칼럼만 저장하고 있다. 쿼리가 인덱스에 포함되지 않은 칼럼 데이터를 요구하는 경우 추가적인 테이블 접근이 필요하다. Covering Index(Include) - include의 경우 B-tree 인덱스의 최하위 leaf 레벨에 Payload 칼럼 정보를 저장하여, 키칼럼 외의 칼럼 데이터를 요구하는 경우에도 추가적인 테이블 접근 없이 데이터 조회가 가능하다. 5. 표현식(expresions)과 Index-only 스캔 원칙적으로 index-only 스캔은 expression 인덱스와 함께 사용할 수 있다. 예를 들어 x 칼럼에 인덱스가 설정되어 있다면 다음 쿼리도 index-only scan이 가능하다.\nSELECT f(x) FROM tab WHERE f(x) \u0026lt; 1; 만약 f() 함수가 고 코스트의 함수라면 매우 효과적인 쿼리가 될 것이다. 그러나 PostgreSQL 플래너는 이 경우에 그렇게까지 똑똑하게 Index-only 스캔을 적용하진 않는다. 플래너는 조건절의 모든 칼럼이 인덱스 테이블에서 사용이 가능할 때만 index-only 스캔을 고려하는데, 이 경우 f(x)라는 함수가 x의 칼럼만을 참조하고 있음에도, 직접적으로 x를 사용하지 않기에 플래너는 이를 알지 못하고 index scan을 할 수 없다는 결론만을 내린다. 이 경우 다음과 같이 x 자체를 include 칼럼에 추가하여 index-only스캔을 적용할 수 있다.\nCREATE INDEX tab_f_x ON tab (f(x)) INCLUDE (x); 추가적인 주의사항으로, 해당 인덱스를 적용 시 f(x) 함수의 재연산을 피하는 것이 목적이라면, 함수의 구성이 where절에 그대로 있지 않은 경우 주의해야 한다. f(x) 함수를 통해 데이터를 필터링하거나 계산하는 로직이 있을 경우, 그 로직이 where 절에 그대로 포함되지 않는다면 PostgreSQL 플래너는 인덱스를 적절하게 사용하지 않는다. 보통 위와 같은 단순 테이블 조회는 인덱스 설정으로 간단하게 해결할 수 있지만, join 절 등으로 다른 테이블을 참조하게 될 경우 해결할 수 없다. (PostgreSQL 이후 버전에서 해결 가능할 수도 있다고 함)\n6. 부분 인덱스 (Partial Index)와 Index-only 스캔 부분인덱스는 index-only 스캔과 흥미로운 상호작용을 한다. 다음과 같이 조건절에 success를 추가한 채로 인덱스를 설정하면\nCREATE UNIQUE INDEX tests_success_constraint ON tests (subject, target) WHERE success; 다음 쿼리에 index-only 스캔을 적용할 수 있다.\nSELECT target FROM tests WHERE subject = \u0026#39;some-subject\u0026#39; AND success; 하지만 where = success라는 조건은 인덱스의 결과 칼럼으로 그대로 사용할 수 없는데 어떻게 적용될 수 있을까? 결과 칼럼으로 그대로 사용할 수 없지만, 인덱스 자체에 조건을 걸었기에 인덱스에 있는 모든 항목은 success=true를 만족하게 된다. 이 경우 Planner는 런타임에서 해당 조건을 명시적으로 다시 확인할 필요가 없기에 Index-only 스캔이 가능한 것이다. (다만 9.6 이상 버전에서는 자동으로 해당 상황을 인지하고 Index-only스캔하지만, 이전 버전에서는 생성되지 않는다.)\n7. 결론 및 성능 테스트 이전 포스트의 ORDER_BY_INDEX_TEST 테이블을 다시 사용하여 성능 테스트를 진행하였다.\n테스트 테이블(order_by_index_test) [1857637 ROWS]\nid - PK name - VARCHAR not null non_null_varchar - NOT NULL VARCHAR(8) null_varchar - NULLABLE VARCHAR(8) 현재 **non_null_varchar **칼럼에만 인덱스를 설정한 상황이다.\nSELECT * FROM ORDER_BY_INDEX_TEST ORDER BY NOT_NULL_VARCHAR ASC NULLS LAST LIMIT 1000; 먼저 다음과 같이, 정렬 순서는 인덱스 효과를 볼 수 있게 설정하고, 결과 칼럼은 인덱스 외 전체 칼럼을 조회를 하면\n일반적인 INDEX SCAN을 타게 된다. 하지만 결과 칼럼 또한 인덱스 칼럼만을 조회하게 되면, SELECT NOT_NULL_VARCHAR FROM ORDER_BY_INDEX_TEST ORDER BY NOT_NULL_VARCHAR ASC NULLS LAST LIMIT 1000; INDEX ONLY SCAN을 타게 되며 TOTAL COST가 약 1/4 정도 줄어든다. (위에서 언급한 것처럼 Visiblity table이 약 1/4 크기) 그렇다면 인덱스 외 칼럼을 조회하면서 인덱스 효과를 보기 위해 INCLUDE를 사용한 COVERING 인덱스를 사용해 보자\nCREATE INDEX ORDER_BY_INDEX_TEST_NULL_VARCHAR_INDEX ON ORDER_BY_INDEX_TEST (NULL_VARCHAR) INCLUDE (NOT_NULL_VARCHAR) 다음과 같이 COVERING 인덱스를 추가하고\nSELECT NOT_NULL_VARCHAR, NULL_VARCHAR FROM ORDER_BY_INDEX_TEST ORDER BY NULL_VARCHAR ASC NULLS LAST LIMIT 1000; 조회하게 되면, 키 칼럼에 해당하는 NULL_VARCHAR 칼럼 외의 다른 칼럼도 SELECT 항목에 존재하지만 INDEX ONLY SCAN을 타면서 성능 효과를 볼 수 있다. 복잡하지 않은 테이블 조합에서, 조회되는 칼럼의 크기가 작고 명확하게 정의된 상태에서는 COVERING INDEX를 사용하는 것으로 엄청난 성능 향상을 볼 수 있는 것을 확인하였다. 다만 공식 문서의 주의사항 대로 남용할 경우 데이터 인서트시 실패하거나, 인덱스 사이즈가 커져 검색 성능저하가 언제든지 일어날 수 있기에, 인덱스 테이블의 사이즈 확인 및 쿼리 플랜 등을 정확히 확인하여 최대한 보수적으로 적용해야 한다.\n","permalink":"http://localhost:50666/posts/70/","summary":"\u003chr\u003e\n\u003ch2 id=\"1-index-only-scans\"\u003e1. Index-Only Scans\u003c/h2\u003e\n\u003cp\u003ePostgreSQL의 모든 인덱스는 \u003cstrong\u003e\u0026quot;보조(Secondary)\u0026quot;\u003c/strong\u003e 인덱스이다. 각 인덱스는 테이블의 메인 데이터 영역(테이블의 \u003cstrong\u003eheap\u003c/strong\u003e 영역)과 분리되어서 저장된다. 그렇기 때문에 일반적인 인덱스 스캔에서 각 ROW를 찾기 위해서는, index와 heap 영역 모두에 접근하여 데이터를 탐색해야 한다. 보통 WHERE 절 조건에 부합하는 데이터들은\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e인덱스 영역 -\u003c/strong\u003e 서로 가까이 존재하여 정렬된 순서로 빠르게 접근할 수 있다. (인덱스 테이블은 정렬된 상태로 생성)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eheap 영역 -\u003c/strong\u003e  특별한 규칙 없이 어디에서든 분포할 수 있기에 heap 영역을 스캔할 때는 무작위로 접근하게 되어 속도가 느리다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이 퍼포먼스 문제를 해결하기 위해 PostgreSQL은 힙 영역에 대한 접근 없이 인덱스 내에서만 데이터를 조회하는 \u003cstrong\u003eIndex-only\u003c/strong\u003e 스캔을 지원한다. 기본 개념은 말 그대로 heap 영역의 참조 없이 index 항목에서 바로 값을 반환하는 것으로 매우 효율적으로 보이지만 몇 가지 제한사항이 있다.\u003c/p\u003e","title":"[PostgreSQL] Index-Only 스캔과 Covering 인덱스, Index-only스캔의 효율적인 사용"},{"content":" 1. 인덱스(INDEX)와 오더바이(ORDER BY) 인덱스는 쿼리의 결과로 특정 row를 찾는 것뿐만 아니라, 특정 순서로 데이터를 정렬하는데도 효율적일 수 있다. ORDER BY와 인덱스를 효율적으로 사용하면 별도의 정렬 과정 없이 ORDER BY를 수행할 수 있다. PostgreSQL에서 현재 지원하는 인덱스 타입 중에서는 B-tree 인덱스만이 정렬 결과로 인덱스를 생성할 수 있다. 다른 인덱스 유형은 특정되지 않은 순서로, 실행 때마다 다른 순서로 열을 반환한다.\n* 상세한 B-tree 인덱스의 개념은 다음 글을 참고 - [Postgresql] - [PostgreSQL] B-tree 인덱스의 원리 및 특징\n플래너는 ORDER BY를 수행할 때 해당 조건에 맞는 사용 가능한 인덱스를 스캔 테이블을 물리적으로 스캔하여 명시적으로 정렬을 수행한 후 ORDER BY 사양에 충족하는 row를 스캔 (실제 테이블 스캔) 중 효율적인 스캔을 실행한다. 테이블의 많은 부분을 조회하는 쿼리의 경우, 명시적 조회가 인덱스를 조회하는 것보다 빠르다. (대량 데이터를 조회할 시에는 데이터를 순차적 접근 패턴이 디스크 I/O를 덜 필요로 하기 때문이다.) 이는 기본적인 인덱스의 특징과 동일하고, 적은 수의 row를 반환하는 경우에 더 유용하다. (선택도가 낮을수록 효율적, 10~15%) * 효율적인 인덱스 설계 및 작동 방식은 다음 글을 참고 - [Postgresql] - [PostgreSQL] 인덱스(INDEX) 개념 및 생성, 삭제, 분석, 설계 방법\n특히 LIMIT n과 ORDER BY를 결합하여 결과 값을 제한하는 경우가 인덱스 테이블의 사용이 효율적이다. 이 경우 명시적 조회는 첫 n개의 rows를 반환하기 위해 전체 데이터를 조회해야 하지만, 해당 ORDER BY와 일치하는 인덱스가 있다면 첫 n개의 row는 나머지 row를 조회할 것 없이 바로 출력된다. 기본적으로 B-tree인덱스는 오름차순(ASC)에 NULLS LAST로 정렬된 채로 데이터를 저장한다. (같은 순서의 경우 테이블의 TID를 기준으로 정렬) 그렇기 때문에, 칼럼 x의 인덱스를 일반적인 정방향 스캔을 할 경우, x칼럼의 오름차순([ORDER BY x ASC NULLS LAST])의 정렬과 동일한 결과가 출력된다. 정렬 인덱스는 정렬 설정된 방향의 역방향으로도 스캔될 수 있기 때문에 [ORDER BY x DESC NULLS FIRST]에 대한 정렬도 인덱스 스캔할 수 있다.\n2. ORDER BY 인덱스 생성 B-tree 인덱스를 생성할 때 ASC, DESC, NULLS FIRST, NULLS LAST 옵션을 부여하여 정렬을 조정할 수 있다.\nCREATE INDEX test2_info_nulls_low ON test2 (info NULLS FIRST); CREATE INDEX test3_desc_index ON test3 (id DESC NULLS LAST); 칼럼 x에 대해 ASC NULLS FIRST로 저장된 인덱스는 스캔 방향에 따라 x ASC NULLS FIRST 혹은 x DESC NULLS LAST의 쿼리에 효과를 볼 수 있다는 것인데, 여기까지 보면 정방향, 역방향 (역스캔 가능한) 옵션이 ORDER BY 모든 변형을 포함할 수 있다. ORDER BY의 모든 변형 4가지를 살펴보면 다음과 같은데\n1. ASC NULLS FIRST\n2. DESC NULLS LAST 3. ASC NULLS LAST 4. DESC NULLS FIRST\n1,2번과 3,4 번은 서로의 역방향에서 조회가 가능하기에 인덱스를 각각 한 개씩(1번 3번, 2개) 적용해도 네 개를 개별적용 하는 것과 같은 효과를 볼 수 있다. 그렇다면 4가지 적용 방식이 아닌 2가지 적용방식으로도 충분히 ORDER BY 인덱스를 컨트롤할 수 있을 텐데, 왜 4가지 적용 방식이 존재할까? 단일 칼럼의 인덱스를 설정하기에는 해당 옵션이 충분하지만, 복합 칼럼 (Multi-column) 인덱스를 설정할 때는 옵션의 다양성이 유리할 수 있다. 예를 들어 x, y칼럼에 인덱스가 있다고 가정해 보면, 정방향 스캔은 [ORDER BY x, y] 혹은 역방향은 [ORDER BY x DESC, y DESC] 가 될 것이다. 하지만 만약 [ORDER BY x ASC, y DESC]의 조회가 애플리케이션에서 자주 일어날 수 있다면 기존 일반 인덱스로는 해당 순서 조회를 할 수 없고, 인덱스가 [x ASC, y DESC] 혹은 [x DESC, y ASC]로의 인덱스 조회가 가능할 것이다. PostgreSQL 공식문서에 의하면, 디폴트 정렬에 맞지 않는 ORDER BY 인덱스는 상당히 전문화된 기능이지만 특정 쿼리에 한해서 굉장히 큰 효율을 보일 수 있다. 특정 순서로 정렬된 상태의 조회가 얼마나 자주 일어나냐에 따라 인덱스를 사용여부를 판단하면 하면 될 것이다.\n3. 성능 비교 및 결론 기존에 ORDER BY 자체를 인덱싱에 포함시켜 정렬된 자체로 인덱스 테이블을 생성할 수 있는지는 알고 있었다. 하지만\n동일 인덱스 테이블을 역방향 조회할 때도 인덱스의 효과를 볼 수 있는지는 인지하지 못하였다. (생각해 보면 정방향으로 정렬된 인덱스테이블의 마지막 부분을 역방향으로 조회하면 동일한 효과를 보는 건 인덱스의 당연한 개념이고 다른 부분에도 그렇게 적용 사용 중이 많았음에도 불구하고) ASC NULLS LAST \u0026lt;=\u0026gt; DESC NULLS FIRST\n칼럼 자체에 인덱스 설정 시 디폴트가 ASC NULLS LAST, DESC NULLS LAST인지 알지 못하였다. 그래서 역방향, 정방향 인덱스를 둘 다 생성해 보고, 해당 칼럼자체의 인덱스도 별도로 생성하여 테스트해 보니 다른 인덱스의 효율을 떨어 트리는 상황이 발생하여서 인덱스 튜닝 시점에 롤백을 했었었다. 정확한 영향도 파악과 성능 비교를 위해 ORDER BY 인덱스를 부여 시 정확히 어떤 효과를 보는지 테스트를 진행하였다.\n테스트 테이블(order_by_index_test) [1857637 ROWS]\nid - PK\nname - VARCHAR not null\nnon_null_varchar - NOT NULL VARCHAR(8)\nnull_varchar - NULLABLE VARCHAR(8)\n* 쿼리 성능 분석하기에 대한 상세 내용은 다음 포스트를 참고\n[PostgreSQL] 쿼리 성능향상 (실행계획 보는 법, 상세 확인방법, Explain의 어떤 지표를 봐야 할까?)\n3-1. 단일 인덱스 칼럼 정방향 ORDER BY id 칼럼에 PK만 부여된 상태 (인덱스 부여)로 ORDER BY 정렬하면 기본 정렬인 [ORDER BY id ASC NULLS LAST, ORDER BY id DESC NULLS FIRST]에서만 인덱스 효과를 볼 수 있다.\nSELECT * FROM ORDER_BY_INDEX_TEST ORDER BY ID ASC NULLS LAST LIMIT 1000; SELECT * FROM ORDER_BY_INDEX_TEST ORDER BY ID DESC NULLS FIRST LIMIT 1000; 하지만 반대 정렬의([ASC NULLST FIRST, DESC NULLS LAST]) 경우 인덱스의 효과를 볼 수 없다.\nSELECT * FROM ORDER_BY_INDEX_TEST ORDER BY ID ASC NULLS FIRST LIMIT 1000; SELECT * FROM ORDER_BY_INDEX_TEST ORDER BY ID DESC NULLS LAST LIMIT 1000; 3-2. NOT NULL 속성 칼럼에 기본 인덱스 부여 NOT NULL 인 칼럼에 기본 인덱스를 부여할 경우 [ORDER BY id ASC NULLS LAST, ORDER BY id DESC NULLS FIRST](동일 순서가 없다면 결과는 같다.)의 효과를 동시에 볼 수 있을까?\nCREATE INDEX ORDER_BY_INDEX_TEST_NOT_NULL_VARCHAR_INDEX ON ORDER_BY_INDEX_TEST (NOT_NULL_VARCHAR); 기본 정렬에 대해서는 여전히 인덱스를 타지만,\nSELECT * FROM ORDER_BY_INDEX_TEST ORDER BY NOT_NULL_VARCHAR ASC NULLS LAST LIMIT 1000; SELECT * FROM ORDER_BY_INDEX_TEST ORDER BY NOT_NULL_VARCHAR DESC NULLS FIRST LIMIT 1000; 반대 정렬은 여전히 인덱스의 효과를 볼 수 없다.\nSELECT * FROM ORDER_BY_INDEX_TEST ORDER BY NOT_NULL_VARCHAR ASC NULLS FIRST LIMIT 1000; SELECT * FROM ORDER_BY_INDEX_TEST ORDER BY NOT_NULL_VARCHAR DESC NULLS LAST LIMIT 1000; -\u0026gt; 칼럼 자체에 NULL 데이터가 없어서 NULLS FIRST, NULLS LAST의 결과가 같더라도, 인덱스 테이블은 별도로 생성되며 반대로 정렬 시 인덱스의 효과를 볼 수 없다.\n3-3. 기본 정렬 인덱스와, 역방향 정렬 인덱스를 동시에 부여 기본 정렬과 역방향 정렬의 인덱스를 동시에 부여할 시 성능차이가 어떻게 될까?\nCREATE INDEX ORDER_BY_INDEX_TEST_NOT_NULL_VARCHAR_ASC_FIRST_INDEX ON ORDER_BY_INDEX_TEST (NOT_NULL_VARCHAR ASC NULLS FIRST); 역방향으로 인덱스를 설정하고 다시 조회를 할 경우,\nSELECT * FROM ORDER_BY_INDEX_TEST ORDER BY NOT_NULL_VARCHAR ASC NULLS FIRST LIMIT 1000; SELECT * FROM ORDER_BY_INDEX_TEST ORDER BY NOT_NULL_VARCHAR DESC NULLS LAST LIMIT 1000; 역방향은 인덱스 효과를 볼 수 있고,\nSELECT * FROM ORDER_BY_INDEX_TEST ORDER BY NOT_NULL_VARCHAR ASC NULLS LAST LIMIT 1000; SELECT * FROM ORDER_BY_INDEX_TEST ORDER BY NOT_NULL_VARCHAR DESC NULLS FIRST LIMIT 1000; 기존 정방향도 인덱스 효과를 볼 수 있다.\n하지만 지금 테이블 같은 경우는 간단한 구조의 단일 테이블 조회이고 캐시나 다른 영향도 있을 수 있다. 예전에 도입 검토 했을 당시, 복잡한 대량 데이터의 쿼리를 기준으로 적용할 시(해당 시점에 120만 건 x 500만 건 x 1200만 건 + 1:1의 데이터 조회였던 걸로 기억한다.), 양방향 인덱스를 모두 설정할 경우, 플랜이 변경되어 기존 인덱스가 적용되지 않거나, 둘 중 코스트가 낮은 한 방향 인덱스만 적용되는 현상이 있었다. 플래너가 한정적인 자원에서 최상의 인덱싱 효과를 위해 실행계획을 짜는 만큼 복잡한 구조에서는 실제로 의도한 방향으로 쿼리가 플래닝 되는지 꼭 확인해보아야 할 듯하다. 해당 테스트에는, 양방향으로도 인덱스 설정이 가능하다 정도를 확인하면 좋을 듯하다. 3-4. NULLABLE 테이블에서, NOT NULL 조건과 함께 정렬 NULLABLE 테이블에서 NOT NULL 조건을 줄 경우 NULLS LAST와 NULLS FIRST의 결과가 동일할 텐데(동순위가 없을 경우), 한 방향 인덱스로 해결할 수 있지 않을까? (2번의 NOT NULL 칼럼과 동일)\n보통 정렬 기능을 제공할 때 단방향만 제공하는 경우는 잘 없고 산군의 경우 ASC NULLS LAST, DESC NULLS LAST와 같이 NULL 데이터를 배제한 상태로 조회하는 경우가 많기에 성능을 비교해 보았다. CREATE INDEX ORDER_BY_INDEX_TEST_NULL_VARCHAR_INDEX ON ORDER_BY_INDEX_TEST (NULL_VARCHAR); 다음과 같이 정방향으로 인덱스를 설정하고 해당 칼럼에 NOT NULL 조건을 부여한 후 조회를 시도하면\nSELECT * FROM ORDER_BY_INDEX_TEST WHERE NULL_VARCHAR IS NOT NULL ORDER BY NULL_VARCHAR ASC NULLS FIRST LIMIT 1000; SELECT * FROM ORDER_BY_INDEX_TEST WHERE NULL_VARCHAR IS NOT NULL ORDER BY NULL_VARCHAR DESC NULLS LAST LIMIT 1000; 역방향은 NOT NULL 여부에 상관없이 역방향 인덱스의 효과를 볼 수 없다.\n3-5. 결론 및 적용 검토 PostgreSQL 공식 문서의 설명대로, 단순히 인덱스 자체에 순서를 붙이는 것이 아닌 훨씬 고도화된 기능인듯하다. 위에서 설명한 것처럼 일전에 테스트 시 복잡한 쿼리의 경우 역방향 인덱스를 거는 것만으로 기존 플랜에 영향을 주는 것을 확인하였다. 단방향으로만 조회하는 것이 확실한 상황에서는 상당한 성능개선이 있겠지만, 양방향 조회나 기존 플랜에 영향을 주는 경우, ORDER BY 인덱스 튜닝을 검토 시 해당 테이블을 참조하는 쿼리들의 사용 빈도 및 영향도를 파악하여야 하고, 충분한 테스트 및 플랜 분석이 선행되어야 할 듯하다. ","permalink":"http://localhost:50666/posts/69/","summary":"\u003chr\u003e\n\u003ch2 id=\"인덱스index와-오더바이order-by\" ke-size=\"size26\"\u003e1. 인덱스(INDEX)와 오더바이(ORDER BY)\u003c/h2\u003e\n\u003cp\u003e인덱스는 쿼리의 결과로 특정 row를 찾는 것뿐만 아니라, 특정 순서로 데이터를 정렬하는데도 효율적일 수 있다. ORDER BY와 인덱스를 효율적으로 사용하면 별도의 정렬 과정 없이 ORDER BY를 수행할 수 있다. PostgreSQL에서 현재 지원하는 인덱스 타입 중에서는 B-tree 인덱스만이 정렬 결과로 인덱스를 생성할 수 있다. 다른 인덱스 유형은 특정되지 않은 순서로, 실행 때마다 다른 순서로 열을 반환한다.\u003c/p\u003e\n\u003cp\u003e* 상세한 B-tree 인덱스의 개념은 다음 글을 참고 - \u003ca href=\"https://junhkang.tistory.com/6\"\u003e[Postgresql] - [PostgreSQL] B-tree 인덱스의 원리 및 특징\u003c/a\u003e\u003c/p\u003e","title":"[PostgreSQL] 인덱스(INDEX)와 오더바이(ORDER BY), ORDER BY 성능개선, 효율적인 인덱스 적용"},{"content":" 1. 2단계 커밋 프로토콜([two-phase commit (2PC)) PostgreSQL은 two-phase commit (2PC) 프로토콜을 지원한다. 다수의 분산 시스템 환경에서 모든 데이터베이스가 정상적으로 수정되었음을 보장하는 두 단계 커밋 프로토콜로 분산 트랜잭션에 참여한 모든 데이터베이스가 모두 함께 커밋되거나 롤백되는 것을 보장한다. PostgreSQL의 2단계 트랜잭션은 외부 트랜잭션 관리 시스템에서 사용하기 위해 존재하며 X/Open XA 표준에서 제안된 특징과 모델을 따른다. (사용빈도가 낮은 일부 기능은 구현되지 않았다.) 2단계 커밋은 다음 스탭에 따라 작동된다.\nCoordinator Cohort QUERY TO COMMIT --------------------------------\u0026gt; VOTE YES/NO prepare*/abort* \u0026lt;------------------------------- commit*/abort* COMMIT/ROLLBACK --------------------------------\u0026gt; ACKNOWLEDGMENT commit*/abort* \u0026lt;-------------------------------- end [커밋 요청단계]\nPREPARE를 통해 Coordinator가 데이터베이스 각 노드에 커밋을 준비 요청 각 데이터베이스는 필요 리소스에 LOCK 설정, 로그파일 저장 등 커밋 준비 작업 실행 준비 과정의 실패/성공 여부 알림 [커밋 단계]\n모든 데이터베이스 노드로부터 완료 메시지를 받을 때까지 대기 한 데이터베이스라도 PREPARE OK를 받지 못하면, 모든 데이터베이스 노드에 롤백 메시지를 보내 해당 작업 롤백 모든 데이터베이스에서 PREPARE OK를 받으면 모든 데이터베이스 노드에 커밋 메시지를 보내고 모든 작업 커밋 짧은 기간의 PREPARED 트랜잭션은 공유 메모리와 WAL에 저장되며 체크포인트의 트랜잭션은 pg_twophase 디렉터리에 기록된다.\n2. PREPARE TRANSACTION 2-1. PREPARE TRANSACTION란? PREPARE TRANSACTION \u0026#39;foobar\u0026#39;; PREPARED TRANSACTION 구문은 2단계 커밋을 위해 현재 트랜잭션을 준비 상태로 변경한다. 이 명령어 후에 해당 트랜잭션은 현재 세션과 완전히 분리되며, 해당 트랜잭션의 상태가 디스크에 완전히 저장된다. 그리고 해당 트랜잭션은 데이터베이스가 커밋요청 전에 충돌하더라도 성공적으로 커밋될 확률이 높다. PREPARED TRANSACTION이 성공하면 트랜잭션은 COMMIT PREPARED, ROLLBACK PREPARED로만 커밋/롤백된다. 해당 커맨드는 PREPARED를 실행한 세션 외에 어떤 세션에서도 실행 가능하다. \b구문을 실행한 세션의 시점에서 봤을 때 PREPARED TRANSACTION과 ROLLBACK 명령어는 큰 차이가 없다. 현재 진행 중인 트랜잭션은 없어지고, PREPARE TRANSACTION의 실제 결과는 보이지 않기 때문이다. (PREPARED COMMIT 후에나 차이를 알 수 있다.) PREPARE TRANSACTION 명령어가 실패한다면, 현재 트랜잭션이 취소되며 ROLLBACK과 동일한 결과를 나타낸다. 지금 실행 중인 prepared 트랜잭션은 pg_prepared_xacts에서 확인 가능하다.\n[pg_prepared_xacts]\nTransactionId - 트랜잭션 ID GID - 유저가 정의한 트랜잭션 이름 Prepared Date - 트랜잭션 생성일, timestamp with timezone Owner - 트랜잭션 생성자 Database - 해당 데이터베이스 명 2-2. 주의사항 PREPARE TRANSACTION은 애플리케이션, 혹은 상호 통신하는 세션을 위한 기능이 아니다. 외부 트랜잭션 관리자가 다양한 트랜잭션이나 기타 트랜잭션 리소스에 걸쳐 원자단위의 글로벌 트랜잭션을 수행할 수 있도록 하는 것이 목적이기에, 만약 transaction manager를 쓰는 것이 아니라면 PREPARE TRANSACTION의 사용을 중단해야 한다. 트랜잭션 내부에서만 사용해야 한다. 임시 테이블이나 세션의 임시 네임스페이스를 포함한 명령어나 WITH HOLD, LITEN, UNLISTEN, NOTIFY를 PREPARE에 사용하는 것은 불가능하다. 해당 기능들은 현재세션과 너무 타이트하기 연결되어 있어 트랜잭션 사용에 유용하지 않다. 런타임 파라미터를 변경한다면, PREPARE TRANSACTION 이후에도 적용되며 COMMIT PREPARED, ROLLBACK PREPARED의 영향을 받지 않는다. 이러한 측면에서 PREPARED TRANSACTION은 롤백보다는 커밋에 더 가깝다. 클라이언트가 사라지면 종료되지 않은 채로 트랜잭션이 남을 수 있고, PREPARE 스탭만 백업이 복구되면서 트랜잭션을 닫는 스탭이 없어 계속 유지될 수도 있다. PREPARE 구문을 사용한다면, 트랜잭션 매니저를 통해 해당 트랜잭션들을 주기적으로 관리해주어야 한다. PREPARE 구문이 계속 열려있을 경우 Lock 같은 주요 시스템 자원을 계속 잡고 있을 수 있고, 트랜잭션 ID를 계속 유지하고 있기에 vacuum 시에 사용되지 않는 dead tuple 임에도 정리되지 않을 수 있다. 3. COMMIT/ROLLBACK PREPARE ROLLBACK PREPARED transaction_id; COMMIT PREPARED \u0026#39;foobar\u0026#39;; PREPARE 상태의 트랜잭션을 커밋/롤백시킨다. 커밋/롤백을 위해서는 해당 트랜잭션을 실행시킨 유저와 동일하거나, superuser권한이 있어야 한다. 트랜잭션이 실행된 세션과 동일한 세션에 있을 필요는 없다. 트랜잭션 블록 안에서 사용이 불가능하다. (PREPARED TRANSACTION이 즉시 커밋/롤백된다.) PostgreSQL Extension 기능이며, 외부 트랜잭션 관리 시스템에서 사용하기 위해 만들어졌으며, 트랜잭션 매니저와 함께 사용하는 것을 권장한다. 참고\nhttps://www.postgresql.org/docs/9.6/sql-rollback-prepared.html\nhttps://www.postgresql.org/docs/9.6/sql-prepare-transaction.html\nhttps://www.postgresql.org/docs/9.6/sql-commit-prepared.html\nhttps://www.highgo.ca/2020/01/28/understanding-prepared-transactions-and-handling-the-orphans/\n","permalink":"http://localhost:50666/posts/68/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/68/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"단계-커밋-프로토콜two-phase-commit-2pc\" ke-size=\"size26\"\u003e1. 2단계 커밋 프로토콜([two-phase commit (2PC))\u003c/h2\u003e\n\u003cp\u003ePostgreSQL은 two-phase commit (2PC) 프로토콜을 지원한다. 다수의 분산 시스템 환경에서 모든 데이터베이스가 정상적으로 수정되었음을 보장하는 두 단계 커밋 프로토콜로 분산 트랜잭션에 참여한 모든 데이터베이스가 모두 함께 커밋되거나 롤백되는 것을 보장한다. PostgreSQL의 2단계 트랜잭션은 외부 트랜잭션 관리 시스템에서 사용하기 위해 존재하며 X/Open XA 표준에서 제안된 특징과 모델을 따른다. (사용빈도가 낮은 일부 기능은 구현되지 않았다.) 2단계 커밋은 다음 스탭에 따라 작동된다.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCoordinator                                         Cohort\n                              QUERY TO COMMIT\n                --------------------------------\u0026gt;\n                              VOTE YES/NO           prepare*/abort*\n                \u0026lt;-------------------------------\ncommit*/abort*                COMMIT/ROLLBACK\n                --------------------------------\u0026gt;\n                              ACKNOWLEDGMENT        commit*/abort*\n                \u0026lt;--------------------------------  \nend\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e \u003c/p\u003e","title":"[PostgreSQL] 2단계 커밋 프로토콜(Two-Phase Commit Protocol), Prepare transaction"},{"content":" 1. 기본 트랜잭션의 개념 및 원리 트랜잭션의 기본 개념과 사용 방법은 다음 포스트에서 확인이 가능하다.\n[Postgresql] - [PostgreSQL] 트랜잭션(Transaction)의 개념 및 사용\n기본적으로 트랜잭션이 어떤 것인지, COMMIT, ROLLBACK도 익숙하게 사용하고 있다면, PostgreSQL 내부의 트랜잭션이 어떤 구조로 작동하며 세부 단계를 어떻게 확인 가능한지 자세히 알아보자.\n2. 트랜잭션과 식별자 (Transactions and Identifiers) 기본 개념에서 확인했듯이 트랜잭션은 명시적으로 실행(BEGIN, START TRANSACTION), 종료 (COMMIT, ROLLBACK) 할 수 있다. 명시적 트랜잭션 외의 SQL 구문들은 단일 트랜잭션이 자동으로 적용된다. 그렇다면 각각의 트랜잭션이 어떻게 구분되는지 먼저 살펴보자.\n2-1. Virtual Transaction Id 모든 트랜잭션은 유니크한 Virtual Transaction Id(virtualXID, vxid)로 식별된다. 이 Virtual Transaction Id는 Backend ID와 각 백앤드에 순차적으로 부여된 로컬 아이디 (LocalXID)로 구성되어 있다. 캡처의 virtualxid를 확인해 보면 다음과 같다.\nVirtual Transaction ID = 115/10798 Backend Id = 115 LocalXID = 10798 2-2. Non-Virtual TransactionIds 그 외 가상이 아닌 Non-Virtual TransactionIds(or xid) (캡처의 transactionId = 114016445)들은 PostgreSQL 클러스터의 모든 데이터베이스에서 공통으로 사용하는 global counter를 순차적으로 사용한다. 이러한 TransactionID는 트랜잭션이 처음 데이터베이스에 write 할 때 적용되며, xids가 낮을수록 먼저 수행된 트랜잭션임을 의미한다. 하지만 트랜잭션이 처음 데이터베이스 write를 실행한 순서와 트랜잭션이 시작된 순서는 다를 수 있다. 특히 트랜잭션이 데이터베이스를 read만으로 시작할 때 그렇다. 각 xid는 32비트이고 20억 건의 트랜잭션마다 wraps around 한다.\nwraps around - 트랜잭션 번호가 40억 건이 넘으면 0에서부터 다시 시작하며, 중복된 트랜잭션이 부여될 수 있다. 이 경우 데이터는 존재하지만 접근하지 못하는 데이터가 발생하며, 치명적인 데이터 손실로 이어질 수 있다. 이를 방지하기 위해 20억 트랜잭션이 일어날 때마다 모든 데이터베이스, 모든 테이블에 최소 1번의 vacuum을 실행한다. 해당 내용에서 트랜잭션의 40억 건이 넘으면 0부터 다시 실행되는데 왜 40억이 아닌 20억 트랜잭션마다 최소 1번의 Vacuum이 일어날까? 에 대한 궁금증이 들어 PostgreSQL 측에 문의하였고, PostgreSQL 측 공식 답변은 다음과 같다.\n트랜잭션 ID를 \u0026quot;순환 카운터(Circular Counter)\u0026quot;로 취급하기 때문이다. 특정 트랜잭션 (A)와 트랜잭션 (B)를 비교할 때 (A의 트랜잭션 ID-B의 트랜잭션 ID)의 연산 후 부호를 포함한 정수로 연산을 한다. 만약 결과가 음수라면 A가 더 최근 트랜잭션이고, 공간이 순환적이고 서로 다른 트랜잭션 A, B의 ID가 다를 수 없기에 부호를 가진 정수로써 모두 표현이 가능하다. \\\n순환 공간에서는 1G(10억 건의 트랜잭션)이 3G보다 이전에 있을 수도 있고, 이후에 있을 수도 있다. (1G가 3G의 이전 사이클에서 생성된 트랜잭션 ID인지, 이후 사이클에서 생성된 트랜잭션 ID인지 부호 없이는 구분이 불가능하다.)\n1.5G와 2.5G의 비교 : 1.5G는 2.5G보다 10억 트랜잭션 이전이므로, 부호 있는 비교에서는 1.5G는 2.5G보다 이전 트랜잭션 0.5G 와 3.5G 비교 : 순환 공간을 생각할 때 4G가 최대치인 상황에서 0.5G가 3.5G보다 이후로 간주된다. (0이 최대치(4G) 이후에 존재하기 때문) 그렇기 때문에 부호 있는 비교에서는 0.5G가 3.5G 이후로 판단됨.\n그렇기 때문에 순환 카운터에서는 최대치(4G)의 절반인 2G 트랜잭션 제한을 통해 +/-2G의 범위 내에서 트랜잭션 ID의 \u0026ldquo;전후\u0026rdquo; 관계를 파악한다. 매 wraps around 마다 32bit epoch가 증가하며 64비트의 xid8 유형도 존재한다. (pg_current_xact_id[ () → 로 현재 xid8 조회 가능]{style=\u0026ldquo;background-color: #ffffff; color: #212529; text-align: left;\u0026rdquo;}) Xid들은 PostgreSQL의 MVCC의 동시성, 및 스트리밍 복제의 근간으로 사용된다. 최상위 트랜잭션의 non-virtual xid가 커밋되면, pg_xact 디렉터리에 커밋으로 기록되며 추가 정보들은 pg_commit_ts 디렉터리에 기록된다. (track_commit_timestamp가 활성화 필요) virtual xid, non-virtual xid에, PREPARED TRANSACTION의 경우에는 Global Transaction Identifiers(GID)가 추가로 부여된다. GID는 200바이트의 문자열로 현재의 다른 PREPARED TRANSACTION과 중복되지 않아야 한다. (GID와 xid의 매핑관계는 pg_prepared_xacts에서 확인가능하다.)\n3. 트랜잭션과 락 (Transactions and Locking) 현재 진행 중인 트랜잭션의 Transaction ID는 pg_locks의 virtualxid와 transactionId 칼럼에서 확인할 수 있다.\nSELECT LOCKTYPE, VIRTUALXID, TRANSACTIONID FROM PG_LOCKS; 두 칼럼 모두 read/write 트랜잭션에 존재하지만 Read-only 트랜잭션에는 virtualxid는 있으나 transactionId는 null이다. READ만 수행한 트랜잭션\nRead/Write를 모두 실행한 트랜잭션\nRow-level의 read/write locks는 잠긴 row에 바로 기록되며 pgrowlocks extension을 통해 확인 가능하다.\n4. 서브트랜잭션 (Subtransactions) 서브 트랜잭션(Subtransactions, subxact)은 트랜잭션 안에서 시작되며 큰 트랜잭션을 더 작은 트랜잭션 단위로 분리할 수 있게 해 준다. 서브 트랜잭션은 부모 트랜잭션이 영향을 받지 않고 계속 진행되게 한 채로 commit 혹은 rollback 할 수 있다. 서브 트랜잭션은 SAVEPOINT라는 커맨드로 명시적 실행이 가능하지만 PL/pgSQL의 Exception 구문으로도 실행 가능하다. 서브트랜잭션은 다른 서브트랜잭션으로부터도 시작 가능하다. 최상위 트랜잭션과 그 자식 서브트랜잭션은 계층구조 혹은 트리 구조를 형성하며 그렇기 때문에 메인 트랜잭션을 top-level 트랜잭션이라고 한다. 서브트랜잭션이 non-virtual TransactionalId를 할당받았다면, 그 트랜잭션아이디는 subxid라고 한다. Read-only 서브트랜잭션은 subxids가 부여되지 않지만 write를 하려고 하는 순간 1개를 부여받는다. 또한 상위 레벨 트랜잭션을 포함한 하위 xid의 부모 모두에게 non-virtual 트랜잭션 ID가 할당되며, 부모 xid가 자식 xid보다 항상 낮도록 유지한다.\n각 subxid의 바로 윗 부모 xid는 pg_subtrans디렉터리에 기록된다. 최상위 xid의 경우 부모 트랜잭션이 없기에 기록되지 않으며 읽기 전용 하위 트랜잭션에 대해서도 기록하지 않는다. 서브 트랜잭션이 커밋되면, subxid에 포함된 모든 하위 트랜잭션도 해당 트랜잭션에서 임시로 commit 된 것으로 고려된다. 서브 트랜잭션이 취소되면 모든 자신의 서브 트랜잭션도 취소된다. 최상위 트랜잭션의 xid가 커밋되면, 모든 하위 트랜잭션의 임시 커밋들이 pg_xact 디렉터리에 기록된다. 최상위 트랜잭션이 취소되면, 임시 커밋된 트랜잭션을 포함한 모든 서브 트랜잭션이 또한 취소된다. 각 트랜잭션이 서브트랜잭션을 많이 열어둘수록 트랜잭션 관리 오버해드가 증가한다. 각 백앤드마다 서브트랜잭션을 64개까지는 공유메모리에 캐시 하지만, 그 후로는 pg_subtrans의 subxid를 추가로 찾으면서 저장소 I/O 오버헤드가 극도로 증가한다. 5. 2단계 트랜잭션 (Two-Phase Transactions) PostgreSQL은 two-phase commit (2PC) 프로토콜을 지원한다. 다수의 분산 시스템 환경에서 모든 데이터베이스가 정상적으로 수정되었음을 보장하는 두 단계 커밋 프로토콜로 분산 트랜잭션에 참여한 모든 데이터베이스가 모두 함께 커밋되거나 롤백되는 것을 보장한다.\n(해당 내용은 PREPARE 구문의 상세 내용과 함께 다음 포스트에서 정리할 예정) 참고\nhttps://www.postgresql.org/docs/16/routine-vacuuming.html\nhttps://www.postgresql.org/docs/16/view-pg-prepared-xacts.html\nhttps://www.postgresql.org/docs/16/two-phase.html\nhttps://www.postgresql.org/docs/16/subxacts.html\nhttps://www.postgresql.org/docs/16/xact-locking.html\nhttps://www.postgresql.org/docs/16/transaction-id.html\n","permalink":"http://localhost:50666/posts/67/","summary":"\u003chr\u003e\n\u003cp\u003e \u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/67/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"기본-트랜잭션의-개념-및-원리\" ke-size=\"size26\"\u003e1. 기본 트랜잭션의 개념 및 원리\u003c/h2\u003e\n\u003cp\u003e트랜잭션의 기본 개념과 사용 방법은 다음 포스트에서 확인이 가능하다.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://junhkang.tistory.com/20\"\u003e[Postgresql] - [PostgreSQL] 트랜잭션(Transaction)의 개념 및 사용\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e기본적으로 트랜잭션이 어떤 것인지, COMMIT, ROLLBACK도 익숙하게 사용하고 있다면, PostgreSQL 내부의 트랜잭션이 어떤 구조로 작동하며 세부 단계를 어떻게 확인 가능한지 자세히 알아보자.\u003c/p\u003e\n\u003ch2 id=\"트랜잭션과-식별자-transactions-and-identifiers\" ke-size=\"size26\"\u003e2. 트랜잭션과 식별자 (Transactions and Identifiers)\u003c/h2\u003e\n\u003cp\u003e기본 개념에서 확인했듯이 트랜잭션은 명시적으로 실행(BEGIN, START TRANSACTION), 종료 (COMMIT, ROLLBACK) 할 수 있다. 명시적 트랜잭션 외의 SQL 구문들은 단일 트랜잭션이 자동으로 적용된다. 그렇다면 각각의 트랜잭션이 어떻게 구분되는지 먼저 살펴보자.\u003c/p\u003e","title":"[PostgreSQL] 트랜잭션(Transaction)의 작동원리"},{"content":" 1. WAL (Write-Ahead Logging) / 아카이브 모드 백업(Archive mode backup)이란? 아카이브 모드 백업을 이해하기 위해 WAL에 대한 개념을 먼저 살펴보자. WAL은 PostgreSQL에서 데이터의 무결성을 보장하는 표준 방법으로, 기본 콘셉트는 모든 데이터의 변경을 로깅 완료 후에 실행하는 것이다. WAL 기록을 영구적인 저장소에 먼저 기록한 후에 데이터의 변경 내용을 실행하는 것으로, 이 과정을 거치면 충돌 혹은 데이터에 문제가 있을 때 WAL 로깅 내용을 바탕으로 특정 시점으로 복구가 가능하여 데이터 무결성을 보장할 수 있다. 충돌이 발생할 때마다 Log를 통해 데이터베이스를 복구할 수 있기 때문에,모든 트랜잭션 커밋 시마다 디스크의 데이터를 flush 할 필요 없다. 또한 WAL을 사용하면 디스크 쓰기 횟수가 현저히 줄어든다. 트랜잭션에 의해 변경된 모든 데이터 파일을 write 하는 것이 아니라 WAL 파일만 디스크에 flush 하면 트랜잭션이 커밋되기 때문이다. WAL은 순차적으로 작성되기 때문에 WAL 동기화 비용은 데이터 페이지를 flush 하는 것보다 코스트가 훨씬 적다. (특히 데이터베이스가 여러 군데의 작은 트랜잭션을 처리할 때 효율적이다.) 2. 장점 WAL은 온라인 백업 및 특정 시점 복구(PIT, point-in-time)를 지원 (WAL 데이터를 보관함으로써 사용가능한 WAL 데이터에 포함된 어떠한 순간으로도 롤백이 가능) 복원에 필요한 WAL 파일의 수량 제한이 없기에 백업을 시작한 시점 이후의 WAL 로그파일만 존재한다면 백업기간이 아무리 길더라도 복원이 가능 운영 서버의 WAL 파일을 주기적으로 백업해 놓는다면 운영 서버 장애 발생 시 빠르게 복구가 가능 3. 단점 특정 데이터베이스만을 대상 불가능, 전체 데이터베이스를 대상으로 진행 WAL 로그를 충분히 저장할 디스크 여유 공간 필요 대량의 데이터를 처리할때 WAL write 코스트 증가 4. 설정 $PGDATA의 postgresql.conf 파일 내의 파라미터를 통해 상세 설정 가능\n4-1. wal_level (Enum) WAL 레코드 정보의 양을 결정한다. 디폴트 값은 minimal이다.\nminimal - 충돌 또는 즉시 셧다운으로부터 복구하기 위한 최소한의 정보 archive - WAL 아카이브에 필요한 로깅 추가 hot_standby - 대기 서버에서 읽기전용 쿼리에 필요한 정보 추가 4-2. archive_mode (Boolean) archive_mode를 선택하면 완료된 WAL 세그먼트가 아카이브 저장소에 저장 (wal_level이 minimum인 경우 사용 불가)\n4-3. archive_command (String) 완료된 WAL 파일 세그먼트를 아카이빙 할 때 실행하는 쉘 명령어\n참고\nhttps://www.postgresql.org/docs/16/wal-intro.html\nhttps://postgresql.kr/docs/13/continuous-archiving.html#BACKUP-PITR-RECOVERY ","permalink":"http://localhost:50666/posts/66/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/66/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"wal-write-ahead-logging-아카이브-모드-백업archive-mode-backup이란\" ke-size=\"size26\"\u003e1. WAL (Write-Ahead Logging) / 아카이브 모드 백업(Archive mode backup)이란?\u003c/h2\u003e\n\u003cp\u003e아카이브 모드 백업을 이해하기 위해 WAL에 대한 개념을 먼저 살펴보자. WAL은 PostgreSQL에서 데이터의 무결성을 보장하는 표준 방법으로, 기본 콘셉트는 모든 데이터의 변경을 로깅 완료 후에 실행하는 것이다. WAL 기록을 영구적인 저장소에 먼저 기록한 후에 데이터의 변경 내용을 실행하는 것으로, 이 과정을 거치면 충돌 혹은 데이터에 문제가 있을 때 WAL 로깅 내용을 바탕으로 특정 시점으로 복구가 가능하여 데이터 무결성을 보장할 수 있다.\n \u003c/p\u003e","title":"[PostgreSQL] WAL (Write-Ahead Logging) / 아카이브 모드 백업(Archive mode backup)의 개념 및 장단점"},{"content":" 대량 데이터 인서트 시 성능 개선 최초 서비스 배포나 데이터 마이그레이션을 할 때 대량의 데이터를 한 번에 인서트 하는 경우가 있다. PostgreSQL 공식문서에서는 대량 인서트 시에 효율적으로 진행할 수 있는 방법을 제시해 준다. (대량 데이터를 인서트 할 때 효율적인 설정이지 데이터베이스 조회나 업데이트 등실제 운영 시에 사용할 방법은 아니다.)\n1. Autocommit 옵션 해제 대량의 인서트 실행 시, Autocommit 옵션을 해제하고 한 트랜잭션에서 작업 후에 커밋을 진행해야 한다(일반적으로 SQL를 실행 시에 자동으로 시작 시 BEGIN, 끝날 때 COMMIT으로 트랜잭션 처리가 되지만, 확실히 되고 있는지 확인필요하다.). 대량 데이터 인서트의 각각을 별도로 commit 한다면, PostgreSQL은 인서트 되는 각 열에 대해 너무 많은 작업을 수행하게 된다. 또한 모든 인서트를 한 트랜잭션에 처리할 경우에는 한 INSERT가 실패할 경우 그 시점까지 인서트 된 모든 작업이 취소되기에 실패 작업에 대한 부분 보완 및 무결성을 고려하지 않아도 된다.\n2. COPY, PREPARE 사용 INSERT를 여러 번 실행시키기보다 COPY 커맨드 한 번으로 해결하라. COPY 명령어는 많은 ROW를 로드하는데 최적화되어 있다. INSERT 구문보다 덜 유연하지만, 대량 데이터를 로딩하는데 훨씬 적은 오버헤드를 발생시킨다. (COPY는 단일 명령어이기에 실행 시 Autocommit을 따로 비활성화시킬 필요 없다.) COPY를 사용할 수 없는 상황이라면 PREPAPRE 구문을 통해 준비된 INSERT 구문을 EXECUTE를 통해 필요한 만큼 실행시키는 방법도 있다. 이 방법은 INSERT 구분을 반복적으로 사용할 때 드는 파싱과 실행계획의 오버헤드를 줄여준다. COPY를 사용한 대량 데이터 로딩은 INSERT보다 거의 모든 경우에 더 빠르다. (PREPARE 구문을 사용하고 단일 트랜잭션에서 배치를 통해 INSERT를 한다고 해도 COPY를 사용하는 것이 더 빠르기 때문에 가능하다면 COPY를 사용하는 것이 유리하다.) COPY는 해당 테이블의 CREATE TABLE 혹은 TRUNCATE 명령어와 같이 쓸 때 더 빠르다. 이 경우에 에러가 날 경우에 최신으로 로드된 데이터를 포함하고 있는 파일은 무조건 삭제되기 때문에 WAL write가 필요 없다. (WAL의 개념은 다음 포스트를 참고)\n[PostgreSQL] WAL (Write-Ahead Logging) / 아카이브 모드 백업(Archive mode backup)의 개념 및 장단점\n그러나 wal_level이 minimal로 설정되어 있을 경우에만 유효하다.\n3. 인덱스를 삭제하라 아얘 새로운 테이블을 생성하는 상황이라면, 가장 빠른 방법은 테이블을 만들고 COPY를 사용해서 테이블 데이터를 채우고 테이블에 필요한 인덱스를 그 후에 생성하는 것이다. 이미 존재하는 데이터에 인덱스를 생성하는 것이 데이터를 인서트 하면서 인덱스를 생성하는 것보다 더 빠르다. 기존 테이블에 데이터를 인서트 하는 상황이라면, 인덱스를 삭제하고, 데이터를 채우고, 인덱스를 다시 생성하는 것이 유리하다. 물론 인덱스가 사리진 기간 동안 해당 데이터에 접근하는 다른 사용자들은 성능 이슈를 겪을 것이고 Unique 인덱스를 drop 하는 경우에는 인덱스가 누락된 동안 무결성에 영향을 줄 수 있기에 작업과정에서의 영향도를 철저히 확인해야 한다.\n4. FK 제약을 삭제하라 인덱스와 동일하게, FK는 벌크로 한 번에 체크하는 것이 row단위로 체크하는 것보다 효율적이다. 그래서 FK제약을 작업 전에 삭제하고, 데이터를 넣고, FK제약조건을 다시 생성한다. 인덱스와 동일하게 인서트 속도가 향상되는 반면 제약조건이 없는 사이에 데이터에 대한 무결성이 깨질 수 있다. 이미 존재하는 FK 제약조건이 있는 테이블에 데이터를 넣을 때, FK 제약조건을 체크하는 행위가 서버의 pending trigger 이벤트 목록에 추가된다. 수백만건의 데이터를 인서트 하는 경우에 트리거 이벤트 큐의 가용 메모리를 초과하여 적정량 이상의 메모리 스왑이 발생하거나 실행명령이 완전히 실패할 수도 있다. 그래서 많은 양의 데이터를 인서트 할 때는 FK 조건을 삭제하고 다시 설정해야 한다. 제약조건을 일시적으로 해제할 수 없다면, 더 작은 트랜잭션 단위로 분할하는 것이 유일한 방법일 수도 있다.\n5. maintenance_work_mem 증가 일시적으로 maintenance_work_mem 설정 값을 늘리는 것은 대량 데이터를 로딩 시 성능을 향상할 수 있다. 이 옵션은 CREATE INDEX, ALTER TABLE ADD FOREIGN KEY 명령어의 속도를 올려준다. COPY 자체의 속도를 올려주진 않기에 CREATE INDEX, ALTER TABLE ADD FOREIGN KEY를 사용할 때만 유효하다.\n6. max_wal_size 증가 일시적으로 max_wal_size 설정값을 늘리는 것은 대량 데이터 로딩을 더 빠르게 해 준다. PostgreSQL에서의 대량 데이터 로딩은 일반적인 checkpoint 체크(checkpoint_timeout 옵션에 해당하는) 보다 더 잦은 checkpoint확인을 요한다. checkpoint가 발생할 때마다, 모든 부적절한 pages가 디스크에서 flush 된다. max_wal_Size를 늘림으로써 필요한 checkpoint의 빈도를 낮출 수 있다.\n7. WAL Archival, Streaming Replication을 미사용 WAL Archiving이나 Streaming replication을 사용하는 경우 대량 데이터를 로딩할 때, 급격히 증가하는 WAL 데이터를 처리하는 것보다 로드가 완료된 후 새로운 백업을 실행하는 것이 유리하다. 데이터 로딩 중에 WAL 로깅이 증가하는 것을 방지하기 위해 WAL Archiving, streaming replication을 중지한다. (다음 3가지 옵션을 통해 설정가능)\nwal_level = minimal archive_mode = off max_wal_senders = 0 변경 후 서버를 재시작해야 하고, 기존의 기본 백업 정보들을 아카이브 복구나 standby서버로 사용이 불가능하게 된다. 데이터 손실로 이어질 수 있기 때문에 기존 백업 데이터에 대한 확인이 필요하다. 해당 옵션을 적용하면 Archiver 시간이나 WAL 샌더가 WAL데이터를 처리하는 시간을 줄여주는 것뿐만 아니라, 이 작업을 하면 실제로 특정 명령어들의 실행 속도를 더 빠르게 해 준다. WAL_LEVEL 이 mininal일 때, 현재 트랜잭션 혹은 상위 트랜잭션에서 테이블 변경 혹은 인덱스 생성/삭제할 때 WAL을 전혀 작성하지 않기 때문이다. (WAL 아카이빙을 하지 않아도, 마지막에 fsync 실행으로 충돌을 더 효율적으로 방지할 수 있다.)\n8. 작업 후 ANALYZE 실행 테이블 내의 데이터 분포를 크게 변경하는 경우에는 ANALYZE를 실행시켜야 한다. 대량 데이터를 인서트 할 때도 유효하며, ANALYZE 혹은 VACUUM ANALZYE를 실행시키면 플래너가 테이블의 최신 통계를 가져오는 것을 확인할 수 있다. 정확하지 않은 통계나 수집되지 않은 통계가 있을 때 플래너는 쿼리 실행계획을 비효율적으로 세울 수 있고, 이는 테이블의 성능저하를 유발한다. (Autovacuum 데몬이 실행 중이라면 Analyze를 자동으로 실행하고 있을 것이다.)\n9. pg_dump 확인 시 주의사항 pg_dump에 의해 생성된 Dump script는 대량 데이터 인서트시에 위의 가이드라인(1~8)의 일부만을 자동으로 적용시킨다. pg_dump의 덤프를 최대한 빨리 복구하려면, 몇몇 추가 세팅을 수동으로 해야 한다. (이 작업은 dump를 복구하는데 적용되는 거지, 생성할 때 적용되는 것이 아니다.) Default로, pg_dump는 COPY를 사용하며(가이드 2번 적용), 완벽한 schema-and-data 덤프를 생성할 때는 인덱스 및 외부키를 생성하기 전에 데이터를 로드 (가이드 3,4 적용) 하기 때문에 몇몇 가이드라인이 자동으로 적용되고, 유저는 다음 항목들만 정의하면 된다. maintenance_work_mem, max_wal_size를 적절한 값으로 설정 WAL archiving, streaming replication을 사용 중이라면, 덤프 복구 중에는 미사용을 고려 archive_mode = off, wal_level = minimal, max_wal_sender = 0을 덤프를 로딩하기 전에 설정하고, 덤프 복구 후에 원래 값으로 되돌리고 기본 백업을 최신으로 실행 pg_dump, pg_restore의 병렬 dump 및 복구 모드를 테스트해서 최적의 동시 job 실행 개수를 적용. (-j 옵션을 사용하여 덤프 및 복원을 적절하게 병렬로 수행하면 직렬보다 더 높은 성능 가능하다.) 모든 덤프가 단일 트랜잭션으로 복구되도록 설정 (-1 혹은 --single-transaction 커맨드라인 옵션을 psql 혹은 pg_restore에 날리면 된다.) 다만, 이 모드를 사용하면, 아주 작은 에러라도 날 경우 전체 복구 과정이 롤백되어 몇 시간의 작업을 날릴 수도 있게 된다 DB서버에 여러 개의 CPU를 가용하는 것이 가능하다면 pg_restore의 --jobs 옵션을 쓸 수 있다. 이를 통해 병렬로 동시에 데이터를 인서트 하고 인덱스도 생성할 수 있다. 작업 이후 ANALYZE를 실행시킨다 data-only 덤프는 여전히 COPY를 사용하지만 인덱스를 삭제하거나 재생성하지 않고 FK에 영향을 주지 않는다. 그래서 data-only 덤프를 로딩할 때, 인덱스나 FK를 삭제한 후 다시 생성하는 방식을 사용할지 말지는 유저의 선택이다. 대량 데이터를 인서트 할 때와 동일하게, max_wal_size를 증가시키는 것은 유리하다, 하지만 data-only 덤프는 maintenance_work_mem을 늘리는 것에 영향을 받지 않는다. 그보다 인덱스와 FK키를 삭제 후 수동으로 다시 생성하는 것이 효율적이다. (* FK를 삭제하는 것 대신에 --disable-triggers 옵션으로 FK검증트리거 실행을 방지할 수 있지만, 단지 체크를 지연시키는 것으로 무결성에 위배되는 데이터가 들어올 수 있음은 동일하다)\n10. 결론 및 적용 검토 실제로 산군의 데이터베이스는 수백만 건의 데이터를 마이그레이션 \u0026amp; 백업하는 작업이 주기적으로 있다. 일부 작업의 경우 타 테이블을 참조하거나 대량 업데이트가 포함되어 있어 해당 테이블의 조회 성능에 직접적인 영향을 주는 경우가 있기에 트랜잭션을 분할하여 배치성으로 작업을 하거나, 특정 테이블을 격리 후 작업 및 동기화를 진행하고 있다. 인서트 시간이 길어지는 만큼 부담이 가는 작업인 만큼, 속도 향상을 위해 다양한 방법을 시도하고 있고, 공식문서에 나온 다음 가이드들을 추가 검토해 보게 되었다. 1~8 가이드 중 직접적으로 적용 가능한 부분이 있을까? 1. Autocommit 해제 - 작업은 기존에도 사용 중 (데이터 무결성, 작업 내용 검증 및 백업을 위해 트랜잭션 컨트롤은 필수) 2. COPY, PREPARE - COPY 명령어도 사용 중이지만 한계가 있는 경우가 많고, 배치성으로는 이미 작업 중 3, 4 인덱스, FK 삭제 후 재설정 - 실제 운영 중인 라이브 테이블에 인덱스, FK를 일시적으로 없애는 것은 효율적이지 못함 5,6,7,8 PG옵션 추가 - 실제 운영 중인 DB에 설정값 적용 및 재부팅을 시도하는 것은 불가능 간단하게 적용가능한 1,2번은 보통 다 사용중일 것이고, 전체적으로 운영 중인 데이터베이스에 실행하기엔 위험부담이 큰 작업들이 대부분이다(특히 5~8 옵션을 통해 DB자체를 다운시켜야 하거나 기존 백업을 사용할 수 없는 상황은 적용 불가). 대부분 최초 서비스 배포 혹은 리뉴얼 시에 적용 가능한 방법들로 보인다. 현재 운영 중인 데이터베이스의 특정 테이블을 격리하는 방식으로 3~4번이 부분적용이 가능해 보이기는 하지만, 인덱스를 삭제하고 데이터를 인서트시 코스트가 줄어들더라도 대량의 데이터가 들어간 후의 테이블에 인덱스를 추가하는 시간도 만만치 않을 것이다. 운영 중인 데이터베이스에 대량 데이터를 인서트시 인덱스, FK에 대한 조정, 옵션값 변경 후 DB 재실행이 불가능한 상황이라면, 배치성, 트랜잭션 분할을 통해 (데이터베이스 성능과 데이터 무결성) 모니터링을 함께 진행하는 것이 가장 효과적일 듯하다. 참고\nhttps://www.postgresql.org/docs/current/populate.html\nhttps://postgresql.kr/docs/9.6/wal-intro.html\n","permalink":"http://localhost:50666/posts/65/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/65/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"대량-데이터-인서트-시-성능-개선\" ke-size=\"size26\"\u003e대량 데이터 인서트 시 성능 개선\u003c/h2\u003e\n\u003cp\u003e최초 서비스 배포나 데이터 마이그레이션을 할 때 대량의 데이터를 한 번에 인서트 하는 경우가 있다. PostgreSQL 공식문서에서는 대량 인서트 시에 효율적으로 진행할 수 있는 방법을 제시해 준다. (대량 데이터를 인서트 할 때 효율적인 설정이지 데이터베이스 조회나 업데이트 등실제 운영 시에 사용할 방법은 아니다.)\u003c/p\u003e\n\u003ch2 id=\"autocommit-옵션-해제\" style=\"color: #333333; text-align: start;\" ke-size=\"size26\"\u003e1. Autocommit 옵션 해제\u003c/h2\u003e\n\u003cp\u003e대량의 인서트 실행 시, Autocommit 옵션을 해제하고 한 트랜잭션에서 작업 후에 커밋을 진행해야 한다(일반적으로 SQL를 실행 시에 자동으로 시작 시 BEGIN, 끝날 때 COMMIT으로 트랜잭션 처리가 되지만, 확실히 되고 있는지 확인필요하다.). 대량 데이터 인서트의 각각을 별도로 commit 한다면, PostgreSQL은 인서트 되는 각 열에 대해 너무 많은 작업을 수행하게 된다. 또한 모든 인서트를 한 트랜잭션에 처리할 경우에는 한 INSERT가 실패할 경우 그 시점까지 인서트 된 모든 작업이 취소되기에 실패 작업에 대한 부분 보완 및 무결성을 고려하지 않아도 된다.\u003c/p\u003e","title":"[PostgreSQL] 대량 데이터 인서트 시 성능 개선 및 주의 사항"},{"content":" 1. Foreign Key 외래키란? Foreign key constraint 외래키 제약은 특정 칼럼 혹은 칼럼들의 값이 다른 테이블의 특정 row와 매칭되어야 하는 제약조건이다. 이를 두 관련 테이블 사이의 참조 무결성 (referential integrity)를 유지한다고 말한다. 그렇게 복잡한 개념은 아니니 바로 사용법을 확인해 보도록 하자\n2. 예제 2-1. 기본 외래키(Foreign Keys) 생성 products 테이블은 물품의 이름, 가격 정보 테이블이고, orders 테이블은 존재하는 물품 각각에 대한 순서 정보가 들어있는 테이블이다. orders, products 테이블의 product_no에 외래키 제약을 적용하는 예제이다.\nCREATE TABLE products ( product_no integer PRIMARY KEY, name text, price numeric ); CREATE TABLE orders ( order_id integer PRIMARY KEY, product_no integer REFERENCES products (product_no), quantity integer ); orders 테이블의 제약조건을 위와 같이 주었을 때 products 테이블에 없는 product_no로는 데이터 생성이 불가능하다. 이 경우 다음과 같이 명칭 한다.\norders - referencing(참조하는) 테이블 products - referenced(참조된) 테이블 2-2. 칼럼을 지정하지 않은 외래키(Foreign Keys) CREATE TABLE orders ( order_id integer PRIMARY KEY, product_no integer REFERENCES products, quantity integer ); 특정 칼럼을 지정하지 않는다면 reference 칼럼으로 Primary Key에 해당하는 칼럼을 자동으로 사용하기에 별도로 칼럼명을 명시하지 않아도 된다. (PK가 바뀔 일은 거의 없겠지만) 테이블 구조가 바뀔 수 있고, 명확한 제약조건을 명시하는 것이 좋기에 칼럼을 지정하는 것이 좋다. 2-3. 복합 칼럼 외래키(Foreign Keys) FK 제약조건을 여러 개의 칼럼을 대상으로도 사용할 수 있다.\nCREATE TABLE t1 ( a integer PRIMARY KEY, b integer, c integer, FOREIGN KEY (b, c) REFERENCES other_table (c1, c2) ); 물론 참조하는 칼럼과 참조되는 테이블의 칼럼 수는 일치하여야 한다.\n2-4. 자기 참조 외래키(Self-referential Foreign Keys) 종종 다른 테이블이 아니라 같은 테이블 내의 칼럼의 FK로 두는 것이 효율적일 때가 있다. 이를 자기 참조 외래키 (self-referential foreign key)라고 한다. 예를 들어 트리 구조의 노드들을 테이블 row로 표현하고 싶을 때, 다음과 같이 parent_id를 node_id에 참조시키면 된다. CREATE TABLE tree ( node_id integer PRIMARY KEY, parent_id integer REFERENCES tree, name text, ... ); 최상위 노드의 parent_id는 null이 될 것이고, parent_id가 null이 아닌 항목들은 해당 테이블의 유효한 노드를 참조하도록 제한된다.\n2-5. 다중 외래키(Foreign Keys) 한 개의 테이블은 여러 개의 FK를 가질 수 있으며 이는 다대다 (many-to-many) 테이블 관계 구현에 사용된다. 기존 예제의 상품, 상품순서 구조에 추가로, 한 순서에 많은 상품을 포함할 수 있게 한다면 다음과 같은 테이블 구조를 사용할 수 있을 것이다. CREATE TABLE products ( product_no integer PRIMARY KEY, name text, price numeric ); CREATE TABLE orders ( order_id integer PRIMARY KEY, shipping_address text, ... ); CREATE TABLE order_items ( product_no integer REFERENCES products, order_id integer REFERENCES orders, quantity integer, PRIMARY KEY (product_no, order_id) );​ 3. 외래키(Foreign Keys) 옵션 앞서 말한 대로, FK 제약조건이 걸려있다면 참조되지 않은 값으로는 데이터 생성이 불가능하다. 그러나 참조된 orders가 생성된 후에 product가 삭제되면 어떻게 될까? Postgresql에서는 다음 상황들 중에 선택적으로 사용이 가능하다.\n참조하는 데이터(orders)가 있을 경우 삭제 불가 참조하는 데이터(orders)까지 함께 삭제 등등... 데이터를 처리하는 상황을 선택하는 ON DELETE, ON UPDATE 등의 옵션과, 처리하는 방식을 선택하는 RESTRICT, CASCADE 등의 옵션을 조합하여 원하는 결과를 만들면 된다. 다음 예제는 서로 다른 테이블에 참조된 칼럼 각각의 값이 삭제됐을 때 참조된 상위 값이 있을 때 각각 같이 삭제할지, 삭제를 방지할지를 설정한 테이블 생성 쿼리이다.\nCREATE TABLE order_items ( product_no integer REFERENCES products ON DELETE RESTRICT, order_id integer REFERENCES orders ON DELETE CASCADE, quantity integer, PRIMARY KEY (product_no, order_id) ); \b다음과 같이 설정하면 order_items에서 product_no를 삭제하려 할 때 참조하는 데이터가 있을 경우 삭제가 불가능하고, order_id의 경우 참조하는 데이터와 함께 삭제가 된다. (\bRESTRICT, CASCADE는 가장 기본적으로 사용되는 옵션)\n3-1. RESTRICT Restrict는 참조하는 열의 삭제를 방지한다. 참조하는 오브젝트가 존재할 시 실행 자체를 실패한다.\n3-2. NO ACTION NO ACTION은 제약조건을 선택할 때 참조 행이 존재한다면 오류가 발생하고, 지정하지 않을 시에는 기본 동작(RESTRICT / CASCADE)이 된다. NO ACTION을 선택하나, NO ACTION을 선택하지 않고 RESTRICT를 설정하나 실행이 되지 않는 건 똑같지만, 본질적으로 NO ACTION은 무결성 체크하는 시점을 트랜잭션의 후반부까지 연기할 수 있다는 차이점이 있다. (DEFERRABLE, NOT DEFERRABLE 옵션으로 제어 가능하다.)\n3-3. CASCADE 폭포 혹은 단계적인 이라는 의미로, 연관된 데이터의 일괄적인 적용을 의미한다. CASCADE는 참조하는 열에도 함께 변경을 가한다. 두 가지 옵션을 보면\nSET NULL SET DEFAULT 이 옵션은 참조하는 테이블의 칼럼값(order_id)을 null로 변경할지, default 값으로 변경할지 선택하는 옵션이다. SET NULL, SET DEFAULT는 FK가 추가적인 정보를 나타낼 때 적절하다. 예를 들어, 위 예제에서 products 테이블에서 product manager의 정보가 참조되고 있고, product manager가 삭제될 때 product에 해당 참조 데이터를 null 혹은 default로 자동으로 바꿔준다면 별도의 추가 명령 없이 관리할 수 있을 것이다. 하지만 SET NULL, SET DEFAULT가 참조 테이블의 다른 제약조건들까지 먼저 확인하지는 않기에, 만약 SET DEFAULT로 설정했는데 \bDEFAULT 값이 다른 제약조건에 부합하지 않을 경우 해당 동작은 실패한다. CREATE TABLE tenants ( tenant_id integer PRIMARY KEY ); CREATE TABLE users ( tenant_id integer REFERENCES tenants ON DELETE CASCADE, user_id integer NOT NULL, PRIMARY KEY (tenant_id, user_id) ); CREATE TABLE posts ( tenant_id integer REFERENCES tenants ON DELETE CASCADE, post_id integer NOT NULL, author_id integer, PRIMARY KEY (tenant_id, post_id), FOREIGN KEY (tenant_id, author_id) REFERENCES users ON DELETE SET NULL (author_id) ); 위 예제 테이블을 보면 posts의 데이터를 삭제할 때, FK 제약조건으로 참조된 tenant_id를 null로 변경하려 하지만, tenant_id는 PK의 일부로 null이 될 수 없기에 실행되지 않는다.\n3-4. ON DELETE ON DELETE 옵션은 테이블에 연관된 오브젝트의 유형에 따라 적절하게 사용되어야 한다. 참조하는 테이블이 참조된 테이블 값의 구성요소이며 독립적으로 존재할 수 없다면 CASCADE 옵션을 적용하여 한번에 처리하는 것이 적절하고, 두 테이블의 오브젝트가 독립적인 관계라면 RESTRICT, NO ACTION이 적합하다. 예를 들어, 위의 예제에서 order_items는 orders의 일부분이고 독립적으로 사용될 일이 없기에 orders가 삭제될 때 자동으로 지워지는 것이 편할 것이다. orders와 products는 독립적으로 사용할 여지가 있기에 삭제 시 products를 자동으로 지우는 건 문제가 될 수 있다.\n3-5. ON UPDATE ON DELETE와 유사하게 참조열이 업데이트될 때 호출되는 ON UPDATE도 있다. SET NULL, SET DEFAULT의 설정이 다른 제약조건에 위배된다면 적용할 수 없다는 점은 동일하다. CASCADE와 사용 시 참조하는 칼럼의 업데이트된 값이 참조된 열로 복사된다.\n4. 인덱스 FK 제약조건은 PK이거나, Unique 제약조건(UK)이거나, \u0026quot;복합 인덱스의 일부가 아닌\u0026quot; 칼럼을 참조해야만 한다. 이 뜻은 참조된 칼럼은 인덱스를 항상 가지고 있다는 뜻이다. 하지만 참조하는 칼럼은 인덱스가 필수이거나 인덱스를 자동으로 생성하지 않는다. 열의 DELETE, UPDATE는 참조하는 테이블에서 이전 값을 찾아야 하기에, 참조하는 칼럼에도 인덱스를 설정하는 것이 유리한 경우도 있다.\n5. 장단점 5-1. 장점 데이터의 무결성 쉬운 데이터 구조/관계 확인 update, delete 등의 로직 간소화 5-2. 단점 참조 테이블들을 스캔하는데 드는 추가 코스트 테이블 구조 변경 시 고려해야 할 사항 증가 데이터 변경이 찾을 때 특히 코스트가 증가 테스트 데이터 생성 및 데이터 강제 보정이 번거로움 6. 적용 검토 이미 FK를 사용하는 테이블도 다수 존재한다. 중요 기준 데이터의 무결성을 보장하는 데는 효율적인 기능이라고 생각한다. 다만 \u0026quot;실시간으로 업데이트되는 대용량 데이터의 경우에도 무결성 보장을 위해 FK를 적용해야 하는가?\u0026quot;에 대한 판단을 위해 FK에 대한 내용을 좀 깊게 들여다보았다. 일단 결론은, 실시간으로 변동되는 대용량 데이터 베이스, 특히 타 시스템과의 동기화가 이루어지고 있는 산군의 데이터베이스에는 적합하지 않다고 판단된다. FK의 스캔 코스트가 성능에 큰 영향이 없다\u0026quot;, 혹은 \u0026ldquo;적절한 인덱싱으로 스캔 코스트를 관리할 수 있다.\u0026ldquo;라는 상황 자체가 참조 테이블의 인덱스를 전제한다. FK만을 위한 인덱스를 참조된 테이블에 추가하거나, 참조된 테이블에 업데이트나 삭제가 발생하여 참조하는 테이블에서 이전 값을 조회하기 위해 역스캔하는 경우의 코스트를 줄이기 위해 인덱스를 추가해야 하는 상황이 발생한다. 현재 운영 중인 데이터베이스는 최소 수백만 건에서 수천만건의 테이블이 서로 연결되어 있으며 read/write 간 동기화, ElasticSearch 등으로의 동기화를 고려하여 최적의 인덱스로 세팅, 되어있고 실시간 모니터링을 통한 튜닝이 지속적으로 진행 중이다. 모든 칼럼에 인덱스를 거는 것이 효율적이지 않은 것처럼, 한정된 자원으로 최적의 인덱싱을 찾아야 하고, 데이터의 변경에 민감하게 대응해야 하는 상황에서는 FK를 설정하는 것이 효율적이지 못하다는 판단이다. 또한, 참조되는 테이블의 추가 인덱스 설정 없이 FK를 설정하는 것만으로는 코스트가 증가하지 않지만 실시간으로 몇천 건, 많게는 몇만 건의 데이터가 업데이트 및 동기화되는 상황에서 테이블의 Lock이 여러 테이블로 전파될 위험성도 무시할 수 없다. 참고 : https://www.postgresql.org/docs/16/ddl-constraints.html#DDL-CONSTRAINTS-FK\n","permalink":"http://localhost:50666/posts/64/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/64/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"foreign-key-외래키란\" ke-size=\"size26\"\u003e1. Foreign Key 외래키란?\u003c/h2\u003e\n\u003cp\u003eForeign key constraint 외래키 제약은 특정 칼럼 혹은 칼럼들의 값이 다른 테이블의 특정 row와 매칭되어야 하는 제약조건이다. 이를 두 관련 테이블 사이의 참조 무결성 (referential integrity)를 유지한다고 말한다. 그렇게 복잡한 개념은 아니니 바로 사용법을 확인해 보도록 하자\u003c/p\u003e\n\u003ch2 id=\"예제\" ke-size=\"size26\"\u003e2. 예제\u003c/h2\u003e\n\u003ch3 id=\"기본-외래키foreign-keys-생성\" ke-size=\"size23\"\u003e2-1. 기본 외래키(Foreign Keys) 생성\u003c/h3\u003e\n\u003cp\u003eproducts 테이블은 물품의 이름, 가격 정보 테이블이고, orders 테이블은 존재하는 물품 각각에 대한 순서 정보가 들어있는 테이블이다. orders, products 테이블의 product_no에 외래키 제약을 적용하는 예제이다.\u003c/p\u003e","title":"[PostgreSQL] 외래키(Foreign Keys) 개념, 사용법, 장단점, 적용검토"},{"content":" 1. 컴포지트(Composite) 패턴이란? Composite는 혼합물, 복합물이란 뜻으로 중첩된 구조, 재귀적인 구조를 만드는 패턴이다. 대표적인 예로 윈도우 디렉터리와 파일을 들 수 있다. 디렉터리, 파일은 엄연히 다른 속성이지만 둘 다 디렉터리 안에 넣을 수 있다는 공통점이 있다. 디렉터리 내에는 또 다른 디렉터리가 있을 수 있기에 중첩, 재귀적인 구조를 만들어낸다. 디렉터리와 파일을 합쳐 디렉터리 엔트리라고 부르기도 한다. 두 속성을 같은 종류로 간주하는 것이다. 어떤 디렉터리 안에 무엇이 있는지 차례대로 조사할 때 조사하는 것이 디렉터리일 수도, 파일일 수도 있다, 한마디로 디렉터리 엔트리를 차례로 조사한다는 것이다. 이처럼 그릇과 내용물을 동일시하여 재귀적인 구조를 만드는 것이 Composite 패턴이다. 다이어그램을 보면서 자세히 살펴보자\n출처:https://mygumi.tistory.com/343\nLeaf(잎) - \u0026quot;내용물\u0026quot;로 내부에 다른 것을 넣을 수 없다. Composite(복합체) - \u0026ldquo;그릇\u0026quot;으로 Leaf나 또 다른 Composite를 넣을 수 있다. Component - Leaf와 Composite를 동일시하는 역할을 한다. Leaf역과 Composite역에 공통되는 상위 클래스로 구현된다. Client - Composite 패턴의 사용자이다. 다이어그램을 보면 Composite가 포함하는 Component(Leaf 나 Composite)를 부모에 대한 자식으로 간주한다. getChild() 메서드는 Component로부터 자식을 얻는 메서드이다.\n2. 예제 \u0026quot;JAVA 언어로 배우는 디자인 패턴 입문 3편\u0026quot;의 예제로 Composite 패턴을 구현해 보자. 처음 예시로 든 것처럼 파일/디렉터리/디렉터리 엔트리의 관계를 구현하였다.\nEntry - File과 Directory를 동일시하는 추상 클래스 File - 파일 클래스 Directory - 디렉터리 클래스 2-1. Entry 클래스 이름과 크기를 얻기 위한 getName, getSize 메서드를 정의하고 하위 클래스에 구현을 맡긴다. 오버로딩된 printList(), printList(String) 메서드 중 printList()는 public으로 구현하여 외부에 공개하고 printList(String)은 protected로 Entry 하위 클래스에서만 사용할 수 있도록 하였다. public abstract class Entry { // 이름을 얻는다 public abstract String getName(); // 크기를 얻는다 public abstract int getSize(); // 목록을 표시한다 public void printList() { printList(\u0026#34;\u0026#34;); } // prefix를 앞에 붙여서 목록을 표시한다 protected abstract void printList(String prefix); // 문자열 표시 @Override public String toString() { return getName() + \u0026#34; (\u0026#34; + getSize() + \u0026#34;)\u0026#34;; } } 2-2. File 클래스 \u0026quot;파일\u0026quot;을 나타내는 클래스이며 Entry 하위 클래스로 선언되어 있다. getName, getSize, printList(String)을 여기서 구현한다.\npublic class File extends Entry { private String name; private int size; public File(String name, int size) { this.name = name; this.size = size; } @Override public String getName() { return name; } @Override public int getSize() { return size; } @Override protected void printList(String prefix) { System.out.println(prefix + \u0026#34;/\u0026#34; + this); } } 2-3. Directory 클래스 \u0026quot;디렉터리\u0026quot;를 표현하는 클래스로 File클래스와 같이 Entry 클래스의 하위 클래스로 선언되어 있다. directory는 디렉터리 엔트리를 보관해 두는 필드로, List \u0026lt;Entry\u0026gt; 형으로 선언되어 있다. Entry는 파일의 인스턴스인지, 디렉터리의 인스턴스인지 알 수 없지만 getSize() 메서드를 통해 일괄적으로 파일크기를 알 수 있다. public class Directory extends Entry { private String name; private List\u0026lt;Entry\u0026gt; directory = new ArrayList\u0026lt;\u0026gt;(); public Directory(String name) { this.name = name; } @Override public String getName() { return name; } @Override public int getSize() { int size = 0; for (Entry entry: directory) { size += entry.getSize(); } return size; } @Override protected void printList(String prefix) { System.out.println(prefix + \u0026#34;/\u0026#34; + this); for (Entry entry: directory) { entry.printList(prefix + \u0026#34;/\u0026#34; + name); } } // 디렉터리 엔트리를 디렉터리에 추가한다 public Entry add(Entry entry) { directory.add(entry); return this; } } 다음과 같이 Directory / File 클래스(그릇과 내용물)를 같은 것으로 본다는 것이 Composite패턴의 특징이다. add(Entry) 메서드를 통해 디렉터리 엔트리에 파일, 혹은 디렉터리를 추가하지만, 추가하는 entry가 파일인지 디렉터리인지를 세부적으로 체크하진 않는다. 또한 둘 다 Entry의 하위 클래스 인스턴스이기 때문에 getSize() 메서드를 안심하고 호출할 수 있다. Entry 하위에 새로운 클래스가 생성되어도 getSize()를 적절하게 구현할 것이기에 기존 클래스에서 이 부분을 수정할 필요가 없다. getSize() 메서드를 자세히 살펴보면, Entry가 Directory의 인스턴스일 때 entry.getSize()를 루프를 돌며 더하며, 그 안에 디렉터리가 있으면 다시 하위 디렉터리의 getSize() 메서드를 호출한다. 이런 식으로 재귀적으로 메서드가 호출되게 되는데, Composite 패턴의 재귀적 구조가 그대로 getSize 메서드의 재귀적 호출에 대응한다는 것을 볼 수 있다.\n3. 결론 어디에 적용할 수 있을까? \u0026quot;JAVA 언어로 배우는 디자인 패턴 입문 3편\u0026quot; 도서에서는 동작테스트 시 KeyboardTest, FileTest, NetworkTest 등을 모아 InputTest로 한 번에 다루거나 매크로 명령어를 재귀적 구조로 구현하여 매크로 명령의 매크로 명령을 만드는 것을 예제로 들고 있다. 일반적인 트리구조로 된 데이터 구조는 Composite 패턴에 해당하며, 재귀적 특징을 주의해서 사용한다면 그룹과 개인이 같은 특징을 가지거나 동시에 변경할 여지가 많은 부분에 효율적으로 적용 가능하다. 참고 : JAVA 언어로 배우는 디자인 패턴 입문 3편\n상세 예제소스는 깃허브에서 확인가능\nhttps://github.com/junhkang/java-design-pattern/tree/master/src/main/java/com/example/javadesignpattern/composite\n","permalink":"http://localhost:50666/posts/63/","summary":"\u003chr\u003e\n\u003ch2 id=\"컴포지트composite-패턴이란\" ke-size=\"size26\"\u003e1. 컴포지트(Composite) 패턴이란?\u003c/h2\u003e\n\u003cp\u003eComposite는 혼합물, 복합물이란 뜻으로 중첩된 구조, 재귀적인 구조를 만드는 패턴이다. 대표적인 예로 윈도우 디렉터리와 파일을 들 수 있다. 디렉터리, 파일은 엄연히 다른 속성이지만 둘 다 디렉터리 안에 넣을 수 있다는 공통점이 있다. 디렉터리 내에는 또 다른 디렉터리가 있을 수 있기에 중첩, 재귀적인 구조를 만들어낸다.\n \u003c/p\u003e\n\u003cp\u003e디렉터리와 파일을 합쳐 디렉터리 엔트리라고 부르기도 한다. 두 속성을 같은 종류로 간주하는 것이다. 어떤 디렉터리 안에 무엇이 있는지 차례대로 조사할 때 조사하는 것이 디렉터리일 수도, 파일일 수도 있다, 한마디로 디렉터리 엔트리를 차례로 조사한다는 것이다.\n \u003c/p\u003e","title":"[디자인패턴] 컴포지트(Composite) 패턴의 개념, 예제, 장단점, 활용"},{"content":" 1. 개념 단순히 정적 메서드와 정적 필드만을 담은 클래스를 생성하는 경우가 있다. 객체 지향적 사고하지 않는 이들이 종종 남용하지만 쓰임새는 분명히 존재한다. 단순히 정적 메서드와 정적 필드만으로 클래스를 생성하는 경우\n기본 타입 값이나 배열 관련 메서드의 집합 ex) java.lang.Math, java.Util.Arrays 특정 인터페이스를 구현하는 객체를 생성해 주는 정적 메서드의 집합 ex) java.util.Collections final 클래스와 관련된 메서드의 집합 (final 클래스를 상속해서 하위 클래스에 메서드를 넣는 것은 불가능하기 때문) 해당 정적 멤버만 담은 유틸리티 클래스는 인스턴스로 만들어 쓰기 위해 설계한 것이 아니지만 생성자를 명시하지 않으면 컴파일러가 자동으로 기본 생성자를 만들어주기에 매개변수를 받지 않는 public 생성자가 만들어지며, 사용자는 이 생성자가 자동생성된 것인지 구분할 수 없다. 실제로 공개된 API들에서도 이처럼 의도치 않게 인스턴스화할 수 있게 된 클래스가 종종 보인다. 단순히 추상 클래스로 만드는 것으로는 인스턴스화를 막을 수 없다. 하위 클래스를 만들어 인스턴스화하면 그만이기 때문이다. 이 경우 사용자는 상속해서 사용하라는 것으로 오해할 수도 있으니 문제이지만, 다행히 인스턴스화를 막는 것은 간단하다. 컴파일러가 기본생성자를 만드는 경우는 오직 명시된 생성자가 없을 때뿐이니 private 생성자를 추가하면 클래스의 인스턴스화를 막으면 된다. 책의 예제를 확인해보자.\npublic class UtilityClass { // 기본 생성자가 만들어지는 것을 막는다(인스턴스화 방지용). private UtilityClass() { throw new AssertionError(); } } 명시적 생성자가 private이니 클래스 밖에서는 접근불가하다. AssertionError()도 필요 없지만 혹시 클래스 내에서 호출된 경우를 방지한다. 이 코드는 어떤 상황에서도 클래스가 인스턴스화되는 것을 방지한다.\n(하지만 생성자가 존재하는데 호출이 불가한 상태이니 코드만 봐선 직관적이지 않다. 앞의 코드처럼 적절한 주석을 달아주자) 이방식은 상속을 불가능하게 하는 효과도 있다. 모든 생성자는 명시적이든 묵시적이든 상위 클래스의 생성자를 호출하게 된다. 이를 private으로 선언해 두면 하위 클래스가 상위 클래스의 생성자에 접근할 길이 없게 된다.\n2. 결론 객체 지향적 사고와 어울리지 않지만 정적 메서드/정적필드만을 담을 클래스를 생성해야 할 경우가 있다. 이 경우에는 private 생성자를 통해 인스턴스화를 막아주어 클래스 내/외부에서의 생성자 호출을 막아주자. 책의 예제 소스와 상세 내용은 다음 repo에서 확인 가능\nhttps://github.com/junhkang/effective-java-summary\nhttps://github.com/junhkang/effective-java-summary/tree/master/src/main/java/org/example/ch01/item04\n","permalink":"http://localhost:50666/posts/62/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/62/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"개념\" ke-size=\"size26\"\u003e1. 개념\u003c/h2\u003e\n\u003cp\u003e단순히 정적 메서드와 정적 필드만을 담은 클래스를 생성하는 경우가 있다. 객체 지향적 사고하지 않는 이들이 종종 남용하지만 쓰임새는 분명히 존재한다.\n \u003c/p\u003e\n\u003cp\u003e단순히 정적 메서드와 정적 필드만으로 클래스를 생성하는 경우\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e기본 타입 값이나 배열 관련 메서드의 집합 ex) java.lang.Math, java.Util.Arrays\u003c/li\u003e\n\u003cli\u003e특정 인터페이스를 구현하는 객체를 생성해 주는 정적 메서드의 집합 ex) java.util.Collections\u003c/li\u003e\n\u003cli\u003efinal 클래스와 관련된 메서드의 집합 (final 클래스를 상속해서 하위 클래스에 메서드를 넣는 것은 불가능하기 때문)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e해당 정적 멤버만 담은 유틸리티 클래스는 인스턴스로 만들어 쓰기 위해 설계한 것이 아니지만 생성자를 명시하지 않으면 컴파일러가 자동으로 기본 생성자를 만들어주기에 매개변수를 받지 않는 public 생성자가 만들어지며, 사용자는 이 생성자가 자동생성된 것인지 구분할 수 없다.\n \u003c/p\u003e","title":"[이펙티브 자바] 4. 인스턴스화를 막으려거든 private 생성자를 사용하라"},{"content":" 1. 추상 팩토리(Abstract Factory) 패턴 이란? 추상 - 구체적으로 어떻게 구현되는지 생각하지 않고 인터페이스(API)에만 주목하는 상태\n공장 - 부품을 조립하여 제품 완성 추상 + 공장 패턴 : 추상적인 공장에서 추상적인 부품을 조합하여 추상적인 제품을 만든다. 부품의 구체적인 구현에 집중하지 않고 인터페이스에 주목, 인터페이스만 사용하여 부품을 조립하고 제품으로 완성한다. 다음 표를 보면 추상 팩토리가 어떤 구조로 이루어졌는지 확인할 수 있다.\nAbstract Factory : 최상위 공장, 메서드들을 추상화한다. AbstractProduct의 인스턴스를 만들기 위한 인터페이스를 결정한다. Concrete Factory : 서브 공장 클래스, 유형에 맞는 객체를 반환하도록 메서드들을 재정의한다. AbstractFactory의 인터페이스를 구현한다. Abstract Product : 타입의 제품을 추상화한 인터페이스이다. AbstractFactory에 의해 만들어지는 추상적인 부품이나 제품의 인터페이스(API)를 결정한다. ConcreteProduct : 각 유형의 구현체, 팩토리 객체로부터 생성한다. AbstractProduct의 인터페이스를 구현한다. 2. 예제 사용할 예제는 계층 구조로 된 링크 페이지를 HTML파일로 바꾸는 코드이다. (\u0026quot;JAVA 언어로 배우는 디자인 패턴 입문 3편\u0026quot;의 예제 활용) HTML 계층 구조를 추상 팩토리 패턴을 통해 구현한 것으로 2개의 패키지로 분리된 클래스군으로 구성되어 있다.\nfactory : 추상적인 공장, 부품, 제품을 포함하는 패키지 listFactory : 구체적인 공장, 부품, 제품을 포함하는 패키지 factory 하위의 추상 공장/부품과 listFactory 하위의 구체적인 부품/공장을 통해 html list를 구현한다.\n2-1. 추상적인 부품 - Item, Link, Tray HTML 요소들을 다룰 추상적인 부품들을 정의한다. Link와 Tray를 통일적으로 다루기 위한 Item 클래스를 생성한다. HTML 문자열을 반환하는 makeHTML()은 추상메서드로 선언하여 하위 클래스에서 상황에 맞게 구현할 수 있게 한다.\n2-1-1. Item public abstract class Item { protected String caption; public Item(String caption) { this.caption = caption; } public abstract String makeHTML(); } 2-1-2. Link public abstract class Link extends Item { protected String url; public Link(String caption, String url) { super(caption); this.url = url; } } 2-1-3. Tray public abstract class Tray extends Item { protected List\u0026lt;Item\u0026gt; tray = new ArrayList\u0026lt;\u0026gt;(); public Tray(String caption) { super(caption); } public void add(Item item) { tray.add(item); } } 2-2. 추상적인 공장 - Factory class명을 통해 구체적인 공장의 인스턴스를 생성한다. getFactory를 통해 구체적인 공장 인스턴스를 생성하지만, 리턴값은 추상적인 공장(Factory) 임을 주의하자. 추상 부품들을 반환하는 createLink, createTray, createPage 같은 추상 메서드들은 메서드 이름과 시그니처만 여기서 확실히 정의하고, 제품의 구제적인 생성 및 부품 선정은 하위 클래스에게 일임한다.\npublic abstract class Factory { public static Factory getFactory(String classname) { Factory factory = null; try { factory = (Factory)Class.forName(classname).getDeclaredConstructor().newInstance(); } catch (ClassNotFoundException e) { System.out.println(classname + \u0026#34; 클래스가 발견되지 않았습니다.\u0026#34;); } catch (Exception e) { e.printStackTrace(); } return factory; } public abstract Link createLink(String caption, String url); public abstract Tray createTray(String caption); public abstract Page createPage(String title, String author); } 2-3. 구체적인 공장 - ListFactory Factory 클래스의 createLink, createTray, createPage 추상 메스드들을 구체적으로 정의한다. public class ListFactory extends Factory { @Override public Link createLink(String caption, String url) { return new ListLink(caption, url); } @Override public Tray createTray(String caption) { return new ListTray(caption); } @Override public Page createPage(String title, String author) { return new ListPage(title, author); } } 2-4. 구체적인 부품 - ListLink, ListTray 상위 클래스의 makeHTML 추상 메서드를 구현한다. 각 클래스의 요청에 맞는 HTML을 파싱 하여 String 형태로 리턴한다.\n2-4-1. ListLink public class ListLink extends Link { public ListLink(String caption, String url) { super(caption, url); } @Override public String makeHTML() { return \u0026#34; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;\u0026#34; + url + \u0026#34;\u0026#34;\u0026gt;\u0026#34; + caption + \u0026#34;\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\\n\u0026#34;; } } 2-4-2. ListTray public class ListTray extends Tray { public ListTray(String caption) { super(caption); } @Override public String makeHTML() { StringBuilder sb = new StringBuilder(); sb.append(\u0026#34;\u0026lt;li\u0026gt;\\n\u0026#34;); sb.append(caption); sb.append(\u0026#34;\\n\u0026lt;ul\u0026gt;\\n\u0026#34;); for (Item item: tray) { sb.append(item.makeHTML()); } sb.append(\u0026#34;\u0026lt;/ul\u0026gt;\\n\u0026#34;); sb.append(\u0026#34;\u0026lt;/li\u0026gt;\\n\u0026#34;); return sb.toString(); } } 3. Abstract Factory 패턴의 장단점 Abstract Factory 패턴에 Concrete Factory(구체적인 공장)을 추가하는 것은 간단하다. 어떤 클래스를 만들고 어떤 메서드를 구현해야 하는지가 명확하기 때문이다. 예제에서 ListFactory 외에 다른 Factory를 생성하려 한다면, Factory, Link, Tra 하위 클래스를 생성하고 각각 추상 메서드를 다시 구현하면 된다. 이 과정에서 Abstract Factory(추상 공장)에는 어떠한 수정도 가해지지 않는다. 여기서 오는 장점으로는 객체 생성코드의 확장성 보장 객체 간의 결합도 낮춤 구현체 클래스에 대한 의존성 감소 하지만 공장을 추가하는 게 아닌 부품을 추가해야 한다면 어떨까? Factory 추상 팩토리에 Picture라는 부품을 추가해야 한다면, 이미 구현된 Concrete Factory 전체를 Picture에 대응하도록 수정해야 한다. 현재 예제에서는 createPicture라는 메서드를 모든 구체적인 공장에 추가해 주어야 한다. 이미 만들어진 공장이 많을수록 더 큰 작업이 될 것이다. 여기서 오는 단점으로는 복잡한 구조 유연성이 저하 추가적인 클래스 생성 필요 4. 결론 / 활용 같은 유형의 다양한 제품, 부품을 생성할 때 굉장히 효율적인 패턴이다. 수정에는 닫혀있고 확장에는 열려있는 패턴으로 객체 간의 결합도를 낮춰주지만 추가적인 클래스 생성으로 유연성이 떨어지고 복잡한 구조가 될 우려가 있다. 확장 방향성에 대한 충분한 검토가 끝난 후 적용해야 효율을 볼 수 있다. 참고 : JAVA 언어로 배우는 디자인 패턴 입문 3편\n상세 예제소스는 깃허브에서 확인가능\nhttps://github.com/junhkang/java-design-pattern/tree/master/src/main/java/com/example/javadesignpattern/abstractFactory ","permalink":"http://localhost:50666/posts/61/","summary":"\u003chr\u003e\n\u003ch2 id=\"추상-팩토리abstract-factory-패턴-이란\" ke-size=\"size26\"\u003e1. 추상 팩토리(Abstract Factory) 패턴 이란?\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e추상 - 구체적으로 어떻게 구현되는지 생각하지 않고 인터페이스(API)에만 주목하는 상태\u003cbr\u003e\n공장 - 부품을 조립하여 제품 완성\n \u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003e추상 + 공장 패턴 :\u003c/strong\u003e 추상적인 공장에서 추상적인 부품을 조합하여 추상적인 제품을 만든다. 부품의 구체적인 구현에 집중하지 않고 인터페이스에 주목, 인터페이스만 사용하여 부품을 조립하고 제품으로 완성한다.\n \u003c/p\u003e\n\u003cp\u003e다음 표를 보면 추상 팩토리가 어떤 구조로 이루어졌는지 확인할 수 있다.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/61/img.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAbstract Factory :\u003c/strong\u003e 최상위 공장, 메서드들을 추상화한다. AbstractProduct의 인스턴스를 만들기 위한 인터페이스를 결정한다.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConcrete Factory :\u003c/strong\u003e 서브 공장 클래스, 유형에 맞는 객체를 반환하도록 메서드들을 재정의한다. AbstractFactory의 인터페이스를 구현한다.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAbstract Product :\u003c/strong\u003e 타입의 제품을 추상화한 인터페이스이다. AbstractFactory에 의해 만들어지는 추상적인 부품이나 제품의 인터페이스(API)를 결정한다.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConcreteProduct :\u003c/strong\u003e 각 유형의 구현체, 팩토리 객체로부터 생성한다. AbstractProduct의 인터페이스를 구현한다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"예제\" ke-size=\"size26\"\u003e2. 예제\u003c/h2\u003e\n\u003cp\u003e사용할 예제는 계층 구조로 된 링크 페이지를 HTML파일로 바꾸는 코드이다. (\u0026quot;JAVA 언어로 배우는 디자인 패턴 입문 3편\u0026quot;의 예제 활용) HTML 계층 구조를 추상 팩토리 패턴을 통해 구현한 것으로 2개의 패키지로 분리된 클래스군으로 구성되어 있다.\u003c/p\u003e","title":"[디자인패턴] 추상 팩터리(Abstract Factory) 패턴의 개념, 예제, 장단점, 활용"},{"content":" 1. 문제 상황 JDK21을 서버에 적용하면서, 기존 버전과 동시에 사용해야 하는 상황 발생\n2. 해결 물론 빌드할 때마다 자바 홈 경로를 바꿔서 적용하거나, 각 톰캣의 catalina.sh에 echo JAVA_HOME=개별 JAVA경로 혹은 startup.sh 스크립트 상단에 개별 JAVA_HOME을 명시하는 방법도 있지만, 심볼릭 링크를 통해 여러 패키지를 관리하는 방식인 update-alternatives를 통해 적용하였다. 개념은 심볼릭링크 (상세 내용은 아래 링크 참고)\n[Linux] - [Linux] 심볼릭 링크 (Symbolic link) 설정하기\n를 설정 후에 필요한 시점에 해당 심볼릭 링크에 연결된 링크를 선택하여 사용하는 방법이다. 2-1. update-alternatives 설정 확인 update-alternatives --list 를 통해 현재 java에 매핑된 경로를 확인해 보자\n현재 jdk21 버전을 보고 있고, 여기에 다른 버전의 jdk를 추가한 후 그때그때 선택할 수 있도록 수정해 보겠다. 2-2. 다른 버전의 jdk 경로 등록 1.8 버전의 jdk를 추가하고 싶다면 다음과 같이 /etc/alternatives/java의 심볼릭 링크에 신규 버전의 java 경로를 등록해 주면 된다.\nupdate-alternatives --install /etc/alternatives/java java /lib/jvm/jdk-21/java/bin 1 update-alternatives --install /etc/alternatives/java java /lib/jvm/java-1.8.0-openjdk/java/bin 2 맨뒤의 숫자 1, 2는 우선순위로, 여러 개의 링크가 등록될 경우 디폴트 값을 정의하며 숫자가 높을수록 우선순위가 높다. (현재 스크립트상으로는 jdk1.8이 기본 링크로 설정되어 있다.) 혹시 잘못된 경로를 등록하였다면 --remove를 통해 삭제가 가능하다.\nupdate-alternatives --remove java /lib/jvm/jdk-21/java/bin 2-3. 링크 정상 등록 확인 update-alternatives --config java 두 옵션 중 한 가지를 선택한 후 java -version을 확인해 보면 선택한 jdk 경로로 정상 변경된 것을 확인할 수 있다.\n2-4. java -version이 안될 경우 (java 경로를 못 찾을 경우) /etc/alternatives/java 경로에 심볼릭 링크를 연결하였기에 자바 경로를 해당 경로와 맞춰주어야 한다.\nvi /etc/bashrc 스크립트에 java, javac에 해당하는 alias를 등록\nalias java=\u0026#34;/etc/alternatives/java alias javac=\u0026#34;/etc/alternatives/javac bashrc 적용\nsource /etc/bashrc java -version을 입력할 시 정상적으로 선택한 java 경로를 찾아가게 된다.\n3. 결론 실제 운영서버는 jdk버전을 통일할 것이기에 상관없지만, jdk를 여러버전 써야 하는 서버의 경우 매번 alternatives 옵션으로 선택을 하는 것이 과연 효율적 일지, 배포 시 실수로 이어지기 쉬운 작업은 아닌지 고려해봐야할 듯하다. alternatives를 배포시 자동으로 설정하는 스크립트를 생성하여 alias를 생성하던지, 최초 언급했던 대로 자바 환경변수 설정자체를 톰캣별 스크립트에 설정하는 것도 고려해 보아야겠다. (운영 과정에서 더 나은 방법이나 변경되는 내용이 있다면 내용 추가 예정)\n","permalink":"http://localhost:50666/posts/60/","summary":"\u003chr\u003e\n\u003ch2 id=\"문제-상황\" style=\"color: #3a4954; text-align: start;\" ke-size=\"size26\"\u003e1. 문제 상황\u003c/h2\u003e\n\u003cp\u003eJDK21을 서버에 적용하면서, 기존 버전과 동시에 사용해야 하는 상황 발생\u003c/p\u003e\n\u003ch2 id=\"해결\" style=\"color: #3a4954; text-align: start;\" ke-size=\"size26\"\u003e2. 해결\u003c/h2\u003e\n\u003cp\u003e물론 빌드할 때마다 자바 홈 경로를 바꿔서 적용하거나, 각 톰캣의 catalina.sh에 \u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eecho JAVA_HOME=개별 JAVA경로\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e혹은 startup.sh 스크립트 상단에 개별 JAVA_HOME을 명시하는 방법도 있지만, 심볼릭 링크를 통해 여러 패키지를 관리하는 방식인 \u003cstrong\u003eupdate-alternatives\u003c/strong\u003e를 통해 적용하였다.\n \u003c/p\u003e\n\u003cp\u003e개념은 심볼릭링크 (상세 내용은 아래 링크 참고)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://junhkang.tistory.com/16\"\u003e[Linux] - [Linux] 심볼릭 링크 (Symbolic link) 설정하기\u003c/a\u003e\u003c/p\u003e","title":"[Linux] JDK 여러 버전 적용하기, 여러개의 패키지 관리하기 (update-alternatives)"},{"content":" 1. 문제 상황 스프링 부트 3, 스프링 시큐리티 6 업데이트가 되며 기존에 사용 중인 WebSecurityConfigurerAdapter를 더 이상 지원하지 않기에\n기존의 WebSecurityConfigurerAdapter를 상속받아 기능을 구현하는 대신 컴포넌트화 시켜서 구현해야한다.\nSecurityFilterChain으로 스프링 시큐리티를 구현 중\n(antMacher -\u0026gt; requestMacher, dispatcherTypeMatcher설정 등 기본 Spring Security6.0 마이그레이션 가이드는 적용 완료 )\nSecurityContext에 로그인 시도 시 정상적으로 인증정보를 바인딩하고 있으나, 페이지 이동 시 인증정보가 없어지는 현상 발생\n2. 해결 SecurityContextPersistenceFilter SecurityContext의 영속성을 컨트롤하는 SecurityContextPersistenceFilter의 설정이 누락되어 있었다. SecurityContext의 영속화는 SecurityContextRepository를 통해 이루어지며 SecurityContextHolder에 SecurityContext를 넣어 요청 전반에 걸쳐 SecurityContext를 사용할 수 있도록 해준다.\n따라서 HttpSecurity에 SecurityContextPersistenceFilter 설정을 추가해 주면 정상적으로 로그인 세션이 유지된다.\n.addFilterBefore(new SecurityContextPersistenceFilter(), BasicAuthenticationFilter.class) * 마이그레이션 전부터 세션유지시간이 커스텀 되어 있었기에 이번 케이스와는 별개지만, SecurityContextPersistenceFilter가 정상적으로 설정되어 있는데도 로그인 메서드 내에서는 정상적으로 인증정보가 등록되지만, 얼마 지나지 않아 풀린다면 스프링시큐리티 세션 시간, 혹은 세션 생성 설정을 확인해봐야 할 수도 있다.\n참고\nhttps://docs.spring.io/spring-security/site/docs/4.0.x/apidocs/org/springframework/security/config/http/SessionCreationPolicy.html\nhttps://docs.spring.io/spring-security/reference/5.8/migration/index.html\n","permalink":"http://localhost:50666/posts/59/","summary":"\u003chr\u003e\n\u003ch2 id=\"문제-상황\" style=\"color: #3a4954; text-align: start;\" ke-size=\"size26\"\u003e1. 문제 상황\u003c/h2\u003e\n\u003cp\u003e스프링 부트 3, 스프링 시큐리티 6 업데이트가 되며 기존에 사용 중인 WebSecurityConfigurerAdapter를 더 이상 지원하지 않기에\u003c/p\u003e\n\u003cp\u003e기존의 WebSecurityConfigurerAdapter를 상속받아 기능을 구현하는 대신 컴포넌트화 시켜서 구현해야한다.\u003c/p\u003e\n\u003cp\u003eSecurityFilterChain으로 스프링 시큐리티를 구현 중\u003c/p\u003e\n\u003cp\u003e(antMacher -\u0026gt; requestMacher, dispatcherTypeMatcher설정 등 기본 Spring Security6.0 \u003ca href=\"https://docs.spring.io/spring-security/reference/5.8/migration/index.html\"\u003e마이그레이션 가이드\u003c/a\u003e는 적용 완료\n)\u003c/p\u003e\n\u003cp\u003eSecurityContext에 로그인 시도 시 정상적으로 인증정보를 바인딩하고 있으나, 페이지 이동 시 인증정보가 없어지는 현상 발생\u003c/p\u003e\n\u003ch2 id=\"해결\" style=\"color: #3a4954; text-align: start;\" ke-size=\"size26\"\u003e2. 해결\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-ebnf\" data-lang=\"ebnf\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eSecurityContextPersistenceFilter\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eSecurityContext의 영속성을 컨트롤하는 SecurityContextPersistenceFilter의 설정이 누락되어 있었다. \u003c/p\u003e","title":"[Spring] 스프링부트 3, 스프링 시큐리티 6, 로그인 세션 유지, 로그인 풀림"},{"content":" 1. 문제 상황 스프링부트 3 버전 업데이트 중 jstl URI / 라이브러리를 찾지 못하는 현상 발생\nhttp://java.sun.com/jsp/jstl/core]을(를), web.xml 또는 이 애플리케이션과 함께 배치된 JAR 파일 내에서 찾을 수 없습니다.\n2. 해결 타 라이브러리와 동일하게 javax -\u0026gt; jakarta 기반 jstl maven dependency를 추가해주면 된다.\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;jakarta.servlet.jsp.jstl\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jakarta.servlet.jsp.jstl-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/jakarta.servlet/jakarta.servlet-api --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;jakarta.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jakarta.servlet-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;6.0.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;jakarta.servlet.jsp\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jakarta.servlet.jsp-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;jakarta.el\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jakarta.el-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.0.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.glassfish.web\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jakarta.servlet.jsp.jstl\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ","permalink":"http://localhost:50666/posts/58/","summary":"\u003chr\u003e\n\u003ch2 id=\"문제-상황\" style=\"color: #3a4954; text-align: start;\" ke-size=\"size26\"\u003e1. 문제 상황\u003c/h2\u003e\n\u003cp\u003e스프링부트 3 버전 업데이트 중 jstl URI / 라이브러리를 찾지 못하는 현상 발생\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"http://java.sun.com/jsp/jstl/core\"\u003ehttp://java.sun.com/jsp/jstl/core\u003c/a\u003e]을(를), web.xml 또는 이 애플리케이션과 함께 배치된 JAR 파일 내에서 찾을 수 없습니다.\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch2 id=\"해결\" style=\"color: #3a4954; text-align: start;\" ke-size=\"size26\"\u003e2. 해결\u003c/h2\u003e\n\u003cp\u003e타 라이브러리와 동일하게 javax -\u0026gt; jakarta 기반 jstl maven dependency를 추가해주면 된다.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e \u0026lt;dependency\u0026gt;\n            \u0026lt;groupId\u0026gt;jakarta.servlet.jsp.jstl\u0026lt;/groupId\u0026gt;\n            \u0026lt;artifactId\u0026gt;jakarta.servlet.jsp.jstl-api\u0026lt;/artifactId\u0026gt;\n            \u0026lt;version\u0026gt;3.0.0\u0026lt;/version\u0026gt;\n        \u0026lt;/dependency\u0026gt;\n        \u0026lt;!-- https://mvnrepository.com/artifact/jakarta.servlet/jakarta.servlet-api --\u0026gt;\n        \u0026lt;dependency\u0026gt;\n            \u0026lt;groupId\u0026gt;jakarta.servlet\u0026lt;/groupId\u0026gt;\n            \u0026lt;artifactId\u0026gt;jakarta.servlet-api\u0026lt;/artifactId\u0026gt;\n            \u0026lt;version\u0026gt;6.0.0\u0026lt;/version\u0026gt;\n        \u0026lt;/dependency\u0026gt;\n        \u0026lt;dependency\u0026gt;\n            \u0026lt;groupId\u0026gt;jakarta.servlet.jsp\u0026lt;/groupId\u0026gt;\n            \u0026lt;artifactId\u0026gt;jakarta.servlet.jsp-api\u0026lt;/artifactId\u0026gt;\n            \u0026lt;version\u0026gt;3.1.0\u0026lt;/version\u0026gt;\n            \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n        \u0026lt;/dependency\u0026gt;\n        \u0026lt;dependency\u0026gt;\n            \u0026lt;groupId\u0026gt;jakarta.el\u0026lt;/groupId\u0026gt;\n            \u0026lt;artifactId\u0026gt;jakarta.el-api\u0026lt;/artifactId\u0026gt;\n            \u0026lt;version\u0026gt;5.0.0\u0026lt;/version\u0026gt;\n        \u0026lt;/dependency\u0026gt;\n        \u0026lt;dependency\u0026gt;\n            \u0026lt;groupId\u0026gt;org.glassfish.web\u0026lt;/groupId\u0026gt;\n            \u0026lt;artifactId\u0026gt;jakarta.servlet.jsp.jstl\u0026lt;/artifactId\u0026gt;\n            \u0026lt;version\u0026gt;3.0.1\u0026lt;/version\u0026gt;\n        \u0026lt;/dependency\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e","title":"[Spring] 스프링부트 3 jstl 적용(maven), (절대 URI인 [http://java.sun.com/jsp/jstl/core]을(를), web.xml 또는 이 애플리케이션과 함께 배치된 JAR 파일 내에서 찾을 수 없습니다.)"},{"content":" 1. 문제 상황 JDK21 버전 업 중 java: java.lang.NoSuchFieldError: Class com.sun.tools.javac.tree.JCTree$JCImport does not have member field 'com.sun.tools.javac.tree.JCTree qualid'\n라는 에러와 함께 빌드 실패\n2. 해결 JDK21과 호환되지 않는 롬복(lombok) 버전 문제로 확인, (JDK21과 호환되는 최소 롬복(lombok) 버전은 1.18.30이다.)\n\u0026lt;!-- https://mvnrepository.com/artifact/org.projectlombok/lombok --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.30\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 기존 사용 중인 구버전 롬복(lombok)을 작성일 기준 최신버전이자 JDK21 호환 최소 버전인 1.18.30으로 변경해 주면 정상적으로 빌드가 된다.\n3. 추가 스프링부트 3, JDK21 버전업을 완료하고 업데이트 중 해결한 문제들을 포스트로 남기던 와중에 돌아보니,\njava: java.lang.NoSuchFieldError: Class com.sun.tools.javac.tree.JCTree$JCImport does not have member field 'com.sun.tools.javac.tree.JCTree qualid\n해당 에러는 롬복뿐만 아니라 JDK21에서 호환 안 되는 라이브러리가 있을 시 자주 보이는 에러이다. 혹시 JDK 업데이트 중 해당 에러를 발견한 것이라면, 롬복뿐 아니라 다른 라이브러리에 대한 버전 검토를 해야 한다. 참고\nhttps://stackoverflow.com/questions/77171270/compilation-error-after-upgrading-to-jdk-21-nosuchfielderror-jcimport-does-n\n* maven repo : https://mvnrepository.com/artifact/org.projectlombok/lombok/1.18.30https://mvnrepository.com/artifact/org.projectlombok/lombok/1.18.30\n","permalink":"http://localhost:50666/posts/57/","summary":"\u003chr\u003e\n\u003ch2 id=\"문제-상황\" style=\"color: #3a4954; text-align: start;\" ke-size=\"size26\"\u003e1. 문제 상황\u003c/h2\u003e\n\u003cp\u003eJDK21 버전 업 중 \u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ejava: java.lang.NoSuchFieldError: Class com.sun.tools.javac.tree.JCTree$JCImport does not have member field 'com.sun.tools.javac.tree.JCTree qualid'\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e라는 에러와 함께 빌드 실패\u003c/p\u003e\n\u003ch2 id=\"해결\" style=\"color: #3a4954; text-align: start;\" ke-size=\"size26\"\u003e2. 해결\u003c/h2\u003e\n\u003cp\u003eJDK21과 호환되지 않는 롬복(lombok) 버전 문제로 확인, (JDK21과 호환되는 최소 롬복(lombok) 버전은 \u003cstrong\u003e1.18.30이다.)\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u0026lt;!-- https://mvnrepository.com/artifact/org.projectlombok/lombok --\u0026gt;\n\u0026lt;dependency\u0026gt;\n     \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt;\n          \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt;\n          \u0026lt;version\u0026gt;1.18.30\u0026lt;/version\u0026gt;\n          \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;\n\u0026lt;/dependency\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e \u003c/p\u003e\n\u003cp\u003e기존 사용 중인 구버전 롬복(lombok)을 작성일 기준 최신버전이자 JDK21 호환 최소 버전인 1.18.30으로 변경해 주면 정상적으로 빌드가 된다.\u003c/p\u003e","title":"[Spring] Java 21 롬복(lombok) 버전 설정 (java: java.lang.NoSuchFieldError: Class com.sun.tools.javac.tree.JCTree$JCImport does not have member field 'com.sun.tools.javac.tree.JCTree qualid')"},{"content":" 1. 자바 스트림(Streams)이란? 기존의 배열 또는 컬렉션 인스턴스는 for, foreach 같은 반복문을 통해 하나씩 핸들링하는 방식이었기에 로직이 복잡할수록 코드양도 많아지고 루프를 여러 번 도는 경우도 발생하였다. 그에 비해 자바 8에서 추가된 스트림(Streams)은 람다를 사용할 수 있는 기술 중 하나로 다음과 같은 특징을 가지고 있다.\n스트림은 데이터의 흐름이라는 뜻으로 컬렉션에 저장되어 있는 요소들을 하나씩 순회하면서 처리할 수 있는 코드패턴이다. 함수형 프로그래밍 언어에서 일반적으로 지원하는 연산관 데이터베이스와 비슷한 연산을 지원한다. 배열/컬렉션의 함수 여러 개를 조합하여 원하는 결과를 필터링, 가공된 결과 추출 가능하며 람다식으로 코드를 간결하게 표현할 수 있다. 하나의 작업을 둘 이상 작업으로 잘게 쪼개 동시에 처리하며 스레드를 이용하여 많은 요소들을 빠르게 처리 가능한다. 컬렉션은 현재 자료구조가 포함하는 모든 값을 메모리에 저장하는 자료구조인데 비해, 스트림은 요청할 때만 요소를 계산하는 고정된 자료 구조이다. 스트림에 사용에 대한 상세한 내용은 다음 3가지로 나눌 수 있다.\n스트림 생성 - 스트림 인스턴스를 생성한다. 스트림 가공 - 필터링/매핑 등 원하는 결과를 가공한다. 스트림 결과 생성 - 스트림 결과를 만들어 내는 작업을 한다. 2. 스트림 생성 2-1. 배열 스트림 (Array Streams) Array.stream() 메서드에 배열의 시작, 끝 인덱스를 인자로 넣으면 배열의 일부만 순회하는 스트림 객체를 생성할 수 있다. (끝 인덱스는 포함되지 않는다.)\nString[] arr = new String[]{\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;}; Stream\u0026lt;String\u0026gt; stream = Arrays.stream(arr); Stream\u0026lt;String\u0026gt; streamOfArrayPart = Arrays.stream(arr, 1, 3); // 1~2 요소 [b, c] 2-2. 컬렉션 스트림 (Collection Streams) 컬렉션 인터페이스의 Stream 메서드로 스트림을 생성할 수 있다.\npublic interface Collection\u0026lt;E\u0026gt; extends Iterable\u0026lt;E\u0026gt; { default Stream\u0026lt;E\u0026gt; stream() { return StreamSupport.stream(spliterator(), false); } // ... } List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;); Stream\u0026lt;String\u0026gt; stream = list.stream(); Stream\u0026lt;String\u0026gt; parallelStream = list.parallelStream(); // 병렬 처리 스트림 2-3. 비어있는 스트림 (Empty Streams) 요소가 없을 때 null 대신 사용 가능한 비어있는 스트림도 생성이 가능하다. public Stream\u0026lt;String\u0026gt; streamOf(List\u0026lt;String\u0026gt; list) { return list == null || list.isEmpty() ? Stream.empty() } 2-4. 빌더 (Builder) 배열이나 컬렉션을 통한 생성이 아닌 직접 원하는 값을 넣어 생성할 수도 있다. build 메서드를 통해 스트림을 리턴한다.\nStream\u0026lt;String\u0026gt; builderStream = Stream.\u0026lt;String\u0026gt;builder() .add(\u0026#34;Eric\u0026#34;).add(\u0026#34;Elena\u0026#34;).add(\u0026#34;Java\u0026#34;) .build(); 2-5. Generator Stream.generate()를 사용하면 Supplier \u0026lt;T\u0026gt;에 해당하는 람다로 값을 넣을 수 있다. 생성되는 스트림의 크기는 무한대 이기 때문에, 특정 사이즈로 최대 크기를 제한해야 한다. \u0026quot;Hi\u0026quot;라는 문자열을 5개 만들어 내는 스트림이다. (제한을 5로 걸지 않는다면 무한대로 생성할 것이다.)\nStream\u0026lt;String\u0026gt; generatedStream = Stream.generate(() -\u0026gt; \u0026#34;Hi\u0026#34;).limit(5); 2-5. iterate Stream.iterate()를 사용하면 초기값과 해당값을 다루는 람다를 사용하여 스트림에 들어간 요소를 만든다. 이 또한 generator와 동일하게 크기가 무한대이기에 특정 사이즈로 제한해야 한다. 초기값 30부터 + 2씩 증가하는 스트림이다.\nStream\u0026lt;Integer\u0026gt; iteratedStream = Stream.iterate(30, n -\u0026gt; n + 2).limit(5); 2-6. 기본 타입형 Streams 원시타입 (int, long, double) 스트림을 제네릭을 사용하지 않고 직접 다룰 수도 있다. range, rangeClose는 범위의 차이이다. 자바 8의 Randm 클래스는 세 가지 타입의 스트림(IntStream, LongStream, DoubleStream)이 생성 가능하다.\nIntStream intStream = IntStream.range(1, 5); // [1, 2, 3, 4] LongStream longStream = LongStream.rangeClosed(1, 5); // [1, 2, 3, 4, 5] DoubleStream doubles = new Random().doubles(3); // 난수 3개 생성 2-7. 파일 스트림 (File Streams) 자바 NIO의 Files 클래스는 파일의 각 라인을 스트링타입의 스트림으로 만들어 준다. 다음은 file.txt 파일의 데이터를 utf-8로 인코딩하여 줄 단위로 읽는 스트림이다.\nStream\u0026lt;String\u0026gt; lineStream = Files.lines(Paths.get(\u0026#34;file.txt\u0026#34;), Charset.forName(\u0026#34;UTF-8\u0026#34;)); 2-8. 병렬 스트림 (Parallel Streams) Stream 대신 parallelStream 메서드를 사용하면 병렬 스트림을 바로 생성할 수 있다. // 병렬 스트림 생성 Stream\u0026lt;Product\u0026gt; parallelStream = productList.parallelStream(); // 배열을 이용한 병렬 스트림 생성 Arrays.stream(arr).parallel(); // 병렬 여부 확인 boolean isParallel = parallelStream.isParallel(); 컬렉션과 배열이 아닌 경우는 parallel 메서드를 사용하여 처리가능하다.\nIntStream intStream = IntStream.range(1, 150).parallel(); boolean isParallel = intStream.isParallel(); 혹은 sequentail 메서드로 다시 되돌릴 수 있다. IntStream intStream = intStream.sequential(); boolean isParallel = intStream.isParallel(); 2-9. 스트림 연결 (Concat) Concat 메서드를 통해 스트림을 합쳐 새로운 스트림을 생성할 수 있다.\nStream\u0026lt;String\u0026gt; stream1 = Stream.of(\u0026#34;Java\u0026#34;, \u0026#34;Scala\u0026#34;, \u0026#34;Groovy\u0026#34;); Stream\u0026lt;String\u0026gt; stream2 = Stream.of(\u0026#34;Python\u0026#34;, \u0026#34;Go\u0026#34;, \u0026#34;Swift\u0026#34;); Stream\u0026lt;String\u0026gt; concat = Stream.concat(stream1, stream2); 3. 스트림 가공 3-1. Filter // 인자로 받는 predicate는 boolean형을 리턴하는 평가식이 들어가야한다. Stream\u0026lt;T\u0026gt; filter(Predicate\u0026lt;? super T\u0026gt; predicate); 스트림 내의 요소를 \u0026quot;필터링\u0026quot;하여 원하는 결과만 걸러내는 작업이다. 조건식에 부합하는 요소만 선별한 스트림을 리턴한다. 다음은 \u0026quot;a\u0026quot;를 포함한 데이터만 뽑아낸 스트림 객체를 리턴하는 예제이다.\nStream\u0026lt;String\u0026gt; stream = names.stream() .filter(name -\u0026gt; name.contains(\u0026#34;a\u0026#34;)); 3-2. Map \u0026lt;R\u0026gt; Stream\u0026lt;R\u0026gt; map(Function\u0026lt;? super T, ? extends R\u0026gt; mapper); 스트림 내의 요소를 하나씩 특정값으로 변환한다. 변환하기 위한 람다를 인자로 받는다. 스트림 내의 값이 input이 되어 특정 로직을 거친 후 리턴 스트림에 담기게 된다. 다음은 스트림 내의 요소를 대문자로 치환한 스트림을 리턴한다.\nStream\u0026lt;String\u0026gt; stream = names.stream() .map(String::toUpperCase); 3-3. flatMap \u0026lt;R\u0026gt; Stream\u0026lt;R\u0026gt; flatMap(Function\u0026lt;? super T, ? extends Stream\u0026lt;? extends R\u0026gt;\u0026gt; mapper); flatMap 메서드의 인자로 받는 mapper는 리턴 타입이 리턴 타입이 Stream이다. 중첩구조를 한 단계 제거하고 단일 컬렉션으로 만들어주는 역할을 한다. 다음은 중첩된 리스트 예제이다.\nList\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; list = Arrays.asList(Arrays.asList(\u0026#34;a\u0026#34;), Arrays.asList(\u0026#34;b\u0026#34;)); flatMap으로 한 껍데기를 벗겨내서 조금 플랫 한 리스트로 변경, 중첩 구조를 제거할 수 있다. List\u0026lt;String\u0026gt; flatList = list.stream() .flatMap(Collection::stream) // (e) -\u0026gt; collection.stream(e)의 축약 .collect(Collectors.toList()); 혹은 객체에 적용하게 되면 다음과 같다. 다음은 학생 객체를 가진 스트림에서 점수만 뽑아 새로운 스트림을 만들어 평균을 구하는 작업으로 map 만으로는 한 번에 할 수 없는 기능이다.\nstudents.stream() .flatMapToInt(student -\u0026gt; IntStream.of(student.getKor(), student.getEng(), student.getMath())) .average().ifPresent(avg -\u0026gt; System.out.println(Math.round(avg * 10)/10.0)); 3-4. sorted Stream\u0026lt;T\u0026gt; sorted(); Stream\u0026lt;T\u0026gt; sorted(Comparator\u0026lt;? super T\u0026gt; comparator); //정렬 시 별도 비교 로직이 있다면 sorted 메서드를 통해 요소를 정렬할 수 있다. 다음과 같이 그냥 호출할 경우 오름차순으로 정렬된다. 정렬할 때 값을 비교하는 별도 로직이 있다면 Comparator를 인자로 넘겨주면 된다.\nIntStream.of(14, 11, 20, 39, 23) .sorted() .boxed() .collect(Collectors.toList()); 3-5. peek Stream\u0026lt;T\u0026gt; peek(Consumer\u0026lt;? super T\u0026gt; action); 특정 결과를 반환하지 않고 스트림 내 요소들 각각에 특정 작업을 수행할 뿐이다. 결과에 영향을 주지 않기에 중간중간 결과를 확인할 때 사용 가능하다.\nint sum = IntStream.of(1, 3, 5, 7, 9) .peek(System.out::println) .sum(); 다음과 같은 방식으로 중간 처리과정을 로깅하는 것도 가능하다.\n4. 스트림 결과 생성 4-1. 통계값 최소, 최대, 합, 평균 등 기본 형 타입으로 결과를 생성할 수 있다.\nlong count = IntStream.of(1, 3, 5, 7, 9).count(); long sum = LongStream.of(1, 3, 5, 7, 9).sum(); int max = IntStream.range(1, 10).max(); int min = IntStream.range(1, 10).min(); int avg = IntStream.range(1, 10).average(); 스트림이 비어있는 경우 count와 sum은 0을 출력하게 되지만, 최대 최소는 표현할 수 없기에 Optional을 이용해 리턴한다.\nOptionalInt min = IntStream.of(1, 3, 5, 7, 9).min(); OptionalInt max = IntStream.of(1, 3, 5, 7, 9).max(); 혹은 스트림에서 ifPresent를 사용해 바로 처리할 수 있다.\nDoubleStream.of(1.1, 2.2, 3.3, 4.4, 5.5) .average() .ifPresent(System.out::println); 4-2. Reduce 중간 연산을 거친 값들은 reduce 메서드로 결괏값을 생성한다.\nreduce는 다음의 3가지 파라미터를 받을 수 있다.\naccumulator - 각 요소를 처리하는 계산 로직 identity - 계산을 위한 초기값 (스트림이 비어서 계산할 내용이 없어도 이 값은 리턴됨) combiner - 병렬 스트림에서 나눠 계산한 결과를 하나로 합치는 로직 // 스트림에서 나오는 값들을 accumulator 함수로 누적 Optional\u0026lt;T\u0026gt; reduce(BinaryOperator\u0026lt;T\u0026gt; accumulator); // 동일하게 accumulator 함수로 누적하지만 초기값(identity)이 있음 T reduce(T identity, BinaryOperator\u0026lt;T\u0026gt; accumulator); \u0026lt;U\u0026gt; U reduce(U identity, BiFunction\u0026lt;U, ? super T, U\u0026gt; accumulator, BinaryOperator\u0026lt;U\u0026gt; combiner); 먼저 인자를 1개만(accumulator) 받는 경우를 보면, OptionalInt reduced = IntStream.range(1, 4) // [1, 2, 3] .reduce((a, b) -\u0026gt; { return Integer.sum(a, b); }); 다음 예제에서는 두 값을 더하는 람다를 넘겨주고 있기에, 배열의 모든 합을 더한 6이 결과가 된다.\n다음은 인자가 2개인(accumulator, identity) 경우를 보면\nint reducedTwoParams = IntStream.range(1, 4) // [1, 2, 3] .reduce(10, Integer::sum); // method reference 최초 값이 10, 스트림 내의 합계 총합계를 더하기에 16을 이 결과가 된다. 마지막으로 인자가 3개인(accumulator, identity, combiner) 경우를 보면\nInteger reducedParallel = Arrays.asList(1, 2, 3) .parallelStream() .reduce(10, Integer::sum, (a, b) -\u0026gt; { System.out.println(\u0026#34;combiner was called\u0026#34;); return a + b; }); 먼저 초기값 10에 각 스트림의 값인 1,2, 3용을 더하여 11,12,13을 생성한다. Combiner는 identity와 accumulator를 가지고 여러 스레드에서 나눠 계산한 결과를 합치기에 11+12+13 = 36을 결과로 반환한다. Combiner는 병렬 처리 시 각 스레드에서 실행한 결과를 마지막에 합치기에 병렬 스트림에서만 작동한다.\n4-3. Collect 자바 스트림을 이용하는 가장 많은 패턴 중 하나로써, 요소의 일부를 필터링하고 값을 변형하여 새로운 Collection을 생성한다.\nCollectors.toList() - 작업결과를 담은 리스트를 반환한다. Collectors.joining() - 스트림 작업 결과를 하나의 String으로 이어서 반환한다. delimeter, prefix, suffix 등을 사용해 문자열을 조합할 수 있다. 다음은 스트림 요소를 [] 안에 쉼표를 기준으로 연결한 스트링을 반환하는 예제이다. String listToString = productList.stream() .map(Product::getName) .collect(Collectors.joining(\u0026#34;, \u0026#34;, \u0026#34;[\u0026#34;, \u0026#34;]\u0026#34;)); Collectors.averageingInt() - 숫자값의 평균을 반환한다. Double averageAmount = productList.stream() .collect(Collectors.averagingInt(Product::getAmount)); Collectors.summingInt() - 숫자 값의 합을 반환한다. Collectors.summarizingInt() - 합계와 평균에 대한 정보를 한 번에 반환한다. (IntSummaryStatistics 객체에는 개수, 합계, 평균, 최소, 최대에 대한 정보가 담겨있다.) IntSummaryStatistics statistics = productList.stream() .collect(Collectors.summarizingInt(Product::getAmount)); Collectors.groupingBy() - 특정 조건으로 요소들을 그룹 지을 수 있다. 어떤 요소가 얼마나 많이 분포하고 있는지 Map타입으로 반환한다. 같은 수량이면 리스트로 묶어서 반환한다. 4-4. Foreach 스트림에서 반환된 각각의 값에 대해 작업을 하고 싶을 때 사용한다. 특정 값을 리턴하지는 않는다. 다음은 1~999 중 짝수만 출력하는 예제이다. Set\u0026lt;Integer\u0026gt; evenNumber = IntStream.range(1, 1000).boxed() .filter(n -\u0026gt; (n%2 == 0)) .forEach(System.out::println); 4-5. Matching 조건식마다 predicate를 받아서 만족하는 요소가 있는지를 체크한 결과를 리턴한다.\n// 하나라도 만족하는 요소가 있는지 boolean anyMatch(Predicate\u0026lt;? super T\u0026gt; predicate); // 모든 조건을 만족하는지 boolean allMatch(Predicate\u0026lt;? super T\u0026gt; predicate); // 모든 조건을 만족하지 않는지 boolean noneMatch(Predicate\u0026lt;? super T\u0026gt; predicate); 참고 https://futurecreator.github.io/2018/08/26/java-8-streams/\nhttps://hbase.tistory.com/171https://hbase.tistory.com/171\n(이미지출처) https://devbksheen.tistory.com/entry/%EB%AA%A8%EB%8D%98-%EC%9E%90%EB%B0%94-%EC%8A%A4%ED%8A%B8%EB%A6%BCStream%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80\n","permalink":"http://localhost:50666/posts/56/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/56/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"자바-스트림streams이란\" ke-size=\"size26\"\u003e1. 자바 스트림(Streams)이란?\u003c/h2\u003e\n\u003cp\u003e기존의 배열 또는 컬렉션 인스턴스는 for, foreach 같은 반복문을 통해 하나씩 핸들링하는 방식이었기에 로직이 복잡할수록 코드양도 많아지고 루프를 여러 번 도는 경우도 발생하였다. 그에 비해 자바 8에서 추가된 스트림(Streams)은 람다를 사용할 수 있는 기술 중 하나로 다음과 같은 특징을 가지고 있다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e스트림은 데이터의 흐름이라는 뜻으로 컬렉션에 저장되어 있는 요소들을 하나씩 순회하면서 처리할 수 있는 코드패턴이다.\u003c/li\u003e\n\u003cli\u003e함수형 프로그래밍 언어에서 일반적으로 지원하는 연산관 데이터베이스와 비슷한 연산을 지원한다.\u003c/li\u003e\n\u003cli\u003e배열/컬렉션의 함수 여러 개를 조합하여 원하는 결과를 필터링, 가공된 결과 추출 가능하며 람다식으로 코드를 간결하게 표현할 수 있다.\u003c/li\u003e\n\u003cli\u003e하나의 작업을 둘 이상 작업으로 잘게 쪼개 동시에 처리하며 스레드를 이용하여 많은 요소들을 빠르게 처리 가능한다. \u003c/li\u003e\n\u003cli\u003e컬렉션은 현재 자료구조가 포함하는 모든 값을 메모리에 저장하는 자료구조인데 비해, 스트림은 요청할 때만 요소를 계산하는 고정된 자료 구조이다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e스트림에 사용에 대한 상세한 내용은 다음 3가지로 나눌 수 있다.\u003c/p\u003e","title":"[Java] 자바 스트림(Streams)의 개념과 사용 방법"},{"content":" 싱글턴이란 인스턴스를 하나만 생성할 수 있는 클래스로, 함수와 같은 무상태 (stateless) 객체나 설계상 유일해야 하는 시스템 컴포넌트, 혹은 익숙한 스프링 빈이 대표적인 싱글턴의 예로 들 수 있다. 클래스를 싱글턴으로 만들 경우, (타입을 인터페이스로 정의한 후 인터페이스를 구현해서 만든 싱글턴이 아니라면) 싱글턴 인스턴스를 Mock으로 대체할 수 없기에 테스트가 어려울 수 있다. 싱글턴을 만드는 보편적인 두가지 방식이 있는데, 두 방식 모두생성자는 private으로 감춰두고 유일한 인스턴스에 접근할 수 있는 수단으로 public static 멤버를 하나 마련해 둔다.\n1. public static final 필드 방식의 싱글턴 public class ElvisField { public static final ElvisField INSTANCE = new ElvisField(); private ElvisField() {} public void leaveTheBuilding() { System.out.println(\u0026#34;Whoa baby, I\u0026#39;m outta here!\u0026#34;); } } Private 생성자는 public static final 필드인 elvis.instance를 초기화할 때 딱 한 번만 호출된다. public, protected 생성자가 없기에 초기화 시 생성된 인스턴스가 전체 시스템에서 유일함이 보장된다. 단, 권한이 있는 클라이언트의 경우 리플렉션 API 인 AccessibleObject.setAccessible을 사용해 private 생성자를 호출가능하다, 이 경우를 방지하기 위해서는 생성자를 수정하여 두 번째 객체가 되려 할 때 Exception을 던지면 된다. 리플렉션 API를 통해 새로운 객체가 생성되는 것을 확인해 보면\npublic static void main(String[] args) { ElvisField ElvisField1 = ElvisField.INSTANCE; ElvisField ElvisField2 = ElvisField.INSTANCE; System.out.println(ElvisField1.hashCode()); System.out.println(ElvisField2.hashCode()); try { Constructor\u0026lt;ElvisField\u0026gt; constructor = ElvisField.class.getDeclaredConstructor(); constructor.setAccessible(true); ElvisField ElvisField3 = constructor.newInstance(); System.out.println(ElvisField3.hashCode()); } catch (Exception e) { throw new RuntimeException(e); } } 821270929 821270929 1160460865 기존 ElvisField1, ElvisField2에서는 동일 인스턴스를 반환하지만, 리플렉션 API를 통해 생성하면 새로운 객체가 생성이 된다. 이 경우, 다음과 같이 인스턴스가 이미 존재하는지를 체크하여 예외처리를 하면 싱글턴에 위배되는 상황을 방지할 수 있다.\npublic class ElvisField { public static final ElvisField INSTANCE = new ElvisField(); private ElvisField() { // private 생성자는 ElvisField.INSTANCE를 초기화할 때 딱 한번만 호출된다. if (INSTANCE != null) { throw new IllegalStateException(\u0026#34;Already initialized\u0026#34;); } } public void leaveTheBuilding() { System.out.println(\u0026#34;Whoa baby, I\u0026#39;m outta here!\u0026#34;); } } 1-1. 장점 - 해당 클래스가 싱글턴임이 API에 명백히 드러남\\\npublic static 필드가 final이니 절대 다른 객체를 참조 불가 코드가 훨씬 간결 2. 정적 팩터리 방식의 싱글턴 두 번째 방법은 정적 팩터리 메서드를 public static 멤버로 제공하는 방식이다. 책의 예제를 먼저 살펴보면\npublic class EvlisStaticFactory { private static final EvlisStaticFactory INSTANCE = new EvlisStaticFactory(); private EvlisStaticFactory() {} public static EvlisStaticFactory getInstance() { return INSTANCE; } public void leaveTheBuilding() { System.out.println(\u0026#34;Whoa baby, I\u0026#39;m outta here!\u0026#34;); } } Elvis.getInstance는 항상 같은 객체를 참조하기에 애플리케이션 내에서 유일함을 보장한다. (1번 방식의 리플렉션 API의 예외는 동일하게 적용된다.)\n2-1. 장점 - API를 바꾸지 않고도 싱글턴이 아니게 변경이 가능하다는 유연성\\\n유일한 인스턴스를 반환하는 팩터리 메서드가 호출하는 스레드별로 다른 인스턴스를 반환하게 수정도 가능 정적 팩터리를 제네릭 싱글턴 팩터리로 생성 가능 정적 팩터리메서드 참조를 공급자로 사용 가능 ex) Elvis::getInstance를 Supplier \u0026lt;Elvis\u0026gt;로 사용 가능 * 이러한 장점이 필요 없다면 public 필드 방식이 더 좋다. 위의 두 방식 중 하나로 만든 싱글턴 클래스를 직렬화하려면 단순히 Serializable을 구현하는 및 선언하는 것으로는 부족하다.\n직렬화된 인스턴스를 역직렬화할 때마다 새로운 인스턴스가 생성되는 것을 방지하기 위해 readResolve() 메서드를 제공해야 한다.\nprivate Object readResolve() { return INSTANCE; } 3. 열거 타입 방식의 싱글턴 - 바람직한 방법 이펙티브 자바에서 말하는 싱글턴을 만드는 가장 바람직한 세 번째 방법은 원소가 하나인 열거 타입을 선언하는 것이다.\npublic enum ElvisEnum { INSTANCE; public void leaveTheBuilding() { System.out.println(\u0026#34;Whoa baby, I\u0026#39;m outta here!\u0026#34;); } } public 방식과 비슷하지만 훨씬 간결하다. 바로 직렬화가 가능하며 복잡한 직렬화 상황이나 리플렉션 공격에도 인스턴스가 중복되어 생성되는 경우를 완벽하게 막아준다. 대부분의 상황에서는 원소가 하나뿐인 열거 타입이 싱글턴을 만드는 가장 좋은 방법이다. (만들려는 싱글턴이 Enum 외의 클래스를 상속해야 한다면 이 방법은 사용불가) 책의 예제 소스와 상세 내용은 다음 repo에서 확인 가능\nhttps://github.com/junhkang/effective-java-summary ","permalink":"http://localhost:50666/posts/55/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/55/img.png\"\u003e\u003c/p\u003e\n\u003cp\u003e싱글턴이란 인스턴스를 하나만 생성할 수 있는 클래스로, 함수와 같은 무상태 (stateless) 객체나 설계상 유일해야 하는 시스템 컴포넌트, 혹은 익숙한 스프링 빈이 대표적인 싱글턴의 예로 들 수 있다. 클래스를 싱글턴으로 만들 경우, (타입을 인터페이스로 정의한 후 인터페이스를 구현해서 만든 싱글턴이 아니라면) 싱글턴 인스턴스를 Mock으로 대체할 수 없기에 테스트가 어려울 수 있다.\n \u003c/p\u003e\n\u003cp\u003e싱글턴을 만드는 보편적인 두가지 방식이 있는데, 두 방식 모두생성자는 private으로 감춰두고 유일한 인스턴스에 접근할 수 있는 수단으로 public static 멤버를 하나 마련해 둔다.\u003c/p\u003e","title":"[이펙티브 자바] 3. private 생성자나 열거 타입으로 싱글턴임을 보장하라"},{"content":" 정적팩토리와 생성자에는 매개변수가 많을 경우 대응이 힘들다는 동일한 제약이 있다. 특히 매개변수가 굉장히 많은데 대부분이 고정값인 경우 일반적인 방법으로는 깔끔한 대응이 힘들다. 유연한 대응을 위해 보통 점층적 생성자패턴, 자바 빈즈 패턴, 빌더 패턴을 사용하는데, 이번 챕터에서는 빌더 패턴의 장점을 강조하고 있다. 각 패턴의 장단점을 자세히 살펴보고 빌더 패턴의 장점을 알아보자\n1. 점층적 생성자 패턴 필수 매개변수만 받는 생성자\n필수 매개변수와 선택 매개변수 1개를 받는 생성자, 필수 매개변수와 선택 매개변수 2개를 받는 생성자, 필수 매개변수와 선택 매개변수 3개를 받는 생성자, ...\n형태로 매개변수를 전부 받는 생성자까지 늘려가는 방식이다. 인스턴스 생성 시에는 매개변수를 모두 포함한 생성자 중 가장 짧은 것을 골라 호출하면 된다. 점층적 생성자 패턴도 충분히 유용하게 사용이 가능하지만, 매개변수가 많아지면 클라이언트 코드를 읽거나 작성하기 힘들고, 설정하길 원하지 않는 값도 인자로 넣어줘야 하는 경우가 있다. 또한 매개변수의 수가 늘어날수록 의미 헷갈리고, 갯수, 위치를 매번 세어야 하기에 버그로 이어지기 쉽다. 인자의 순서를 바꿔도 컴파일 단계에서는 알 수 없고 런타임단계에서 문제를 일으킬 것이다. 다음은 책의 예제를 그대로 구현해 본 점층적 생성자 패턴이다. /* 2-1 점층적 생성자 패턴 - 확장하기 어렵다. */ public class NutritionFactsTcp { private final int servingSize; // (mL) required private final int servings; // (per container) required private final int calories; // (per serving) optional private final int fat; // (g/serving) optional private final int sodium; // (mg/serving) optional private final int carbohydrate; // (g/serving) optional public NutritionFactsTcp(int servingSize, int servings) { this(servingSize, servings, 0); } public NutritionFactsTcp(int servingSize, int servings,int calories) { this(servingSize, servings, calories, 0); } public NutritionFactsTcp(int servingSize, int servings,int calories, int fat) { this(servingSize, servings, calories, fat, 0); } public NutritionFactsTcp(int servingSize, int servings,int calories, int fat, int sodium) { this(servingSize, servings, calories, fat, sodium, 0); } public NutritionFactsTcp(int servingSize, int servings,int calories, int fat, int sodium, int carbohydrate) { this.servingSize = servingSize; this.servings = servings; this.calories = calories; this.fat = fat; this.sodium = sodium; this.carbohydrate = carbohydrate; } } 매개변수가 추가될 수록 유연하게 대응하는 것이 힘들고, fat / sodium 등의 매개변수 위치를 실수로 잘못 넣더라도 컴파일 단계에서 확인이 불가능하다.\n2. 자바빈즈 패턴 매개변수가 없는 생성자로 객체 생성 후 setter 메서드로 값을 부여하는 방식이다. 다음은 책의 예제를 그대로 구현해 본 자바빈즈 패턴이다.\n/* 2-2 자바빈즈 패턴 - 일관성이 깨지고 불변으로 만들수 없음 */ public class NutritionFactsJb { private int servingSize = -1; private int servings = -1; private int calories = 0; private int fat = 0; private int sodium = 0; private int carbohydrate = 0; public NutritionFactsJb() { } // Setters public void setServingSize(int val) { servingSize = val; } public void setServings(int val) { servings = val; } public void setCalories(int val) { calories = val; } public void setFat(int val) { fat = val; } public void setSodium(int val) {sodium = val;} public void setCarbohydrate(int val) {sodium = val;} } 점층적 생성자 패턴의 단점들이 보이지 않게 되었다.코드가 길지만 인스턴스를 만들기 쉽고, 읽고 쓰기 간결한 코드가 되었다\n하지만 해당 인스턴스를 호출하는 부분을 생각해 보자 NutritionFactsJb pepsiCola = new NutritionFactsJb(); pepsiCola.setServingSize(240); pepsiCola.setServings(8); pepsiCola.setCalories(100); pepsiCola.setSodium(35); pepsiCola.setCarbohydrate(27); 완전한 객체 하나를 만들려면 메서드를 여러 개 호출해야 하고, 객체가 완전히 생성되기 전까지는 일관성이 무너진 상태가 된다. 점층적 생성자 패턴에서는 매개변수들이 유효한지 생성자에서만 확인하면 일관성이 유지가 되었지만, 자바빈즈 패턴에서는 불가능하다. 클래스의 속성을 변경하는 메서드를 제공하는 것이기에 불변으로 만들 수 없으며, 일관성이 깨진 객체를 만들면 버그가 생성된 시점과 실제 문제를 일으키는 코드가 물리적으로 떨어져 있을 것이라 디버깅 힘들다. 이렇게 일관성이 무너지는 문제로 자바빈즈 패턴에서는 클래스를 불변으로 만들수 없고, 스레드 안정성을 얻으려면 개발자가 추가작업이 필요하다. 생성이 끝난 객체를 수동으로 얼리고 얼리기 전에는 사용할 수 없도록 하기도 하지만, 다루기 어려워 실전에선 거의 쓰이지 않는다고 한다. 쓰더라도 객체 사용 전에 freeze 메서드를 확실히 호출했는지 컴파일러가 보증할 수 없어 런타임 오류에는 여전히 취약하다.\n3. 빌더패턴 클라이언트는 필요객체를 직접 만드는 대신 필수 매개변수만으로 생서자를 호출하여 빌더 객체를 얻을 수 있다. 그다음 빌더객체가 제공하는 setter메서드들로 원하는 매개변수를 설정하고, 마지막으로 매개변수가 없는 빌드 메서드를 호출하여 필요한 객체를 얻는다.\n(builder는 생성할 클래스 안에 정적 멤버 클래스로 만들어두는 것이 관례이다.)\n다음은 다음은 책의 예제를 그대로 구현해 본 빌더 패턴이다.\n/* 2-3 빌더패턴 - 점층적 생성자 패턴과 자바빈즈 패턴의 장점만 취함 */ public class NutritionFactsBp { private final int servingSize ; private final int servings ; private final int calories; private final int fat; private final int sodium; private final int carbohydrate; public static class Builder { private final int servingSize ; private final int servings ; private int calories = 0; private int fat = 0; private int sodium = 0; private int carbohydrate = 0; public Builder(int servingSize, int servings) { this.servingSize = servingSize; this.servings = servings; } public Builder calories(int val) { calories = val; return this; } public Builder fat(int val) { fat = val; return this; } public Builder sodium(int val) { sodium = val; return this; } public Builder carbohydrate(int val) { carbohydrate = val; return this; } public NutritionFactsBp build() { return new NutritionFactsBp(this); } } private NutritionFactsBp(Builder builder) { servingSize = builder.servingSize; servings = builder.servings; calories = builder.calories; fat = builder.fat; sodium = builder.sodium; carbohydrate = builder.carbohydrate; } } NutirionFactsBp 클래스는 불변이며 모든 매개변수의 기본 값을 한 곳에 모아둔다. 빌더의 세터 메서드들은 빌더 자신을 반환하기에 연쇄적으로 호출가능하다. 이런 방식을 흐르듯 연결되듯 메서드 호출이 된다는 뜻에서 플루언트 API (fluent API) 혹은 메서드 연쇄 (method chaining)이라 한다. 실제로 이 클래스를 사용하는 클라이언트 코드를 보면\nNutritionFactsBp dietCola = new NutritionFactsBp.Builder(240, 8).calories(100).sodium(35).carbohydrate(27).build(); 한눈에 봐도 읽기 쉽고 쓰기 쉽다. 빌더 패턴은 (파이썬과 스칼라에 있는) 명명된 선택적 매개변수(named optional parameters)를 흉내 낸 것이다. 잘못된 매개변수를 일찍 발견하려면 빌더의 생성자와 메서드에서 입력 매개변수를 검사하고, 빌드 메서드가 호출하는 생성자에서 여러 매개변수에 걸친 불변식을 검사하면 된다. 공격에 대비해 이런 불변식을 보장하려면 빌더로부터 매개변수를 복사한 후 해당 객체 필드들도 검사해야 한다, 검사하다가 유효하지 않은 값을 발견하면 자세한 내용의 메시지를 담아 IllegalArgumentException을 던지도록 하자.\n4. 계층적으로 설계된 클래스와 빌더 패턴 빌더패턴은 계층적으로 설계된 클래스와 함께 쓸 때 더욱 유용하다. 각 계층의 클래스에 빌더를 멤버로 정의하여 추상 클래스에는 추상빌더를, 구체클래스는 구체 빌더를 가지게한다. 다음은 다음은 책의 예제를 그대로 구현해 본 계층적 설계 클래스와 빌더 패턴의 조합이다.\n/* 2-4 계층적으로 설계된 클래스와 잘 어울리는 빌더 패턴 */ public abstract class Pizza { public enum Topping {HAM, MUSHROOM, ONION, PEPPER, SAUSAGE} final Set\u0026lt;Topping\u0026gt; toppings; abstract static class Builder\u0026lt;T extends Builder\u0026lt;T\u0026gt;\u0026gt; { EnumSet\u0026lt;Topping\u0026gt; toppings = EnumSet.noneOf(Topping.class); public T addTopping (Topping topping) { toppings.add(Objects.requireNonNull(topping)); return self(); } abstract Pizza build(); protected abstract T self(); } Pizza (Builder\u0026lt;?\u0026gt; builder) { toppings = builder.toppings.clone(); } } Pizza.Builder 클레스는 재귀적 타입 한정을 이용하는 제네릭 타입이다. 추상 메서드인 self를 더해 하위클래스에서는 형변환 없이 메서드 연쇄를 지원할 수 있게 했으며, 이처럼 Self 타입이 없는 자바를 위한 우회 방법을 \u0026quot;시뮬레이트한 셀프타입\u0026quot;(simulated self-type) 관용구라고 한다. 다음은 Pizza 하위의 서로 다른 매개변수를 필수로 받는 2개의 클래스 예제이다.\n/* 2-5 뉴욕피자 */ public class NyPizza extends Pizza { public enum Size {SMALL, MEDIUM, LARGE} private final Size size; public static class Builder extends Pizza.Builder\u0026lt;Builder\u0026gt; { private final Size size; public Builder(Size size) { this.size = Objects.requireNonNull(size); } @Override public NyPizza build() { return new NyPizza(this); } @Override protected Builder self() { return this; } } private NyPizza(Builder builder) { super(builder); size = builder.size; } } /* 2-5 칼조네 */ public class Calzone extends Pizza { private final boolean sauceInside; public static class Builder extends Pizza.Builder\u0026lt;Builder\u0026gt; { private boolean sauceInside = false; // 기본값 public Builder sauceInside() { sauceInside = true; return this; } @Override public Calzone build() { // 구현체 하위 클래스를 반환하도록 선언 return new Calzone(this); } @Override protected Builder self() { return this; } } private Calzone(Builder builder) { super(builder); sauceInside = builder.sauceInside; } } 각 하위 클래스의 빌더의 build 메서드는 해당 구현체 하위 클래스를 반환하도록 선언한다. 예제처럼 하위 클래스의 메서드가 상위 클래스의 메서드가 정의한 반환 타입이 아닌, 그 하위 타입을 반환하는 기능을 공변 반환 타이핑(covariant return typing)이라고이를 통해 클라이언트가 형변환에 신경 쓰지 않고 빌더 사용이 가능하다. 실제 클래스가 사용되는 방식을 확인해 보자.\nNyPizza nyPizza = new NyPizza.Builder(SMALL).addTopping(SAUSAGE).addTopping(ONION).build(); Calzone calzone = new Calzone.Builder().addTopping(ONION).sauceInside().build(); 빌더를 사용하면 가변인수 매개변수를 여러 개 사용 가능하며 각각 적절한 메서드로 나눠 선언하면 된다. 메서드를 여러 번 호출하도록 하고 각 호출 때 넘겨진 변수들을 하나의 필드로 모을 수도 있다. 빌더타입은 유연하다. 빌더하나로 여러 객체를 순회하면서 만들 수 있고 빌더에 넘기는 매개변수에 따라 다른 객체를 만들수도 있다. 객체마다 부여되는 일련번호 같은 특정 필드는 빌더가 알아서 채울 수도 있다.\n4. 빌더 패턴의 단점? 그렇다면 이러한 빌더 패턴엔 어떤 단점이 있을까? 먼저 객체를 만들기 위해 빌더부터 만들어야 한다는 단점이 있다. 빌더 생성하는 비용이 그리 크진 않겠지만 성능이 매우 민감한 경우 문제 될 수 있다.\n그리고 매개변수의 개수가 몇 개 안될 때는 점층적 생성자 패턴보다 코드가 장황하다. 예제에서만 보아도 다른 패턴에 비해 좀더 복잡한 구조를 가지고 있다. 위의 예제들은 매개변수가 몇개 안 되는 경우이고, 빌더패턴은 4개 이상의 매개변수에서 진정한 값 어치를 한다.\n또한 API는 최초 생성 시의 규격보다 시간이 지날수록 매개변수가 점차 많아지는 경향이 있기에 당장은 매개변수가 확장성과 함께 빌더 패턴을 고려해 볼 필요가 있다. 물론 생성자/정적 팩토리 방식으로 시작했다가 나중에 매개변수가 많아지면 빌더패턴으로 전환도 가능하지만 애초에 빌더로 시작하는 편이 나을 때가 많다.\n5. 결론 생성자/정적팩토리가 처리해야 할 매개변수가 많거나, 매개변수 중 다수가 필수값이 아니거나 같은 타입이라면 특히 빌더 패턴을 선택하자.\n빌더는 점층적 생성자보다 클라이언트 코드를 읽고 쓰기 간결하게 해 주고,자바빈즈보다 훨씬 안전하다는 장점이 있다. 책의 예제 소스와 상세 내용은 다음 repo에서 확인 가능하다.\nhttps://github.com/junhkang/effective-java-summary\n","permalink":"http://localhost:50666/posts/54/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/54/img.png\"\u003e\u003c/p\u003e\n\u003cp\u003e정적팩토리와 생성자에는 매개변수가 많을 경우 대응이 힘들다는 동일한 제약이 있다. 특히 매개변수가 굉장히 많은데 대부분이 고정값인 경우 일반적인 방법으로는 깔끔한 대응이 힘들다. 유연한 대응을 위해 보통 점층적 생성자패턴, 자바 빈즈 패턴, 빌더 패턴을 사용하는데, 이번 챕터에서는 빌더 패턴의 장점을 강조하고 있다. 각 패턴의 장단점을 자세히 살펴보고 빌더 패턴의 장점을 알아보자\u003c/p\u003e\n\u003ch2 id=\"점층적-생성자-패턴\" ke-size=\"size26\"\u003e1. 점층적 생성자 패턴\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e필수 매개변수만 받는 생성자\u003cbr\u003e\n필수 매개변수와 선택 매개변수 1개를 받는 생성자,\n필수 매개변수와 선택 매개변수 2개를 받는 생성자,\n필수 매개변수와 선택 매개변수 3개를 받는 생성자,\n...\u003c/p\u003e","title":"[이펙티브 자바] 2. 생성자에 매개변수가 많다면 빌더를 고려하라"},{"content":" 생성자 대신 정적 팩토리 메서드를 고려하라 클라이언트가 클래스 인스턴스를 얻는 방법에는 전통적인 방법 중 하나는 public이다. 하지만 정적 팩터리 메서드(static factory method)도 꼭 알아두어야한다.\n1. 정적 팩터리 메서드란? 그렇다면 정적 팩터리 메서드는 무엇일까? 간단히 말해 객체 생성의 역할을 하는 클래스 메서드로, static 메서드를 통해 인스턴스를 생성하는 것이다. 다음은 java의 기본 Boolean 클래스 내 정적 팩토리 메서드의 간단한 예시이다. 이팩티브 자바에서는 정적 팩토리 메서드를 사용할 시의 5가지 장점과 2가지 단점에 대해 서술하고 있어 자세한 비교를 통해 하나하나 알아보려 한다.\n2. 정적 팩토리 메서드 (static factory method)의 장점 2-1. 이름을 가질 수 있다. 인스턴스를 대표하는 생성자가 명확하거나, 반환될 객체에 대한 설명이 필요하지 않을 경우에는 크게 느껴지지 않는 차이일 수 있지만, 이름을 가질 수 있어 반환될 객체의 특징을 설명할 수 있다는 것은 굉장한 장점이다. 이펙티브 자바에서는 BigInteger와 BigInteger.probablePrime의 차이를 예로 들고 있다. 먼저 BigInteger(int, int, Random)의 예제를 보면\n설명을 읽어보면 \u0026ldquo;지정된 bitLength의 소수일 가능성이 있는 임의의 BigInteger를 생성한다.\u0026ldquo;는 것을 이해할 수 있지만, 그전에는 명확한 반환될 객체의 특성을 알 수 없다. 또한, 하나의 시그니처로는 생성자를 한 개만 만들 수 있기에 제약이 있다. 예를 들어 동일한 BigInteger(int, int, Random) 생성자는 다른 의미를 가질 수 없다. BigInteger(int, Random, int)와 같이 순서를 바꾸거나, 추가하는 식으로 피해 갈 수는 있지만, 당연히 좋지 않은 방식이다. (추가될 때마다 클래스 설명 문서를 확인해야 하고, 호출하는데 실수가 있을 수 있다.)\n그에 비해 자바 4에서 추가된 BigInteger.probablePrime을 보면 java.math.BigInteger.probablePrime\n\u0026quot;값이 소수인 BigInteger를 반환한다\u0026quot;라는 의미를 이름만으로도 충분히 유추가 가능하다. 따라서 한 클래스에 시그니처 생성자가 여러 개 필요하다면 생성자를 정적 팩토리 메서드로 바꾸고 그 특징을 설명할 수 있는 이름을 붙이자\n2-2. 호출될 때마다 인스턴스를 새로 생성하지 않아도 된다. 개인적으로는 큰 시스템일수록 가장 큰 장점이 되지 않을까 싶은데, 불변 클래스는 인스턴스를 미리 만들어 놓거나 새로 생성된 인스턴스를 캐싱하여 활용하기에 불필요한 객체 생성 방지한다. 위의 예제에서 본 Boolean.valueOf(boolean)을 보면 객체를 아얘 생성하지 않는다. 그래서 (특히 생성 비용이 큰) 같은 객체가 자주 요청되는 상황이라면 성능을 상당히 올려준다. 또한 이는 인스턴스의 생명 주기를 철저히 컨트롤 가능하는 뜻이며, 클래스를 싱글턴 혹은 인스턴스화 불가 상태로 만들 수도 있다.\n2-3. 반환 타입의 하위 타입 객체를 반환할 수 있다. 반환할 객체의 클래스를 자유롭게 선택할 수 있는 유연성을 제공한다. 응용하면 API를 만들 때 구현 클래스를 공개하지 않고 객체를 반환할 수 있어 API를 작게 유지가 가능하다. 자바 8 전에는 인터페이스에 정적 메서드 선언 불가하였고, 이름이 \u0026ldquo;Type\u0026quot;인 인터페이스를 반환하는 정적 메서드가 필요하면 \u0026ldquo;Types\u0026quot;라는 (인스턴스화 불가인) 동반 클래스를 만들어 그 안에 정의하는 것이 관례였다. 예를 들어 자바 컬렉션 프레임워크는 핵심 인터페이스들에 수정 불가나 동기화 등의 기능을 붙인 총 45개의 유틸리티 구현체를 제공하고, 이 구현체 대부분을 단 하나의 인스턴스화 불가 클래스인 java.util.Collections에서 정적 팩토리 메서드를 통해 얻도록 한다.\n다음은 java.util.Collections의 동기화 기능의 정적 팩터리 메서드이다.\n컬렉션 프레임워크 자체는 이 45개 클래스를 공개하지 않기 때문에 API 외견을 훨씬 작게 만들 수 있었다. API가 작아진 것은 물론 개념적인 무게, 프로그래머가 API를 사용하기 위해 익혀야 하는 개념의 수와 난이도도 낮아졌다. (프로그래머는 명시한 인터페이스 대로 동작하는 객체를 얻을 것임을 알기에 굳이 문서를 찾거나 실제 구현클래스가 무엇인지 알아보지 않아도 된다. 나아가 정적 팩토리 메서드를 사용하는 클라이언트는 얻은 객체를 인터페이스만으로 다루게 된다.) 추가로, 자바 8부터는 인터페이스가 정적 메서드를 가질 수 없다는 제한이 풀렸기에 인스턴스화 불가 동반 클래스를 둘 이유가 별로 없다. 동반 클래스에 두었던 public 정적 멤버들 상당수를 그냥 인터페이스 자체에 두면 된다. (자바 9에서는 private 정적 메서드까지 허용하지만 정적 필드와 정적 멤버 클래스는 여전히 public이어야만 함)\n2-4. 입력 매개 변수에 따라 매번 다른 클래스의 객체를 반환할 수 있다. 반환 타입의 하위타입이기만 하면 어떤 클래스의 객체를 반환하던 상관없다. 심지어 다음 릴리즈에서는 또 다른 클래스의 객체를 반환해도 된다. 즉 하위 타입이기만 하면 API 변경 시 또 다른 클래스의 객체를 반환해도 된다. 예를들어 EnumSet 클래스는 public 생성자 없이 정적 팩토리만 제공하는데 openjdk에서는 원소의 수에 따라 두 가지 하위 클래스중 하나의 인스턴스를 반환한다.\njava.util.EnumSet\n반환 값을 보면 원소가 64개 이하면 long변수 하나로 원소를 관리하는 RegularEnumSet을, 65개 이상이면 long 배열로 관리하는 JumboEnumSet을 반환한다. 만약 원소가 적을 때 RegularEnumSet을 사용할 이점이 없어진다면 다음 릴리즈에는 이를 삭제해도 클라이언트는 아무런 변화도 알 수 없을 것이다. 클라이언트는 팩토리가 건네주는 객체가 어느 클래스의 인스턴스인지 알 수 없고 알 필요도 없다. EnumSet의 하위 클래스이기만 하면 된다.\n2-5. 정적 팩토리 메서드를 작성하는 시점에는 반환할 객체의 클래스가 존재하지 않아도 된다. 메서드를 작성하는 시점에 반환할 객체의 클래스가 존재하지 않는다는 것은 서비스 제공자 프레임워크 (service provider framework)의 근간이다. 이 뜻을 자세히 살펴보면\n서비스 제공자 프레임워크의 제공자(provider)는 서비스의 구현체이고, 이 구현체들을 클라이언트에 제공하는 역할을 프레임워크가 통제하여 클라이언트를 구현체로부터 분리해준다. 서비스 제공자 프레임워크의 핵심 프레임워크 3가지는\n서비스 인터페이스 (service interface) - 구현체의 동작을 정의한다.\n제공자 등록 API (provider registration API) - 제공자가 구현체를 등록할 때 사용 서비스 접근 API (service access API) - 클라이언트가 서비스의 인스턴스를 얻을 때 사용 (클라이언트는 서비스 접근 API를 사용할 때 원하는 구현체의 조건 명시 가능, 조건을 명시하지 않으면 기본 구현체를 반환하거나 지원하는 구현체들을 하나씩 돌아가며 반환) 이 중 서비스 접근 API가 바로 앞서 말한 서비스 제공자 프레임워크의 근간이라고 한 유연한 정적팩토리의 실체이다. (추가로 서비스 인터페이스의 인스턴스를 생성하는 팩터리 객체를 설명하는 서비스 제공자 인터페이스(Service Provider Interface)가 쓰이기도 한다.) 익숙한 프레임워크이자 대표적인 서비스 제공자 프레임워크인 JDBC(java database connectivity)를 살펴보면 이해가 쉽다.\nConnection - 서비스 인터페이스 역할\nDriverManager.registerDriver - 제공자 등록 API 역할 DriverManager.getConnection - 서비스 접근 API 역할 Driver - 서비스 제공자 인터페이스 역할 (자바 6부터는 java.util.ServiceLoader라는 범용 서비스 제공자 프레임워크가 제공되어 프레임워크를 직접 만들 필요가 거의 없지만, JDBC는 6전에 등장하였기에 ServiceLoader를 사용하지 않는다.)\n3. 정적 팩토리 메서드 (static factory method)의 단점 3-1. 상속을 하려면 public, protected 생성자가 필요하니 정적 팩토리 메서드만 제공하면 하위 클래스 생성이 불가하다. 컬렉션 프레임워크의 유틸리티 구현 클래스들은 상속이 불가하다는 말이다. 상속보다 컴포지션을 사용하도록 유도하고 불변타입으로 만들려면 이 제약을 지켜야 한다는 점에서 오히려 장점일 수도 있다.\n3-2. 정적 팩토리 메서드는 프로그래머가 찾기 힘들다. 생성자처럼 API 설명에 명확히 드러나지 않으니 사용자는 정적 팩터리 메서드 방식 클래스를 인스턴스화할 방법을 알아내야 한다. API 문서를 규격화하고, 메서드 명도 널리 알려진 규약에 따라 짓는 것으로 어느 정도 해결해야 한다.\n3-3. 정적 팩토리 메서드에서 흔히 사용하는 네이밍 From : 매개변수를 하나 받아서 해당 타입 인스턴스를 반환하는 형변환 메서드 ex) Date d = Date.from(instant); Of : 여러 매개변수를 받아 적합한 타입의 인스턴스를 반환하는 집계 메서드 Set\u0026lt;Rank\u0026gt; faceCards =EnumSet.of(JACK,QUEEN, KING); valueOf : from과 of의 더 자세한 버전 BigInteger prime = BigInteger.valueOf(Integer.MAX_VALUE); Instance / getInstance : (매개변수를 받는다면) 매개변수로 명시한 인스턴스를 반환하지만, 같은 인스턴스임을 보장하지는 않음 StackWalker Luke = StackWalker.getInstance(options); Create/newInstance : instance/getInstance와 같지만, 매번 새로운 인스턴스를 생성해 반환함을 보장 Object newArray = Array.newInstance(classObject, arrayLen); getType : getInstance와 같으나, 생성할 클래스가 아닌 다른 클래스에 팩토리 매서드를 정의할 때 사용. \u0026ldquo;Type\u0026quot;은 팩터리 메서드가 반환할 객체의 타입 FileStore fs = Files.getFileStore(path) newType : newInstance와 같으나, 생성할 클래스가 아닌 다른 클래스에 팩터리 메서드를 정의할 때 사용. \u0026ldquo;Type\u0026quot;은 팩터리 메서드가 반환할 객체의 타입 BufferReader br = Files.newFufferedReader(path) type : getType과 newType의 간결한 버전 List\u0026lt;Complaint\u0026gt; litany = Collections.list(legacyLitany); 4. 정리 적정 팩터리 매서드와 public 생성자는 각각 쓰임새가 있으니 장담점을 이해하고 써야 한다. 정적 팩토리를 사용할 경우가 유리한 경우가 더 많기에 무작정 public 생성자를 썼다면 다시 한번 생각해 보자 책의 예제 소스와 상세 내용은 다음 repo에서 확인 가능하다.\nhttps://github.com/junhkang/effective-java-summary\n","permalink":"http://localhost:50666/posts/53/","summary":"\u003chr\u003e\n\u003ch2 id=\"생성자-대신-정적-팩토리-메서드를-고려하라\" ke-size=\"size26\"\u003e\u003cstrong\u003e생성자 대신 정적 팩토리 메서드를 고려하라\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e클라이언트가 클래스 인스턴스를 얻는 방법에는 전통적인 방법 중 하나는  public이다. 하지만 \u003cstrong\u003e정적 팩터리 메서드\u003c/strong\u003e(\u003cstrong\u003estatic factory method\u003c/strong\u003e)도 꼭 알아두어야한다.\u003c/p\u003e\n\u003ch2 id=\"정적-팩터리-메서드란\" ke-size=\"size26\"\u003e1. 정적 팩터리 메서드란?\u003c/h2\u003e\n\u003cp\u003e그렇다면 정적 팩터리 메서드는 무엇일까? 간단히 말해 객체 생성의 역할을 하는 클래스 메서드로, static 메서드를 통해 인스턴스를 생성하는 것이다. 다음은 java의 기본 Boolean 클래스 내 정적 팩토리 메서드의 간단한 예시이다. \u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/53/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-17%20%EC%98%A4%ED%9B%84%205.02.29.png\"\u003e\u003c/p\u003e\n\u003cp\u003e이팩티브 자바에서는 정적 팩토리 메서드를 사용할 시의 5가지 장점과 2가지 단점에 대해 서술하고 있어 자세한 비교를 통해 하나하나 알아보려 한다.\u003c/p\u003e","title":"[이펙티브 자바] 1. 생성자 대신 정적 팩터리 메서드를 고려하라"},{"content":" 1. 문제 상황 Java21 버전 업 중, 인텔리제이 내장 톰캣으로는 정상적으로 실행되지만, 커스텀 옵션을 사용 중인 외장톰캣을 그대로 사용 시 정상적으로 구동되지 않는 현상이 발견되었다.\n2. 해결 먼저, 빌드 및 path 등 기존 설정들은 문제가 없는 것을 확인하였고, 로컬 톰캣에서 정상적으로 구동됨을 확인하였기에 다른 부분을 추가로 확인해 보았다. 버전업 영향도 파악기간 중 기존에 톰캣 8.5 버전과 호환되는 것을 확인하였으나, 공식 홈페이지에서 재확인해보았다.\n톰캣 8.5 버전의 자바 지원버전을 보면 \u0026quot;7 and later\u0026quot;로 되어있어 7 이후 버전을 다 사용 가능한 것으로 이해했었으나, 다른 버전을 시도.\n그중 jdk21 버전과 정확히 일치하는 최신 톰캣 11.0.x 버전을 시도하였고, 톰캣을 11 버전대로 업그레이드 후 실행 시 정상 실행됨을 확인.\nIt is not unusual for the initial early access builds to contain bugs that can cause problems for web applications running on Tomcat. If the new Java version introduces new language features then the default JSP compiler may not support them immediately. Switching the JSP compiler to javac may enable these new language features to be used in JSPs. If you do discover an problem using a Java early access build, please https://tomcat.apache.org/findhelp.htmlask for help. The Tomcat user's mailing list is probably the best place to start. 톰캣 11 버전이 필수인 것인가를 확인해 보던 중, 신규 자바 버전에 새로운 기능이 추가되면 버전 호환성 표와는 같아도 일부 기능이 호환되지 않는 것은 정상적인 현상이라는 내용 확인. 또한 이번 Java 변경 사항 중 Jakarta EE로의 전환이 가장 큰 이슈 중 하나이기에, 해당 내용을 적용할 수 있는 최저 버전을 찾아보기로 하였다.\nAlpha releases may contain large amounts of untested/missing functionality required by the specification and/or significant bugs and are not expected to run stably for any length of time. 또한 톰캣의 알파버전은 공지된 대로 중대한 버그들이 많이 발견될 수 있는 상태이기에 현재 11 버전을 라이브로 도입하는 것은 위험하다.\nApache Tomcat 11.0.x is the current focus of development. It builds on Tomcat 10.1.x and implements the Servlet 6.1, JSP 4.0, EL 6.0, WebSocket 2.2 and Authentication 3.1 specifications (the versions required by Jakarta EE 11 platform).\n(톰캣 11 버전은 개발을 한창 진행 중인 듯하다. 2023/12/18 기준) 결국 jdk21과 호환 가능한 최신 안정화버전을 찾아보기로 하였다. jdk21에 맞춘 jakartaEE 플랫폼을 사용한 최저 버전을 확인해 보면\n9점대 버전이고, 가장 최신의 stable(안정화된) 버전은 10.1.x버전이기에 10.1.x버전으로 시도를 하였고 정상 구동됨을 확인하였다. 참고 https://tomcat.apache.org/whichversion.htmlhttps://tomcat.apache.org/whichversion.html\n","permalink":"http://localhost:50666/posts/52/","summary":"\u003chr\u003e\n\u003ch2 id=\"문제-상황\" ke-size=\"size26\"\u003e1. 문제 상황\u003c/h2\u003e\n\u003cp\u003eJava21 버전 업 중, 인텔리제이 내장 톰캣으로는 정상적으로 실행되지만, 커스텀 옵션을 사용 중인 외장톰캣을 그대로 사용 시 정상적으로 구동되지 않는 현상이 발견되었다.\u003c/p\u003e\n\u003ch2 id=\"해결\" ke-size=\"size26\"\u003e2. 해결\u003c/h2\u003e\n\u003cp\u003e먼저, 빌드 및 path 등 기존 설정들은 문제가 없는 것을 확인하였고, 로컬 톰캣에서 정상적으로 구동됨을 확인하였기에 다른 부분을 추가로 확인해 보았다. 버전업 영향도 파악기간 중 기존에 톰캣 8.5 버전과 호환되는 것을 확인하였으나, 공식 홈페이지에서 재확인해보았다.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/52/img.png\"\u003e\u003c/p\u003e\n\u003cp\u003e톰캣 8.5 버전의 자바 지원버전을 보면 \u0026quot;7 and later\u0026quot;로 되어있어 7 이후 버전을 다 사용 가능한 것으로 이해했었으나, 다른 버전을 시도.\u003c/p\u003e","title":"[Spring] Java 21 외장 톰캣 버전 설정"},{"content":" 1. 현상 스프링부트 3.*.* 버전 업을 하며 Spring Security6으로 업데이트 중이다.\n더 이상 지원하지 않는 WebSecurityConfigurerAdapter를 SecurityFilterChain으로 변경 시 포워딩되는 jsp 파일 경로가 필터에 걸려 노출되지 않는 현상이 발생하였다.\n1-1. 기존 샘플 소스 @Configuration public class SecurityConfig { @Bean public SecurityFilterChain config(HttpSecurity http) throws Exception { http.authorizeHttpRequests((auth) -\u0026gt; auth .requestMatchers(\u0026#34;/\u0026#34;).permitAll() .anyRequest().authenticated() ); } 기존과 같이 \u0026ldquo;/\u0026rdquo; 경로에 대한 권한을 부여하였지만, jsp 경로에 대한 권한부족으로 페이지 접근에 실패하였다.\n2. 원인 Spring Security 5.8 and earlier only\nperform authorization once per request. This means that dispatcher types like FORWARD and INCLUDE that run after REQUEST are not secured by default. Spring Security6 에서 페이지 전환 시 forwards, includes의 타입의 요청이 security filter에 기본적으로 포함되게 변경되었다.\n3. 해결 3-1. forwards/includes 타입 요청을 허용하도록 수정 (스프링 공식문서 참고) @Configuration public class SecurityConfig { @Bean public SecurityFilterChain config(HttpSecurity http) throws Exception { http.authorizeHttpRequests((auth) -\u0026gt; auth .requestMatchers(\u0026#34;/\u0026#34;).permitAll() .dispatcherTypeMatchers(DispatcherType.FORWARD).permitAll() .dispatcherTypeMatchers(DispatcherType.INCLUDE).permitAll() .anyRequest().authenticated() ); } 3-2. jsp 파일 경로 (ex. /WEB-INF/view/*)를 허용하도록 수정 @Bean public SecurityFilterChain config(HttpSecurity http) throws Exception { http .authorizeHttpRequests((auth) -\u0026gt; auth .requestMatchers(\u0026#34;/\u0026#34;, \u0026#34;/WEB-INF/view/**\u0026#34;).permitAll() .anyRequest().authenticated() ); return http.build(); } 참고 https://docs.spring.io/spring-security/reference/5.8/migration/servlet/authorization.html#_permit_forward_when_using_spring_mvc\n","permalink":"http://localhost:50666/posts/51/","summary":"\u003chr\u003e\n\u003ch2 id=\"현상\" ke-size=\"size26\"\u003e1. 현상\u003c/h2\u003e\n\u003cp\u003e스프링부트 3.*.* 버전 업을 하며 Spring Security6으로 업데이트 중이다.\u003c/p\u003e\n\u003cp\u003e더 이상 지원하지 않는 WebSecurityConfigurerAdapter를 SecurityFilterChain으로 변경 시 포워딩되는 jsp 파일 경로가 필터에 걸려 노출되지 않는 현상이 발생하였다.\u003c/p\u003e\n\u003ch3 id=\"기존-샘플-소스\" ke-size=\"size23\"\u003e1-1. 기존 샘플 소스\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e@Configuration\npublic class SecurityConfig  {\n    @Bean\n    public SecurityFilterChain config(HttpSecurity http) throws Exception {\n       http.authorizeHttpRequests((auth) -\u0026gt; auth\n          .requestMatchers(\u0026#34;/\u0026#34;).permitAll()\n          .anyRequest().authenticated()\n      );\n\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e \u003c/p\u003e\n\u003cp\u003e기존과 같이 \u0026ldquo;/\u0026rdquo; 경로에 대한 권한을 부여하였지만, jsp 경로에 대한 권한부족으로 페이지 접근에 실패하였다.\u003c/p\u003e\n\u003ch2 id=\"원인\" ke-size=\"size26\"\u003e2. 원인 \u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSpring Security 5.8 and earlier only\u003c/p\u003e","title":"[Spring] Spring Security6 filterchain 사용시 jsp 뷰 렌더링 설정"},{"content":" 1. Elastic Search란? Apache Lucene에 구축되어 배포된 검색 및 분석 엔진이다. 현재 검색엔진을 넘어 보안, 로그분석, Full-text 분석 등 다양한 영역에서 사용되며, Kibana, Logstash, Beats들과 함께 사용된다. 오픈 소스 프로젝트로 활발히 개발되고 있으며, 유닉스, 자바의 기초지식 필요하다. Apache Lucene의 한계를 보완하기 위한 새로운 검색엔진 프로젝트로 시작되었고 Logstash, Kibana와 함께 사용되어 ELK Stack (ES, Logstash, Kibana)라고 불렸으나 2013년 Logstash, Kibana 프로젝트 정식 흡수되었다.\n2. Elastic Stack이란? ES, Logstash, Kibana를 묶은 ELK 서비스이다. 5.0.0 버전부터 Beats를 포함하며 Elastic Stack 이란 이름으로 서비스가 제공되고 있다. 서버로부터 모든 유형의 데이터를 가져와 실시간 검색, 분석, 시각화를 도와주는 Elastic 오픈 소스 서비스 제품이다.\n2-1. Elastic Search 아파치 루씬(Apache Lucene) 기반의 Full Text로 검색이 가능한 오픈 소스 분석 엔진 주로 Rest API로 처리 대량 데이터를 거의 실시간으로 신속하게 저장, 검색, 분석 가능 2-2. Logstash 플러그인을 통해 데이터 집계와 보관, 서버 데이터 처리 담당 ES와 상관없이 독자적으로도 사용 가능 파이프라인으로 데이터를 수집, 필터를 통해 변환 후 Elastic Search로 전송 입력 - Beats, CloudWatch, Eventlog 등 다양한 입력 지원, 데이터 수집 필터 - 형식, 복잡성에 상관없이 데이터를 동적으로 변환 출력 - ES, Email, ECS, Kafka 등 원하는 저장소에 데이터 전송 2-3. Kibana 데이터 시각화 도구 검색 및 aggregation 집계기능을 통해 ES로부터 문서, 집계 결과등을 가져와 웹도구로 시각화 DIscover, Visualize, Dashboard 3개의 기본 메뉴와 다양한 App 들로 구성 플러그인을 통해 App 설치 가능 2-4. Beats 경량 에이전트로 설치 데이터를 Logstash 또는 ES로 전송 Logstash 보다 경량화 된 서비스 Filebeat, Metricbeat, Packetbeat, Winlogbeat, Heartbeat 등 Libbeat을 통해 직접 구축 가능 3. Elastic Search의 특징과 장단점 3-1. 장점 → 3-1. 실시간분석 하둡 시스템과 달리 ES 클러스터가 실행되고 있는 동안에는 계속해서 데이터가 입력 (인덱싱) 되고, 동시에 실시간에 가까운 속도로 색인된 데이터의 집계, 검색이 가능 → 3-2. Full Text 검색 엔진 Lucene은 기본적으로 역파일 색인 구조로 데이터를 저장하며 이를 사용하는 ES도 동일 방식 역색인 - 일반적인 색인의 목적은 문서의 위치에 대한 index를 생성하여 그 문서에 빠르게 접근하기 위함이지만, 역색인은 문서 내의 문자와 같은(혹은 유사한) 내용들에 대한 매핑 정보를 색인하는 것이다.\n내부적으로는 역파일 색인이라도 사용자 관점에서는 JSON 형식으로 전달 쿼리문 또는 쿼리에 결과도 모두 JSON 형태로 반환 key-value 형식이 아닌 문서기반으로 되어있어 복잡한 정보를 포함해도 그대로 저장이 가능하여 직관적 여러 계층 구조의 문서로 저장이 가능하며, 계층 구조로 된 문서도 한 번의 쿼리로 조회 가능 → 3-3. RESTFul API Rest API를 기본으로 지원하여 모든 데이터의 조회, 입력, 삭제를 HTTP 프로토콜을 통해 처리 가능 → 3-4. multitenancy ES의 데이터들은 index라는 논리 집합 단위로 구성되며 서로 다른 저장소에 분산-저장된다. 서로 다른 인덱스들을 별도 커넥션 없이 하나의 쿼리로 묶어서 검색, 하나의 출력 결과를 도출한다. (서로 상이한 인덱스일지라도 검색할 필드명만 같으면 여러 인덱스를 동시에 조회 가능) → 3-5. 확장성 분산 구성이 가능, 분산환경에서 데이터는 shard 단위로 분리 플러그인을 사용한 기능 확장 가능 AWS, MS Azure 같은 클라우드 서비스, Hadoop 플랫폼들과도 연동 가능 3-6. 다양한 알고리즘 제공 점수 기반의 다양한 정확도 알고리즘, 실시간 분석 등의 구현 가능 3-2. 단점 색인된 데이터는 내부 commit, flush 등의 프로세스를 거치기에 1초정도 뒤에 검색이 가능 클러스터의 성능향상을 위해 비용소모가 큰 트랜잭션 롤백이 지원되지 않는다. 업데이트 요청시 기존 문서를 삭제 후 신규 문서를 재생성하기에 업데이트 비용이 크다. 4. Elastic Search 구성 Index 데이터 저장공간 하나의 물리 노드에 여러 개의 논리 인덱스 생성 하나의 인덱스가 여러 노드에 분산 저장 Shard 색인된 문서는 하나의 인덱스와 그 내부의 여러 개의 파티션(샤드)으로 나뉘어 구성 Type 인덱스의 논리적 구조 6.1부터 인덱스당 하나의 타입만 설정 가능 Document 인덱스가 저장되는 최소단위 JSON 포멧으로 저장 RDBMS의 ROW와 동일 Field 문서를 구성하기 위한 속성 하나의 필드는 다수의 데이터 타입 정의 가능 RDBMS의 컬럼과 동일 Mapping 문서의 필드, 필드 속성을 정의하고 색인 방법을 정의하는 프로세스 5. Elastic Search와 RDBMS의 관계 익숙한 관계형 데이터베이스와의 유사 기능 관계를 통해 이해해 보면 다음과 같다.\n참고\nhttps://jaemunbro.medium.com/elastic-search-%EA%B8%B0%EC%B4%88-%EC%8A%A4%ED%84%B0%EB%94%94-ff01870094f0\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/indices.html\nhttps://velog.io/@hanblueblue/Elastic-Search-1 ","permalink":"http://localhost:50666/posts/50/","summary":"\u003chr\u003e\n\u003ch2 id=\"elastic-search란\" ke-size=\"size26\"\u003e1. Elastic Search란?\u003c/h2\u003e\n\u003cp\u003eApache Lucene에 구축되어 배포된 검색 및 분석 엔진이다. 현재 검색엔진을 넘어 보안, 로그분석, Full-text 분석 등 다양한 영역에서 사용되며, Kibana, Logstash, Beats들과 함께 사용된다. 오픈 소스 프로젝트로 활발히 개발되고 있으며, 유닉스, 자바의 기초지식 필요하다. Apache Lucene의 한계를 보완하기 위한 새로운 검색엔진 프로젝트로 시작되었고 Logstash, Kibana와 함께 사용되어 ELK Stack (ES, Logstash, Kibana)라고 불렸으나 2013년 Logstash, Kibana 프로젝트 정식 흡수되었다.\u003c/p\u003e\n\u003ch2 id=\"elastic-stack이란\" ke-size=\"size26\"\u003e2. Elastic Stack이란?\u003c/h2\u003e\n\u003cp\u003eES, Logstash, Kibana를 묶은 ELK 서비스이다. 5.0.0 버전부터 Beats를 포함하며 Elastic Stack 이란 이름으로 서비스가 제공되고 있다. 서버로부터 모든 유형의 데이터를 가져와 실시간 검색, 분석, 시각화를 도와주는 Elastic 오픈 소스 서비스 제품이다.\u003c/p\u003e","title":"[Elastic Search] Elastic Search란? Elastic Search의 개념 및 장단점"},{"content":" 1. SSR (Server Side Rendering) 서버에서 렌더링 준비를 마친 상태로 클라이언트에 자원을 전달한다.\n1-1. SSR 작동 방식 유저가 웹사이트 자원을 요청 서버에서 \u0026ldquo;렌더링 가능한\u0026rdquo; HTML 파일 생성 (리소스 체크, 컴파일 후 완성된 HTML 콘텐츠 생성) 브라우저는 즉시 HTML 렌더링, 사이트 조작 불가 상태 클라이언트가 자바스크립트를 다운받는다. 다운로드하여지고 있는 사이 콘텐츠는 볼 수 있지만 조작은 불가, 이 기간 동안 유저의 액션을 기억 브라우저가 자바스크립트 프레임워크를 실행 자바스크립트가 컴파일된 후 기억하고 있던 유저 액션을 실행시킨다. 서버에서 렌더링 가능한 상태로 이미 전달되기에 자바스크립트를 받는 동안 특정 자원을 볼 수 있다. 1-2. SSR 장점 초기 페이지의 로딩속도가 빠르다. 서버에서 컴파일되어 클라이언트로 넘어오기에 클롤러 대응에 용이하여 SEO 친화적이다. 클라이언트 하드웨어 및 소프트웨어 성능에 영향을 덜 받는다. 1-3. SSR 선택 기준 네트워크가 느릴 때 (페이지마다 나눠서 불러오기 때문) 검색엔진 최적화가 필요할 때 최초 로딩이 빨라야 할 때 메인 스크립트가 크고 로딩이 느릴 때 웹사이트 상호작용이 별로 없을 때 2. CSR (Client Side Rendering) 렌더링이 클라이언트에서 일어난다. 서버에선 HTML과 JS를 보내고, 클라이언트에서 렌더링을 시작한다. 모든 로직, 데이터, 템플릿, 라우팅은 클라이언트에서 실행된다. 자바스크립트 번들 크기의 영향을 많이 받기에 코드 분할을 고려해야 하며, 적시 적소에 필요한 기능만을 제공해야 한다.\n2-1. CSR 작동방식 유저가 웹사이트 자원을 요청 CDN이 자바스크립트 링크가 포함된 HTML 파일을 바로 보낸다. 브라우저는 HTML을 다운로드하고 자바스크립트를 다운로드한다. 그동안 사이트는 유저에게 보이지 않는다. 자바스크립트가 실행된다. API로부터 받은 데이터를 위치에 넣어준다. 이제 페이지는 상호작용이 가능하다. 서버에서 처리 없이 클라이언트로 보내주기 때문에 자바스크립트, HTML이 모두 다운되고 실행되기 전에 유저가 볼 수 있는 내용은 없다. 2-2. CSR 장점 이미 모든 스크립트가 사전에 로딩되었기에, 후속 페이지 로드 시간이 빠르다. 서버를 호출할 때마다 전체 UI를 다시 로딩할 필요 없다. 클라이언트 자원을 사용하기에 서버 부하가 적다. 2-3. CSR 선택기준 네트워크가 빠르고, 서버 성능이 좋지 않을 때 사용자에게 보여줄 데이터가 많을 때 메인 스크립트가 가벼울 때 웹 애플리케이션에 사용자와 상호작용할 것들이 많을 때 참고\nhttps://ajdkfl6445.gitbook.io/study/web/csr-vs-ssr-vs-ssghttps://ajdkfl6445.gitbook.io/study/web/csr-vs-ssr-vs-ssg\nhttps://off-dngw.github.io/posts/SSR-CSR/https://off-dngw.github.io/posts/SSR-CSR/ ","permalink":"http://localhost:50666/posts/49/","summary":"\u003chr\u003e\n\u003ch2 id=\"ssr-server-side-rendering\" ke-size=\"size26\"\u003e1. SSR (Server Side Rendering)\u003c/h2\u003e\n\u003cp\u003e서버에서 렌더링 준비를 마친 상태로 클라이언트에 자원을 전달한다.\u003c/p\u003e\n\u003ch3 id=\"ssr-작동-방식\" style=\"color: #000000; text-align: start;\" ke-size=\"size23\"\u003e1-1. SSR 작동 방식\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/49/img.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e유저가 웹사이트 자원을 요청\u003c/li\u003e\n\u003cli\u003e서버에서 \u0026ldquo;렌더링 가능한\u0026rdquo; HTML 파일 생성 (리소스 체크, 컴파일 후 완성된 HTML 콘텐츠 생성)\u003c/li\u003e\n\u003cli\u003e브라우저는 즉시 HTML 렌더링, 사이트 조작 불가 상태\u003c/li\u003e\n\u003cli\u003e클라이언트가 자바스크립트를 다운받는다.\u003c/li\u003e\n\u003cli\u003e다운로드하여지고 있는 사이 콘텐츠는 볼 수 있지만 조작은 불가, 이 기간 동안 유저의 액션을 기억\u003c/li\u003e\n\u003cli\u003e브라우저가 자바스크립트 프레임워크를 실행\u003c/li\u003e\n\u003cli\u003e자바스크립트가 컴파일된 후 기억하고 있던 유저 액션을 실행시킨다.\u003c/li\u003e\n\u003cli\u003e서버에서 렌더링 가능한 상태로 이미 전달되기에 자바스크립트를 받는 동안 특정 자원을 볼 수 있다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"ssr-장점\" ke-size=\"size23\"\u003e1-2. SSR 장점\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e초기 페이지의 로딩속도가 빠르다. \u003c/li\u003e\n\u003cli\u003e서버에서 컴파일되어 클라이언트로 넘어오기에 클롤러 대응에 용이하여 SEO 친화적이다.\u003c/li\u003e\n\u003cli\u003e클라이언트 하드웨어 및 소프트웨어 성능에 영향을 덜 받는다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"ssr-선택-기준\" ke-size=\"size23\"\u003e1-3. SSR 선택 기준\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e네트워크가 느릴 때 (페이지마다 나눠서 불러오기 때문)\u003c/li\u003e\n\u003cli\u003e검색엔진 최적화가 필요할 때\u003c/li\u003e\n\u003cli\u003e최초 로딩이 빨라야 할 때\u003c/li\u003e\n\u003cli\u003e메인 스크립트가 크고 로딩이 느릴 때\u003c/li\u003e\n\u003cli\u003e웹사이트 상호작용이 별로 없을 때\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"csr-client-side-rendering\" ke-size=\"size26\"\u003e2. CSR (Client Side Rendering)\u003c/h2\u003e\n\u003cp\u003e렌더링이 클라이언트에서 일어난다. 서버에선 HTML과 JS를 보내고, 클라이언트에서 렌더링을 시작한다. 모든 로직, 데이터, 템플릿, 라우팅은 클라이언트에서 실행된다. 자바스크립트 번들 크기의 영향을 많이 받기에 코드 분할을 고려해야 하며, 적시 적소에 필요한 기능만을 제공해야 한다.\u003c/p\u003e","title":"[WEB] SSR(Server Side Rendering) 과 CSR(Client Side Rendering)의 개념 및 차이"},{"content":" 1. REST란? REST란 Representational State Transfer의 약자로 자원을 이름으로 구분하여 자원의 상태를 주고받는 것을 의미한다. HTTP URI를 통해 자원을 명시하고 HTTP Method를 통해 행위를 적용한다.\n1-1. REST 구성요소 자원(Resource) : HTTP URI - 서버는 고유한 리소스 식별자로 각 리소스를 식별 행위(Verb) : HTTP Method (GET, POST, PUT, DELETE) 내용(Representations) : HTTP Message Pay Load - 하나의 자원은 JSON,XML, TEST, RSS 등 여러 형태의 Representaion으로 나타내어질 수 있다. 1-2. REST의 특징 Stateless (무상태성) - 서버가 이전의 모든 요청과 독립적으로 클라이언트 요청을 완료함을 의미 Cacheable(캐쉬 가능성) - 일부 응답을 저장하는 프로세스인 캐싱을 지원함을 의미 Layered System (계층화) - 클라이언트는 REST API Server만 호출하지만, 클라이언트 요청을 이행하기 위해 함께 작동하는 비즈니스 로직(보안, 암호화 등)을 여러 계층으로 실행될 수 있도록 유연하게 설계 가능함을 의미 Uniform Interface (\b균일한 인터페이스) - 서버가 표준 형식으로 정보를 전송함을 의미 1-3. 장점 HTTP 프로토콜을 그대로 사용하기에 별도 인프라를 구축할 필요가 없음 HTTP 프로토콜을 따르는 모든 플랫폼에서 사용 가능 API의 의도를 쉽고 명확하게 파악 가능 클라이언트, 서버를 완전히 분리하기에 각 부분이 독립적으로 발전 가능 사용되는 기술과 독립적이기에 API 설계에 영향을 주지 않고 다양한 프로그래밍 언어로 작성이 가능 1-4. 단점 표준이 존재하지 않아 정의가 필요함 HTTP Method 형태가 제한적 2. RESTful API란? REST 아키텍쳐를 따르는 API를 RESTful API (Representaional state transfer API)라고 하며 REST 아키텍처를 구현하는 웹서비스를 RESTful 웹 서비스라고 한다. REST는 복잡한 네트워크에서 통신을 관리하기 위한 지침으로 만들어 졌으며, 대규모의 고성능 통신을 안정적으로 지원할 수 있고 쉽게 구현 및 수정할 수 있어 파악에 용이하고 여러 시스템에서 사용이 가능하다.\n3. RESTful API 설계 동사 보다는 명사, 대문자보다 소문자 사용 /getArticles/1 -\u0026gt; /articles/1 컬렉션 이름으로는 복수 명사 사용 /article/1 -\u0026gt; /articles/1 HTTP Method를 포함하지 않음 /get/articles/1 -\u0026gt; GET /articles/1 행위에 대한 동사 표현이 포함하지 않음 /show/articles/1 -\u0026gt; /articles/1 경로 부분 중 변하는 부분은 유일값으로 대체 /articles/{articleId} -\u0026gt; 각 articleId은 유일한 결과값을 가진다. 마지막에 / 포함하지 않음 /articles/1/ -\u0026gt; /articles/1 언더바 대신 하이픈 사용 /newest_ariticles/1 -\u0026gt; /newest-ariticles/1 파일 확장자는 URI에 포함하지 않음 /articles/1/photo.jpg -\u0026gt; /articles/1/photo [Accept: image/jpg] 참고 https://aws.amazon.com/ko/what-is/restful-api/https://aws.amazon.com/ko/what-is/restful-api/\nhttps://khj93.tistory.com/entry/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-REST-API%EB%9E%80-REST-RESTful%EC%9D%B4%EB%9E%80https://khj93.tistory.com/entry/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-REST-API%EB%9E%80-REST-RESTful%EC%9D%B4%EB%9E%80 https://gmlwjd9405.github.io/2018/09/21/rest-and-restful.html\n","permalink":"http://localhost:50666/posts/48/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/48/img.png\"\u003e\n \u003c/p\u003e\n\u003ch2 id=\"1-rest란\"\u003e1. REST란?\u003c/h2\u003e\n\u003cp\u003eREST란 Representational State Transfer의 약자로 자원을 이름으로 구분하여 자원의 상태를 주고받는 것을 의미한다. HTTP URI를 통해 자원을 명시하고 HTTP Method를 통해 행위를 적용한다.\u003c/p\u003e\n\u003ch3 id=\"rest-구성요소\" ke-size=\"size23\"\u003e1-1. REST 구성요소\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e자원(Resource) :\u003c/strong\u003e HTTP URI - 서버는 고유한 리소스 식별자로 각 리소스를 식별\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e행위(Verb) :\u003c/strong\u003e HTTP Method (GET, POST, PUT, DELETE)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e내용(Representations) :\u003c/strong\u003e HTTP Message Pay Load - 하나의 자원은 JSON,XML, TEST, RSS 등 여러 형태의 Representaion으로 나타내어질 수 있다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"rest의-특징\" ke-size=\"size23\"\u003e1-2. REST의 특징\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eStateless (무상태성) -\u003c/strong\u003e 서버가 이전의 모든 요청과 독립적으로 클라이언트 요청을 완료함을 의미\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCacheable(캐쉬 가능성)\u003c/strong\u003e - 일부 응답을 저장하는 프로세스인 캐싱을 지원함을 의미\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLayered System (계층화)\u003c/strong\u003e - 클라이언트는 REST API Server만 호출하지만, 클라이언트 요청을 이행하기 위해 함께 작동하는 비즈니스 로직(보안, 암호화 등)을 여러 계층으로 실행될 수 있도록 유연하게 설계 가능함을 의미\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUniform Interface (\b균일한 인터페이스)\u003c/strong\u003e - 서버가 표준 형식으로 정보를 전송함을 의미\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"장점\" ke-size=\"size23\"\u003e1-3. 장점\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eHTTP 프로토콜을 그대로 사용하기에 별도 인프라를 구축할 필요가 없음\u003c/li\u003e\n\u003cli\u003eHTTP 프로토콜을 따르는 모든 플랫폼에서 사용 가능\u003c/li\u003e\n\u003cli\u003eAPI의 의도를 쉽고 명확하게 파악 가능\u003c/li\u003e\n\u003cli\u003e클라이언트, 서버를 완전히 분리하기에 각 부분이 독립적으로 발전 가능\u003c/li\u003e\n\u003cli\u003e사용되는 기술과 독립적이기에 API 설계에 영향을 주지 않고 다양한 프로그래밍 언어로 작성이 가능\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"단점\" ke-size=\"size23\"\u003e1-4. 단점\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e표준이 존재하지 않아 정의가 필요함\u003c/li\u003e\n\u003cli\u003eHTTP Method 형태가 제한적\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"restful-api란\" ke-size=\"size26\"\u003e2. RESTful API란?\u003c/h2\u003e\n\u003cp\u003eREST 아키텍쳐를 따르는 API를 RESTful API (Representaional state transfer API)라고 하며 REST 아키텍처를 구현하는 웹서비스를 RESTful 웹 서비스라고 한다. REST는 복잡한 네트워크에서 통신을 관리하기 위한 지침으로 만들어 졌으며, 대규모의 고성능 통신을 안정적으로 지원할 수 있고 쉽게 구현 및 수정할 수 있어 파악에 용이하고 여러 시스템에서 사용이 가능하다.\u003c/p\u003e","title":"[네트워크] REST, RESTful API의 개념 및 설계 가이드"},{"content":" 1. 순환참조란? 순환참조는 맞물린 의존성 주입 (DI) 상태에서 어떤 빈을 먼저 생성할지 결정하지 못해서 생기에 발생한다. BeanA에서 BeanB를 참조(BeanA-\u0026gt;BeanB) 일 경우 스프링은 BeanB를 먼저 생성 후 BeanA를 생성하기에, BeanB에서 다시 BeanA를 참조할 경우 (BeanA-\u0026gt;BeanB-\u0026gt;BeanA) 순환 참조가 발생하게된다. 2. 의존성 주입 의존성 주입의 3가지 상황 (생성자 주입방식, 필드 주입방식, Setter주입)에서 순환참조가 발생할수 있다. 다음 포스트 각각의 상세 내용을 확인할 수 있고, 이번 포스트에서는 각각의 경우에 순환참조가 발생하면 어떤 차이점이 있는지 확인해 보자.\n[Spring] IoC(제어의 역전) \u0026amp; DI(의존성 주입)의 개념\n▶ 2-1. 생성자 주입 @Component public class BeanA { private BeanB beanB; public void BeanA(BeanB beanB){ this.beanB = beanB; } } @Component public class BeanB { private BeanA beanA; public void BeanB(BeanA beanA){ this.beanA = beanA; } } 생성자 주입의 경우, 애플리케이션 구동 시 스프링 컨테이너(IOC)는 BeanA 빈을 생성하기 위해 BeanB를 찾고 BeanB를 찾기 위해 Bean A를 찾기 때문에 순환참조가 발생하게 된다.\n▶ 2-2. 필드 주입, Setter 방식 필드 주입, Setter 방식은 애플리케이션의 실행 시점에서는 에러가 발생되지 않는다. 어플리케이션의 실행 시점이 아닌, 실제로 사용되는 시점에 실행되는 메서드가 순환 호출되기 때문이다. 필요 없는 시점에는 null 상태로 유지 후 사용될 때 의존성이 주입되며 참조되기 시작한다.\n3. 해결책 ▶ 3-1. @Lazy 어노테이션 @Component public class BeanA { private BeanB beanB; public void BeanA(BeanB beanB){ this.beanB = beanB; } } @Component public class BeanB { private BeanA beanA; public void BeanB(@Lazy BeanA beanA){ this.beanA = beanA; } } 다음과 같이 @Lazy 어노테이션을 통해 시점을 지연시킬 수 있으나 스프링에서는 이 방식을 추천하지 않는다. 애플리케이션 로딩시점이 아닌 Bean이 필요한 시점에 주입받기 때문에 특정 HTTP 요청을 받을 때 Heap 메모리가 증가할 수 있으며 메모리가 충분하지 않은 경우 장애로 이어질 수 있다. 또한 잘못된 빈의 생성시점을 늦추기에 문제상황에 대한 인식이 늦어질 수 있다.\n▶ 3-2. 설계 변경 근본적으로 순환참조가 일어나지 않는 설계를 해야 한다.\n단순하게는 BeanA -\u0026gt; BeanB-\u0026gt; BeanA의 관계를 BeanA -\u0026gt; BeanB -\u0026gt; BeanC 형태로 참조가 순환되지 않도록 분리해야 한다. 참고 / 이미지 출처\nhttps://ch4njun.tistory.com/269 ","permalink":"http://localhost:50666/posts/47/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/47/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"순환참조란\" ke-size=\"size26\"\u003e1. 순환참조란?\u003c/h2\u003e\n\u003cp\u003e순환참조는 맞물린 의존성 주입 (DI) 상태에서 어떤 빈을 먼저 생성할지 결정하지 못해서 생기에 발생한다. BeanA에서 BeanB를 참조(BeanA-\u0026gt;BeanB) 일 경우 스프링은 BeanB를 먼저 생성 후 BeanA를 생성하기에, BeanB에서 다시 BeanA를 참조할 경우 (BeanA-\u0026gt;BeanB-\u0026gt;BeanA) 순환 참조가 발생하게된다. \u003c/p\u003e\n\u003ch2 id=\"의존성-주입\" ke-size=\"size26\"\u003e2. 의존성 주입 \u003c/h2\u003e\n\u003cp\u003e의존성 주입의 3가지 상황 (생성자 주입방식, 필드 주입방식, Setter주입)에서 순환참조가 발생할수 있다. 다음 포스트 각각의 상세 내용을 확인할 수 있고, 이번 포스트에서는 각각의 경우에 순환참조가 발생하면 어떤 차이점이 있는지 확인해 보자.\u003c/p\u003e","title":"[Spring] 순환참조란? The dependencies of some of the beans in the application context form a cycle"},{"content":" 1. PostgreSQL Function이란? SQL 함수는 임의의 SQL문들을 실행하고 마지막 쿼리의 결과를 반환한다. 단순한 형태의 함수는 마지막 쿼리의 첫 번째 row가 리턴된다. (order by 를 사용하지 않는 경우 다중 row의 첫 번째 행은 별도 정의되지 않기에 결과 row가 매번 다를 수 있다.) 마지막 쿼리가 row를 하나도 반환하지 않을 경우 null이 리턴된다. SQL 함수는 함수의 리턴 유형을 특정 타입의 집합 (SET)으로 선언하거나, 테이블로 선언하여 반환할 수 있다. 이 경우에는 마지막 쿼리의 모든 ROW가 리턴된다. SQL함수의 body는 세미콜론(;)으로 구분된 SQL구문의 집합이어야만 한다. 마지막 구문 뒤의 세미콜론(;)은 생략하여도된다. 함수가 void를 리턴하는 것으로 선언되지 않았다면, 마지막 구문은 반환절이 존재하는 select, insert, update, delete 여야만 한다. 모든 종류의 SQL 언어의 명령 모음은 패키징 되어 함수로 정의될 수 있다. select쿼리 외에도 insert, update, delete, merge 등의 데이터 수정쿼리 및 기타 SQL을 포함할 수 있지만, 트랜잭션 제어 명령( ex. commit, savepoint) 및 vacutaion 등의 일부 유틸리티 명령은 사용할 수 없다. SQL이 작동은 하지만 특정 값을 리턴하지 않는 SQL 함수를 정의하고 싶다면, void를 리턴하는 것으로 정의할 수 있다. ▶ 1-1. Function 간단 예시 다음은 emp 테이블에서 음수의 salary를 삭제하는 함수이다.\nCREATE FUNCTION clean_emp() RETURNS void AS \u0026#39; DELETE FROM emp WHERE salary \u0026lt; 0; \u0026#39; LANGUAGE SQL; SELECT clean_emp(); clean_emp ----------- (1 row) 리턴 타입이 없는 문제를 해결하기 위해 프러시저로도 실행이 가능하다.\nCREATE PROCEDURE clean_emp() AS \u0026#39; DELETE FROM emp WHERE salary \u0026lt; 0; \u0026#39; LANGUAGE SQL; CALL clean_emp(); 이처럼 단순한 케이스에서 리턴값이 없는 함수와 프러시저는 작성 스타일 외에는 없어보인다. 하지만 프로시져는 transaction 컨트롤 등 추가적인 기능을 제공한다. 또한 프러시저가 SQL 표준이므로 return 값이 없는 경우 프러시저를 사용해야 한다.\n함수와 프러시저의 차이는 다음 포스트에서 확인이 가능하다.\n[PostgreSQL] Trigger, Procedure, Function (history 관리하기)\n2. SQL Function Arguments SQL의 함수 인자는 이름이나 숫자를 사용하여 함수 body에서 참조할 수 있다. 이름을 사용하려면 함수 인자에 이름이 있는 것으로 선언한 다음 함수 본문에 해당 이름을 사용하면 된다. 인자 이름이 함수 내 SQL 테이블의 칼럼과 하나라도 일치한다면, 해당 칼럼이 지정 인자 보다 우선순위를 가진다. 이를 극복하기 위해서 인자의 이름을 함수자체의 이름을 명시한 것으로 지정한다. (ex function_name.argument_name) 이 상태에서 칼럼명과 다시 충돌이 난다면(function_name.argument_name 같은 칼럼명이 있다면), 실제 칼럼이 또다시 우선순위를 가진다.) $n형태의 오래된 방식의 숫자형태의 인자 접근법에 따르면, $1는 첫 번째 인자를 말하며, $2는 두 번째 $n은 n번째 인자를 의미한다. SQL 함수 인자는 식별자가 아닌 데이터 값으로만 사용할 수 있다. -- 가능 INSERT INTO mytable VALUES ($1); -- 불가 INSERT INTO $1 VALUES (42); 3. PostgreSQL Function 예제 ▶ 3-1. 기본 타입 반환 Function 가장 간단한 인자가 없는 integer(기본 타입)을 반환하는 함수이다.\nCREATE FUNCTION one() RETURNS integer AS $$ SELECT 1 AS result; $$ LANGUAGE SQL; -- Alternative syntax for string literal: CREATE FUNCTION one() RETURNS integer AS \u0026#39; SELECT 1 AS result; \u0026#39; LANGUAGE SQL; SELECT one(); one ----- 1 ▶ 3-2. 기본 타입을 인자로 사용하는 Function integer(기본 타입)을 인자로 사용하는 함수이다. -- argument name(인자 명)을 사용하는 경우 CREATE FUNCTION add_em(x integer, y integer) RETURNS integer AS $$ SELECT x + y; $$ LANGUAGE SQL; -- 숫자형태의 인자를 사용하는 경우 CREATE FUNCTION add_em(integer, integer) RETURNS integer AS $$ SELECT $1 + $2; $$ LANGUAGE SQL; SELECT add_em(1, 2) AS answer; answer -------- 3 ▶ 3-3. 이름을 arguments로 사용하는 Function 은행 계좌에서 금액을 차감하는 함수 예제이다. (argument명과 테이블 칼럼이 일치하는 경우)\nCREATE FUNCTION tf1 (accountno integer, debit numeric) RETURNS numeric AS $$ UPDATE bank SET balance = balance - debit WHERE accountno = tf1.accountno; SELECT 1; $$ LANGUAGE SQL; 컬럼 명 ccountno와 인자 accountno의 명칭이 동일하기에 함수 명을 명시해서 tf1.accountno로 사용해야 한다.\n지금은 1을 반환하지만 좀 더 유용하게, 잔액을 반환하게 변경하면\nCREATE FUNCTION tf1 (accountno integer, debit numeric) RETURNS numeric AS $$ UPDATE bank SET balance = balance - debit WHERE accountno = tf1.accountno; SELECT balance FROM bank WHERE accountno = tf1.accountno; $$ LANGUAGE SQL; 혹은 returning을 사용하면\nCREATE FUNCTION tf1 (accountno integer, debit numeric) RETURNS numeric AS $$ UPDATE bank SET balance = balance - debit WHERE accountno = tf1.accountno RETURNING balance; $$ LANGUAGE SQL; 마지막 select 혹은 Returning이 함수에서 정의된 Return 타입과 동일하지 않다면, PostgreSQL에서 자동으로 return 타입을 맞춰 캐스팅한다.\n▶ 3-4. 복합 유형의 Function 복합적 타입을 인자로 사용하는 함수를 작성할 때, 정확히 어떤 인자를 사용하는지 뿐만 아니라 해당 인자의 속성(필드)도 같이 정의해야 한다. 다음은 단일 직원 정보를 보여주는 함수이다.\nCREATE FUNCTION new_emp() RETURNS emp AS $$ SELECT ROW(\u0026#39;None\u0026#39;, 1000.0, 25, \u0026#39;(2,2)\u0026#39;)::emp; $$ LANGUAGE SQL; -- 1. row 자체로 select SELECT new_emp(); new_emp -------------------------- (None,1000.0,25,\u0026#34;(2,2)\u0026#34;) -- 2. 테이블 형태로 select SELECT * FROM new_emp(); name | salary | age | cubicle ------+--------+-----+--------- None | 1000.0 | 25 | (2,2) -- 3. 속성별 select SELECT (new_emp()).name; name ------ None ▶ 3-5. 출력 매개변수(Output Parameters)를 활용한 Function 다음과 같이 출력 매개변수를 정의하여 원하는 출력값을 조합하는 방식의 함수도 생성 가능하다. 이 경우 출력 매개변수의 이름은 각 칼럼 명을 결정한다.\nCREATE FUNCTION sum_n_product (x int, y int, OUT sum int, OUT product int) AS \u0026#39;SELECT x + y, x * y\u0026#39; LANGUAGE SQL; SELECT * FROM sum_n_product(11,42); sum | product -----+--------- 53 | 462 (1 row) 출력 매개변수(Output Parameters)를 활용한 프러시저\n함수와 동일하게 출력 매개변수를 사용할 수 있지만, 호출 시에 출력 매개변수를 포함시켜야 하는 점이 다르다.\nCREATE PROCEDURE tp1 (accountno integer, debit numeric, OUT new_balance numeric) AS $$ UPDATE bank SET balance = balance - debit WHERE accountno = tp1.accountno RETURNING balance; $$ LANGUAGE SQL; -- NULL로 파라미터를 포함시켜야한다. CALL tp1(17, 100.0, NULL); ▶ 3-6. 다수의 인자를 받는 Function 같은 데이터 타입의 다수의 인자를 VARIADIC을 명시한 배열 형태로 받는 함수를 정의할 수 있다. CREATE FUNCTION mleast(VARIADIC arr numeric[]) RETURNS numeric AS $$ SELECT min($1[i]) FROM generate_subscripts($1, 1) g(i); $$ LANGUAGE SQL; SELECT mleast(10, -1, 5, 4.4); mleast -------- -1 (1 row) ▶ 3-7. Default 값을 인자로 가지는 Function 함수는 일부 혹은 모든 입력 인수에 default 값을 설정할 수 있다. 순차적으로 적용되기 때문에 default 설정이 된 변수 뒤의 변수들은 모두 default 설정이 되어야 한다. (ex. a, b, c의 인자 중 b가 default 값이 정의되면 c도 default 값 정의가 되어야만 한다.)\nCREATE FUNCTION foo(a int, b int DEFAULT 2, c int DEFAULT 3) RETURNS int LANGUAGE SQL AS $$ SELECT $1 + $2 + $3; $$; SELECT foo(10, 20, 30); foo ----- 60 (1 row) SELECT foo(10, 20); foo ----- 33 (1 row) SELECT foo(10); foo ----- 15 (1 row) SELECT foo(); -- fails since there is no default for the first argument ERROR: function foo() does not exist ▶ 3-8. 테이블 자원으로써의 Function 함수결과 칼럼을 일반 테이블의 열과 동일하게 사용 가능하다. (SETOF를 사용하지 않았기 때문에 1개의 열만 선택하였다.)\nCREATE TABLE foo (fooid int, foosubid int, fooname text); INSERT INTO foo VALUES (1, 1, \u0026#39;Joe\u0026#39;); INSERT INTO foo VALUES (1, 2, \u0026#39;Ed\u0026#39;); INSERT INTO foo VALUES (2, 1, \u0026#39;Mary\u0026#39;); CREATE FUNCTION getfoo(int) RETURNS foo AS $$ SELECT * FROM foo WHERE fooid = $1; $$ LANGUAGE SQL; SELECT *, upper(fooname) FROM getfoo(1) AS t1; fooid | foosubid | fooname | upper -------+----------+---------+------- 1 | 1 | Joe | JOE (1 row) ▶ 3-9. 집합을 반환하는 Function SETOF some type 형태를 반환하는 함수로 선언되면 함수가 출력하는 각 행은 결과 set의 각 요소로 반환된다.\nCREATE FUNCTION getfoo(int) RETURNS SETOF foo AS $$ SELECT * FROM foo WHERE fooid = $1; $$ LANGUAGE SQL; SELECT * FROM getfoo(1) AS t1; fooid | foosubid | fooname -------+----------+--------- 1 | 1 | Joe 1 | 2 | Ed (2 rows) 또한 output parameters에 정의된 칼럼에 맞춰 여러 개의 행을 반환할 수도 있다.\nCREATE TABLE tab (y int, z int); INSERT INTO tab VALUES (1, 2), (3, 4), (5, 6), (7, 8); CREATE FUNCTION sum_n_product_with_tab (x int, OUT sum int, OUT product int) RETURNS SETOF record AS $$ SELECT $1 + tab.y, $1 * tab.y FROM tab; $$ LANGUAGE SQL; SELECT * FROM sum_n_product_with_tab(10); sum | product -----+--------- 11 | 10 13 | 30 15 | 50 17 | 70 (4 rows) ▶ 3-10. 트리구조 요소를 나열하기 위한 SET 반환 Function SELECT * FROM nodes; name | parent -----------+-------- Top | Child1 | Top Child2 | Top Child3 | Top SubChild1 | Child1 SubChild2 | Child1 (6 rows) CREATE FUNCTION listchildren(text) RETURNS SETOF text AS $$ SELECT name FROM nodes WHERE parent = $1 $$ LANGUAGE SQL STABLE; SELECT * FROM listchildren(\u0026#39;Top\u0026#39;); listchildren -------------- Child1 Child2 Child3 (3 rows) SELECT name, child FROM nodes, LATERAL listchildren(name) AS child; name | child --------+----------- Top | Child1 Top | Child2 Top | Child3 Child1 | SubChild1 Child1 | SubChild2 (5 rows) ▶ 3-11. 테이블을 리턴하는 Function 집합을 반환하는 다른 방법은 RETURNS TABLE을 사용하는 것이다. (SETOF와 동일) 이 표기법은 최근 버전 PostgreSQL 표준에 정의되어 있기 때문에 SETOF보다 뛰어날 수 있다.\nCREATE FUNCTION sum_n_product_with_tab (x int) RETURNS TABLE(sum int, product int) AS $$ SELECT $1 + tab.y, $1 * tab.y FROM tab; $$ LANGUAGE SQL; OUT, INOUT 인자를 사용할 수 없고, 모든 결과 칼럼을 TABLE에 정의해야 한다.\n▶ 3-12. 다형성(Polymorphic) Function 인자, 반환값의 형태에 관계없이 함수를 정의할 수 있다.\nCREATE FUNCTION anyleast (VARIADIC anyarray) RETURNS anyelement AS $$ SELECT min($1[i]) FROM generate_subscripts($1, 1) g(i); $$ LANGUAGE SQL; SELECT anyleast(10, -1, 5, 4); anyleast ---------- -1 (1 row) SELECT anyleast(\u0026#39;abc\u0026#39;::text, \u0026#39;def\u0026#39;); anyleast ---------- abc (1 row) CREATE FUNCTION concat_values(text, VARIADIC anyarray) RETURNS text AS $$ SELECT array_to_string($2, $1); $$ LANGUAGE SQL; SELECT concat_values(\u0026#39;|\u0026#39;, 1, 4, 2); concat_values --------------- 1|4|2 (1 row) 참고 + 예제 출처\nhttps://www.postgresql.org/docs/16/xfunc-sql.html#XFUNC-SQL-BASE-FUNCTIONS ","permalink":"http://localhost:50666/posts/46/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/46/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"postgresql-function이란\" ke-size=\"size26\"\u003e1. PostgreSQL Function이란?\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSQL 함수는 임의의 SQL문들을 실행하고 마지막 쿼리의 결과를 반환한다. 단순한 형태의 함수는 마지막 쿼리의 첫 번째 row가 리턴된다. (order by 를 사용하지 않는 경우 다중 row의 첫 번째 행은 별도 정의되지 않기에 결과 row가 매번 다를 수 있다.)\u003c/li\u003e\n\u003cli\u003e마지막 쿼리가 row를 하나도 반환하지 않을 경우 null이 리턴된다.\u003c/li\u003e\n\u003cli\u003eSQL 함수는 함수의 리턴 유형을 특정 타입의 집합 (SET)으로 선언하거나, 테이블로 선언하여 반환할 수 있다. 이 경우에는 마지막 쿼리의 모든 ROW가 리턴된다.\u003c/li\u003e\n\u003cli\u003eSQL함수의 body는 세미콜론(;)으로 구분된 SQL구문의 집합이어야만 한다.\u003c/li\u003e\n\u003cli\u003e마지막 구문 뒤의 세미콜론(;)은 생략하여도된다.\u003c/li\u003e\n\u003cli\u003e함수가 void를 리턴하는 것으로 선언되지 않았다면, 마지막 구문은 반환절이 존재하는 select, insert, update, delete 여야만 한다.\u003c/li\u003e\n\u003cli\u003e모든 종류의 SQL 언어의 명령 모음은 패키징 되어 함수로 정의될 수 있다.\u003c/li\u003e\n\u003cli\u003eselect쿼리 외에도 insert, update, delete, merge 등의 데이터 수정쿼리 및 기타 SQL을 포함할 수 있지만, 트랜잭션 제어 명령( ex. commit, savepoint) 및 vacutaion 등의 일부 유틸리티 명령은 사용할 수 없다.\u003c/li\u003e\n\u003cli\u003eSQL이 작동은 하지만 특정 값을 리턴하지 않는 SQL 함수를 정의하고 싶다면, void를 리턴하는 것으로 정의할 수 있다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"function-간단-예시\" ke-size=\"size23\"\u003e▶ 1-1. Function 간단 예시\u003c/h3\u003e\n\u003cp\u003e다음은 emp 테이블에서 음수의 salary를 삭제하는 함수이다.\u003c/p\u003e","title":"[PostgreSQL] 함수(Function)의 정의 및 상세 사용법 (다양한 예제)"},{"content":" 1. 단위 테스트 하나의 모듈을 기준으로 독립적으로 진행되는 가장 작은 단위의 테스트이다. 통합 테스트의 경우 시스템을 구성하는 컴포넌트들이 커질수록 테스트 시간이 길어지지만, 단위 테스트의 경우 해당 부분만 독립적으로 테스트하기에 코드의 변경이 있어도 빠르게 문제 여부를 확인할 수 있다. CleanCode 책에 의하면 깨끗한 테스트 코드는 다음 5가지 규칙을 따라야 한다.\nFast - 빠르게 동작하여 자주 돌릴 수 있어야 한다.\nIndependent - 테스트는 독립적이며 서로 의존해서는 안된다.\nRepeatable - 어느 환경에서도 반복이 가능해야 한다.\nSelf-validating - 테스트는 성공 또는 실패로 결과를 내어 자체 검증되어야 한다.\nTimely - 테스트는 적시에, 테스트하려는 실제코드를 구현하기 직전에 구현해야 한다.\n2. Junit Junit은 단위 테스트를 지원하는 오픈소스 프레임워크로 다음과 같은 특징을 가진다.\n문자 혹은 GUI 기반으로 실행 @Test 메서드를 호출할 때마다 새 인스턴스 생성 예상결과를 검증하는 assertion 제공 자동실행, 자체결과 확인 및 즉각적인 피드백 제공 테스트 방식을 구분할 수 있는 어노테이션을 제공하며, 어노테이션만으로 간결하게 실행이 가능 ▶ 2-1. 어노테이션 종류 @DisplayName - 테스트 이름 명시\n@Test - 테스트를 수행할 메서드, Junit은 각 테스트끼리 영향을 주지 않도록 테스트 실행 객체를 매 테스트마다 만들고 종료 시 삭제\\\n@BeforeAll - 전체 테스트를 시작하기 전에 1회 실행 (ex. 데이터베이스 연결, 테스트환경 초기화, 전체 테스트 실행주기에 한 번만 호출)\n@BeforeEach - 테스트 케이스를 시작하기 전마다 실행 (테스트 메서드에 사용하는 객체 초기화, 테스트에 필요한 데이터 삽입 등)\n@AfterAll - 전체 테스트를 마치고 종료하기 전에 1회 실행 (데이터베이스 연결 종료, 공통으로 사용하는 자원 해제 등)\n@AfterEach - 테스트 케이스를 종료하기 전마다 실행 (테스트 이후 특정데이터를 삭제 등)\n▶ 2-2. AssertJ Junit과 사용해 가독성을 높여주는 라이브러리로 다양한 문법을 지원한다. 기존 Junit은 기댓값과 실제 비교대상이 확실히 보이지 않아 잘 구분이 안되지만 isEqualTo 등 명확한 의미의 매머드로 대체가 가능하다.\n▶ 2-3. given-when-then 패턴 요즘 단위테스트의 가장 보편적인 형태로 1개의 단위테스트를 3단계로 나눠서 처리하는 패턴이다.\ngiven = 테스트 실행을 준비하는 단계 (어떤 상황, 데이터가 주어졌을 때) when = 테스트를 진행하는 단계 (어떤 함수를 실행시키면 ) then = 테스트 결과를 검증하는 단계 (어떤 결과가 기대된다.) 3. 단위 테스트 예제 점수의 평균을 계산해주는 클래스에 대한 단위 테스트를 해보자. 해당 예제는 객체 간의 메시지 교환이 없는 단순한 값 비교, 예외 확인을 위한 테스트 케이스이다.(일반적으로 스프링 애플리케이션은 다양한 객체에서 메시지를 전달받아 의존성이 생기는데, 이럴 경우 Mock(가짜) 객체를 사용하여 테스트가 가능하다.)\n▶ 3-0. 테스트 대상인 평균점수 조회 다음과 같이 0점 이상의 점수들에 대한 평균을 구하는 클래스가 있을 때\npublic class AverageScoreCalculator { private static Integer sum = 0; private static Integer count = 0; public void addScore(Integer score) { if (!validateScores(score)) { throw new IllegalStateException(\u0026#34;Invalid score\u0026#34;); } sum += score; count++; } private boolean validateScores(Integer score) { return score \u0026gt; 0; } public Double getAverageScore() { return (double) (sum / count); } } ▶ 3-1. 점수의 평균이 일치하는지 테스트 실제 평균의 값과, 클래스 연산 결과가 일치하는지 테스트한다.\n@DisplayName(\u0026#34;점수의 평균 테스트\u0026#34;) @Test void averageScoreTest() { //given AverageScoreCalculator averageScoreCalculator = new AverageScoreCalculator(); int[] scores = {10,20,30,40,50}; //when for (int i = 0; i\u0026lt;scores.length; i++) { averageScoreCalculator.addScore(scores[i]); } Double averageScore = averageScoreCalculator.getAverageScore(); //then assertThat(averageScore).isEqualTo(Arrays.stream(scores).average().getAsDouble()); } ▶ 3-2. 평균점수의 범위 테스트 점수의 평균이 1~100점 이내에 존재하는지 확인한다.\n@DisplayName(\u0026#34;평균 점수 범위 테스트\u0026#34;) @Test void averageScoreRangeTest() { //given AverageScoreCalculator averageScoreCalculator = new AverageScoreCalculator(); int[] scores = {10,20,30,40,50}; //when for (int i = 0; i\u0026lt;scores.length; i++) { averageScoreCalculator.addScore(scores[i]); } Double averageScore = averageScoreCalculator.getAverageScore(); //then assertThat(averageScore \u0026gt;= 0 \u0026amp;\u0026amp; averageScore \u0026lt;= 100).isTrue(); } ▶ 3-3. 개별점수 유효성 테스트 유효하지 않은 점수가 인풋 될 경우 IllegalStateException이 기대되기에, assertThrow로 Exception을 테스트한다.\n@DisplayName(\u0026#34;개별 잘못된 점수 테스트\u0026#34;) @Test public void averageScoreInvalidScoreTest(){ //given AverageScoreCalculator averageScoreCalculator = new AverageScoreCalculator(); int[] scores = {10,20,30,40,-1}; //when final IllegalStateException exception = assertThrows(IllegalStateException.class, () -\u0026gt; { for (int i = 0; i\u0026lt;scores.length; i++) { averageScoreCalculator.addScore(scores[i]); } }); //then assertThat(exception.getMessage()).isEqualTo(\u0026#34;Invalid score\u0026#34;); } 4. 주요 Assert 메서드 ▶ 4-1. 주요 비교 검증 테스트 메서드 메서드 이름 설명 isEqualTo(A) A 값과 같은지 검증 isNotEqualTo(A) A 값과 다른지 검증 contains(A) A 값을 포함하는지 검증 doesNotContain(A) A 값을 포함하지 않는지 검증 startWith(A) 접두사가 A인지 검증 endsWith(A) 접미사가 A인지 검증 isEmpty() 비어 있는 값인지 검증 isNotEmpty() 비어 있지 않은 값인지 검증 isPositive() 양수인지 검증 isNegative() 음수인지 검증 isGreaterThan(a) a보다 큰 값인지 검증 isLessThan(a) a보다 작은 값인지 검증 ▶ 4-2. HTTP 주요 응답코드 테스트 메서드 코드 매핑 메서드 설명 200 OK isOk() HTTP 응답코드가 200 OK인지 검증 201 Created isCreated() HTTP 응답코드가 201 Created 검증 400 Bad Request isBadRequest() HTTP 응답코드가 400 BadRequest검증 403 Forbidden isForbidden() HTTP 응답코드가 403 Forbidden검증 404 Not Found isNotFound() HTTP 응답코드가 404 Not Found 검증 4** is4xxClientError() HTTP 응답코드가 4** 검증 500 Internal Server Error isInternalServerError() HTTP 응답코드가 500 InternalServerError 검증 5** is5xxClientError() HTTP 응답코드가 5** 검증 참고\n도서 : 스프링 부트 3 백엔드 개발자 되기 - 자바 편\nhttps://mangkyu.tistory.com/143\n","permalink":"http://localhost:50666/posts/45/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/45/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"단위-테스트\" style=\"color: #000000; text-align: start;\" ke-size=\"size26\"\u003e1. 단위 테스트\u003c/h2\u003e\n\u003cp\u003e하나의 모듈을 기준으로 독립적으로 진행되는 가장 작은 단위의 테스트이다. 통합 테스트의 경우 시스템을 구성하는 컴포넌트들이 커질수록 테스트 시간이 길어지지만, 단위 테스트의 경우 해당 부분만 독립적으로 테스트하기에 코드의 변경이 있어도 빠르게 문제 여부를 확인할 수 있다. CleanCode 책에 의하면 깨끗한 테스트 코드는 다음 5가지 규칙을 따라야 한다.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eFast -\u003c/strong\u003e 빠르게 동작하여 자주 돌릴 수 있어야 한다.\u003cbr\u003e\n\u003cstrong\u003eIndependent -\u003c/strong\u003e 테스트는 독립적이며 서로 의존해서는 안된다.\u003cbr\u003e\n\u003cstrong\u003eRepeatable -\u003c/strong\u003e  어느 환경에서도 반복이 가능해야 한다.\u003cbr\u003e\n\u003cstrong\u003eSelf-validating -\u003c/strong\u003e 테스트는 성공 또는 실패로 결과를 내어 자체 검증되어야 한다.\u003cbr\u003e\n\u003cstrong\u003eTimely -\u003c/strong\u003e 테스트는 적시에, 테스트하려는 실제코드를 구현하기 직전에 구현해야 한다.\u003c/p\u003e","title":"[Spring] 단위 테스트, JUnit의 개념 및 단위 테스트 코드 작성 방법"},{"content":" 1. 개요 PostgreSQL은 쿼리 Planner가 가장 효율적인 쿼리 플랜을 세워 쿼리를 실행시킨다. 이번 포스트는 쿼리 Planner가 플랜을 검색하는 과정을 의도적으로 제한하여 플랜 검색 시간을 단축시키는 방법에 대한 내용이다. 쿼리 선택지를 제한함으로써 시간을 줄이지만, 그만큼 모든 경우를 비교하는 것이기 아니라서 최고의 플랜을 찾을 수 없기에, 테이블 scan 방식 및 인덱스 등 쿼리의 작동방식을 명확히 이해한 후 설정이 필요하며, 설정전 성능비교, 설정 후의 데이터 증감에 따른 지속적인 모니터링이 필요하다.\n2. 플래너의 작동 2-1. JOIN Planner의 작동방식을 보기 위해 간단한 조인 쿼리를 확인해 보자\nSELECT * FROM a, b, c WHERE a.id = b.id AND b.ref = c.id; PostgreSQL 플래너는 조인 순서를 자유롭게 정할 수 있다.\na.id = b.id 조건으로 A, B테이블을 먼저 조인 후에 C테이블을 조인 b.ref = c.id 조건으로 B, C테이블을 먼저 조인 후에 A테이블을 조인 A, C를 조인 후에 B테이블 조인 (A, C의 조인을 최적화하는 조건이 없기에 비효율적) 중요한 점은, 모든 방식이 동일한 결과를 가져오지만, 실행 cost에는 엄청난 차이가 난다는 것이다. 그래서 planner는 가장 효율적인 쿼리 플랜을 찾는다. 사실 쿼리가 2~3개의 테이블만 참조한다면, 고려할 조인방식이 그리 많지 않다. 그러나 테이블 수가 증가할수록 조인 순서의 선택지는 확연히 증가한다. 10개 정도의 테이블이 조인될 경우 모든 경우의 수를 철저히 검색하는 것은 실용적이지 않고, 테이블 수가 6~7개만 되어도 plan을 선택하는데 굉장히 오랜 시간이 걸릴 수 있다. 너무 많은 테이블이 참조될 때, PostgreSQL planner는 검색하는 플랜의 경우의 수를 제한하는 genetic 확률적 검색으로 전환한다.\n이 경우, 검색하는 플랜의 수가 줄어들기에 검색시간은 줄어들지만, 최고의 플랜을 찾지 못할 수도 있다.\n2-2. OUTER-JOIN 다음과 같은 outer 조인에서 planner의 조인 순서 선택지는 확연히 줄어든다.\nSELECT * FROM a LEFT JOIN (b JOIN c ON (b.ref = c.id)) ON (a.id = b.id); 이전 일반 join과 조건은 똑같이 적용되었지만, 작동방식은 현저히 다르다. 기존 일반 조인은 B와 C를 조인한 결과와 일치하지 않는 A의 각 로우들은 생략되어야 하고 outer join은 A의 각 로우들이 생략되지 않는다. 그래서 planner의 선택지는 다음으로 줄어든다.\na.id = b.id 조건으로 A, B테이블을 먼저 조인 후에 C테이블을 조인 b.ref = c.id 조건으로 B, C테이블을 먼저 조인 후에 A테이블을 조인 A, C를 조인 후에 B테이블 조인 (A, C의 조인을 최적화하는 조건이 없기에 비효율적) Planner는 해당 쿼리의 유효한 플랜이 1개로 인식하고, 선택지가 1개이기에 plan을 세우는데 적은 시간이 든다.\n반면에 다음과 같은 경우에 planner는 2개 이상의 plan이 유효하다고 판단할 수도 있다.\nSELECT * FROM a LEFT JOIN b ON (a.bid = b.id) LEFT JOIN c ON (a.cid = c.id); a.id = b.id 조건으로 A, B테이블을 먼저 조인 후에 C테이블을 조인 b.ref = c.id 조건으로 B, C테이블을 먼저 조인 후에 A테이블을 조인 a.cid = c.id 조건으로 A, C테이블을 먼저 조인 후에 B테이블을 조인 현재는 FULL JOIN만이 테이블 간의 조인 순서 제한하지만, 대부분의 LEFT JOIN, RIGHT JOIN은 조인 순서가 재배열될 수 있다. 명시적인 INNER JOIN (INNER JOIN, CROSS JOIN 등)은 구조적으로 FROM 절에 테이블 입력 순서와 의미적으로 동일하게 실행되므로 조인순서를 제약하지 않는다. (영향을 받지 않는다.) 대부분의 JOIN이 순서를 완전히 제약하지 않지만, PostgreSQL 플래너가 모든 JOIN절을 조인 순서를 제약하도록 별도 지시할 수 있다. 예를 들어 다음 세 쿼리는 논리적으로 동일하다.\nSELECT * FROM a, b, c WHERE a.id = b.id AND b.ref = c.id; SELECT * FROM a CROSS JOIN b CROSS JOIN c WHERE a.id = b.id AND b.ref = c.id; SELECT * FROM a JOIN (b JOIN c ON (b.ref = c.id)) ON (a.id = b.id); 그러나 planner에게 조인 순서를 정하도록 하면, 2,3번 쿼리가 첫 번째 쿼리보다 plan을 세우는데 더 적은 시간이 걸리며 실제 플랜도 다르게 나온다. 이 차이는 테이블이 3개 있을 때는 그리 크지 않지만, 많은 테이블을 대상으로 할 때는 굉장히 효율적이다. planner가 명시적 join에 제시된 테이블 조인 순서를 따르게 하려면 join_collapse_limit 매개변수를 1로 설정하면 된다.\n아니면 일반 FROM절 리스트에서 JOIN 구문을 추가해도 되기 때문에, plan 검색시간을 줄이기 위해 파라미터를 조정하여 조인 순서를 완벽하게 제한시킬 필요는 없다.\n예를 들어\nSELECT * FROM a CROSS JOIN b, c, d, e WHERE ...; join_collapse_limit=1 옵션을 주면, 해당 쿼리는 다른 선택지에 대한 고려 없이 A와 B를 우선적으로 조인하도록 강제한다. 그렇기 때문에 이 예제에서는 가능한 조인 순서가 5배로 줄어들게 된다. (5! -\u0026gt; 4!)\n이런 방식으로 planner의 계획 검색을 제한하는 것은 planning 시간을 줄여주고 planner를 좋은 쿼리플랜으로 유도하는데 도움이 된다.\n만약 planner가 안 좋은 조인 순서를 기본으로 선택하였다면, JOIN 문법을 통해 더 좋은 join 순서로 유도할 수 있다. 다만 작성 후 성능비교 및 플랜확인은 필수이다.\n2-3. \bSubquery planning시간이 영향을 주는 유사한 예로는, 서브쿼리를 상위 쿼리에 포함시키는 경우이다. 다음 간단한 서브쿼리를 확인해 보자.\nSELECT * FROM x, y, (SELECT * FROM a, b, c WHERE something) AS ss WHERE somethingelse; 일반적으로 Planner는 서브쿼리를 부모쿼리에 포함시키기에 다음과 같다.\nSELECT * FROM x, y, a, b, c WHERE something AND somethingelse; 이러한 쿼리가 보통 서브쿼리를 따로 planning 하는 것보다 효율적이다. (예를 들어, 부모쿼리의 \bwhere 절은 x를 A에 먼저 조인시켜 A의 많은 row를 제거함으로써 서브쿼리의 결과와 전체 조회를 실행하는 것을 피할 수 있다.) 그러나 동시에, planning 시간이 증가한다.\n2가지의 3개의 테이블을 조인하는 경우의 수[2 x (3!)] -\u0026gt; 5개의 테이블을 조인하는 경우의 수 [5!]\n더 많은 테이블을 참조할 경우, 가능한 조인 방식의 수가 기하급수적으로 늘어남으로써 큰 차이가 된다. planner는 subquery를 상위 쿼리에 포함시킴으로써 조인 방식의 경우의 수가 너무 커지는 문제를 방지하기 위해 from_collapse_limit의 파라미터 값보다 최종 조인될 테이블의 수가 많을 경우 서브쿼리를 상위쿼리에 합치지 않는다. 이 그렇기에 run-time 파라미터를 수정함으로써 planning 시간과 plan의 퀄리티를 조절하여 사용할 수 있다.\n3. from_collapse_limit, join_collapse_limit 설정 from_collapse_limit와 join_collapse_limit 은 비슷한 역할을 하기에 네이밍이 비슷하게 되어있다. from_collapse_limit는 서브쿼리 사용 시 하위 쿼리를 상위쿼리에 포함시키는 조건을 제어하고, join_collapse_limit는 조인 테이블의 수에 따른 테이블 축소 관계의 최대 수를 제어한다. 일반적으로 두 파라미터의 값은 똑같이 설정하거나(명시적 조인과 서브쿼리가 비슷하게 행동하게 하기 위해) 아니면 join_collapse_limit을 1로 세팅한다.(명시적 조인의 순서를 컨트롤하고 싶을 때) 그러나 planning 시간과 run time 시간을 적절하게 조정하기 위해 다른 값을 부여할 수 있다.\n3-1. from_collapse_limit (integer) planner는 FROM 절 테이블 개수가 파라미터 값보다 작을 경우 서브쿼리를 부모쿼리로 재배열한다. 적은 값들은 planning 시간을 줄여주지만 최적의 쿼리 plan을 찾을 수 없을 수도 있다. 기본 값은 8이다. 3-2. join_collapse_limit (integer) planner는 FROM 절 테이블 개수가 파라미터 값보다 작을 경우 명시적인 JOIN절을 다시 구성한다. 작은 값일수록 planning 시간을 줄여주지만, 최적의 쿼리 플랜을 찾을 수 없을 수도 있다. 기본값은 from_collapse_limit과 동일한 값이며 대부분의 경우에 적절한 값이다. 해당값을 1로 설정하는 것은 명시적 Join절의 순서 변환 없이 그대로 사용하게 한다. 쿼리가 항상 이상적인 쿼리 플랜을 선택하는 것이 아니기 때문에, 쿼리를 통해 변수를 일시적으로 1로 설정한 후 원하는 조인순서를 명시적으로 지정할 수 있다. 4. 적용 테스트 결과 FROM절의 순서를 변경함으로써 PostgreSQL Planner의 계획대상을 변경하는 방법은 두 가지가 있다.\n4-1. join_collapse_limit 파라미터의 값을 1로 설정 이 경우 from절의 순서에 맞게 플랜이 변경됨을 확인할 수 있다.\n4-2. 서브쿼리에 OFFSET을 추가하여 서브 쿼리 축소를 방지한다. 다음 두쿼리는 결과는 같지만, 명시적 JOIN 절을 사용하여 Planner의 조인 순서를 변경한 케이스이다.\nselect a.* from a, b, c where a.id = b.id and b.id = c.id; -\u0026gt; a와 b 테이블을 먼저 조인한 후 c 테이블 조인\nselect * from b cross join lateral (select a.* from a, c where a.id = c.id offset 0) as foo; -\u0026gt; a와 c 테이블을 먼저 조인한 후 b 테이블 조인\n5. 결론 from_collapse_limit, join_collapse_limit 파라미터를 통해 조인 및 서브쿼리 사용 시 쿼리 planner의 플랜 선택지를 조정하여 plan 선택시간을 줄일 수 있다. 다만 제한한 조건 내에 최상의 쿼리 플랜이 없을 경우 최적의 쿼리를 찾을 수 없기에, scan, index, 테이블의 크기 등에 따른 상관관계를 명확이 이해하고 추가해야 하며, 설정 전의 충분한 플랜 분석, 설정 후의 데이터 증감에 따른 지속적인 모니터링 및 튜닝이 필요하다. 참고\nhttps://www.postgresdba.com/bbs/board.php?bo_table=C05\u0026amp;wr_id=20\nhttps://www.postgresql.org/docs/current/explicit-joins.html ","permalink":"http://localhost:50666/posts/44/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/44/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"개요\" ke-size=\"size26\"\u003e1. 개요\u003c/h2\u003e\n\u003cp\u003ePostgreSQL은 쿼리 Planner가 가장 효율적인 쿼리 플랜을 세워 쿼리를 실행시킨다. 이번 포스트는 쿼리 Planner가 플랜을 검색하는 과정을 의도적으로 제한하여 플랜 검색 시간을 단축시키는 방법에 대한 내용이다. 쿼리 선택지를 제한함으로써 시간을 줄이지만, 그만큼 모든 경우를 비교하는 것이기 아니라서 최고의 플랜을 찾을 수 없기에, 테이블 scan 방식 및 인덱스 등 쿼리의 작동방식을 명확히 이해한 후 설정이 필요하며, 설정전 성능비교, 설정 후의 데이터 증감에 따른 지속적인 모니터링이 필요하다.\u003c/p\u003e\n\u003ch2 id=\"플래너의-작동\" ke-size=\"size26\"\u003e2. 플래너의 작동\u003c/h2\u003e\n\u003ch3 id=\"join\" ke-size=\"size23\"\u003e2-1. JOIN\u003c/h3\u003e\n\u003cp\u003ePlanner의 작동방식을 보기 위해 간단한 조인 쿼리를 확인해 보자\u003c/p\u003e","title":"[PostgreSQL] 명시적 JOIN 절로 플래너(Planner) 제어, 성능 향상"},{"content":" 1. 스프링 컨테이너(Spring container)란? 스프링 프레임워크의 핵심 컴포넌트이며 주요한 용도는 다음과 같다.\n객체의 생명주기 관리 생성된 객체들의 추가적인 기능 제공 스프링에서는 자바 객체를 빈(Bean)이라고 하며 컨테이너는 내부의 빈 라이프사이클(생성, 제거 등)을 하며 추가 기능을 제공한다. 기존 스프링에서는 xml 파일로 설정하나 스프링 부트에서는 자바 클래스에서 설정가능하다. 주요 설정으로는 [수동]\n@Configuration - 어플리케이션 구성정보를 등록\n@Bean - 메서드를 모두 호출하여 반환된 객체를 스프링 컨테이너에 등록 [자동]\n@ComponentScan - 해당 클레스클래스 패키지와 하위의 @Component, @Service, @Repository, @Controller 클래스를 탐색하여 빈등록 @Component - 스프링 런타임시 자동으로 빈을 찾아 등록 이전 포스트의 IoC는 스프링 빈들의 생명 주기를 관리하기 위해 사용하며, 스프링 컨테이너는 DI(의존성 주입)이 이루어진 빈들을 BeanFactory, ApplicationContext 2개의 컨테이너로 제어하고 관리한다.\n[Spring] - [Spring] IoC(제어의 역전) \u0026amp; DI(의존성 주입)의 개념\n2. 왜 스프링 컨테이너를 사용할까? 자바 코드를 작성시 new생성자를 통해 객체를 매번 생성할 경우, 객체 간의 참조가 많아지고 의존성이 높아지게 된다. 이는 객체 지향 프로그램 원칙에 위배되며 객체 간의 의존성 및 결합도를 낮추기 위해 스프링 컨테이너가 사용된다. 스프링 컨테이너는 구현 클래스의 의존성을 제거하고 인터페이스에만 의존하도록 설계 가능하도록 한다.\n3. 스프링 컨테이너의 종류 도식을 보면 ApplicationContext 인터페이스가 다른 인터페이스를 다중 상속하고 있다\nBeanFactory의 모든 기능을 상속하여 Bean 객체를 관리한다. BeanFactory\n스프링 컨테이너의 최상위 인터페이스 스프링 빈을 관리하고 조회 ApplicationContext\nBeanFactory의 모든 기능을 상속받아서 제공 단순한 빈 관리를 넘어 어플리케이션 개발을 위해 공통적으로 필요한 많은 부가기능을 제공 참고\n도서 : 스프링 부트 3 백엔드 개발자 되기 - 자바 편\nhttps://devloper-dreaming.tistory.com/148\nhttps://ittrue.tistory.com/220%EC%8A%A4%ED%94%84%EB%A7%81%20%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88%EB%8A%94%20%EC%8A%A4%ED%94%84%EB%A7%81%20%ED%94%84%EB%A0%88%EC%9E%84,%EB%B9%88(Bean)%EC%9D%B4%EB%9D%BC%20%ED%95%9C%EB%8B%A4. https://chobopark.tistory.com/200\nhttps://devloper-dreaming.tistory.com/148\n","permalink":"http://localhost:50666/posts/43/","summary":"\u003chr\u003e\n\u003ch2 id=\"스프링-컨테이너spring-container란\" ke-size=\"size26\"\u003e1. 스프링 컨테이너(Spring container)란?\u003c/h2\u003e\n\u003cp\u003e스프링 프레임워크의 핵심 컴포넌트이며 주요한 용도는 다음과 같다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e객체의 생명주기 관리\u003c/li\u003e\n\u003cli\u003e생성된 객체들의 추가적인 기능 제공\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e스프링에서는 자바 객체를 빈(Bean)이라고 하며 컨테이너는 내부의 빈 라이프사이클(생성, 제거 등)을 하며 추가 기능을 제공한다. 기존 스프링에서는 xml 파일로 설정하나 스프링 부트에서는 자바 클래스에서 설정가능하다. 주요 설정으로는\n \u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[수동]\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e@Configuration -\u003c/strong\u003e 어플리케이션 구성정보를 등록\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e@Bean -\u003c/strong\u003e 메서드를 모두 호출하여 반환된 객체를 스프링 컨테이너에 등록\n \u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e[자동]\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e@ComponentScan -\u003c/strong\u003e 해당 클레스클래스 패키지와 하위의 @Component, @Service, @Repository, @Controller 클래스를 탐색하여 빈등록 \u003c/p\u003e","title":"[Spring] 스프링 컨테이너(Spring container)의 개념"},{"content":" 1. IoC (Inversion of Control) 제어의 역전 IoC란 메인 프로그램에서 컨테이너나 프레임워크로 객체와 객체의 의존성에 대한 제어를 넘기는 것을 말한다. 프레임워크 없이 개발할 때는 각 객체에 대한 라이프사이클 (생성, 설정, 초기화, 호출 등)을 개발자가 직접 관리한다. 하지만 프레임워크를 사용하면 객체의 생명주기를 프레임워크에 위임하여, 프레임워크가 개발자의 코드를 호출하고 흐름을 제어하게 할 수 있다. 이처럼, 제어의 역전은 객체를 직접 생성하거나 제어하는 것이아니라 외부에서 관리하는 객체를 가져와서 사용하는 것으로, 클레스 간의 결합을 느슨하게 하여 테스트와 유지관리를 더 쉽게 설계하는 원칙이다. IoC는 설계 패턴이 아닌 원칙이며 세부 구현은 개발자에게 달려있다. 스프링 컨테이너에서는 프로그램 내 객체의 라이프사이클을 인스턴스화하여 관리한다. XML, Java Annotation, Java 코드 등을 통해 애플리케이션에 필요한 객체 및 의존성 정보를 제공한다. IoC 컨테이너의 2가지 핵심 클래스는 다음과 같다.\nBeanFactory - 자바 객체(bean) 인스턴스의 라이프사이클을 관리하는 실질적인 컨테이너이며, 구동될 시가 아니라 요청이 있을 때 Bean 객체를 생성한다. ApplicationContext - 구동되는 시점에 등록된 Bean 객체를 스캔하여 객체화한다. 결론적으로 IoC는...\n어플리케이션 내의 코드 양을 줄인다. 클래스 간 결합을 느슨하게 한다. 애플리케이션 유지관리 및 테스트를 유리하게 한다. 어플리케이션 제어 책임이 개발자에서 프레임워크로 위임되므로, 핵심 비즈니스 로직에 더 집중할 수 있다. 2. DI (Dependency Injection, 의존성 주입) 의존성 주입은 프로그램 디자인이 결합도를 느슨하게 되도록 하고 의존관계 역전원칙과 단일 책임 원칙을 따르도록 클라이언트의 생성에 대한 의존성을 클라이언트 행위로부터 분리하는 것이다.\n2-1. 의존성 주입이란? A가 B에게 \u0026quot;의존\u0026quot;한다는 표현이 어떤 뜻일까? 한 객체가 다른 객체를 사용할 때 의존성이 있다고 표현하며 크게는 다음과 같은 상황에 의존한다고 한다.\n상속 또는 구현하는 경우 메서드를 호출하는 경우 A에서 B를 호출하는 경우 예를 들면 다음과 같은 경우에 Store객체가 Pencil객체에 의존성이 있다고 표현한다.\npublic class Store { private Pencil pencil; } DI는 외부에서 두 객체 간의 관계를 결정해주는 디자인 패턴으로, 인터페이스를 사이에두고 클래스 레벨 의존관계가 고정되지 않도록 관계를 동적으로 주입하여 결합도를 낮출수 있게 해 준다. 어떤 객체가 사용하는 의존 객체를 직접 생성하는 것이 아니라, 주입을 받아 사용 하는것이다. 스프링에서의 의존성 주입은, 각 객체간의 의존관계를 스프링 컨테이너에서 개발자가 정의한 Bean 정보를 바탕으로 자동으로 주입해주는 기능을 의미한다. 이를통해 객체간 결합도를 낮추고 코드양을 줄여주며 유지보수를 편하게 해준다.\n2-2. 의존성 주입 방식 생성자주입, 필드주입, 수정자 주입등의 방법이 있다.\n생성자 주입 (권장) @Controller public class Controller{ private Service service; @Autowired public Controller(Service service){ this.service = service; } } 필드 주입 @Controller public class Controller{ @Autowired private Service service; } 수정자 주입 @Controller public class Controller{ private Service service; @Autowired public setService(Service service){ this.service = service; } } 결론적으로 DI는...\n객체를 직접 생성하는 것이 아닌, 외부에서 생성된 객체를 주입받아서 사용하는 것이다. 강하게 결합된 클래스들을 분리하고, 애플리케이션 실행시점에 관계를 결정해 준다. 결합도를 낮추고 유연성을 확보하여 테스트를 용이하게 하며 개발 및 유지보수를 쉽게 해 준다. 참고\nhttps://kim-oriental.tistory.com/32\nhttps://mangkyu.tistory.com/150\n도서 : 스프링 부트 3 백엔드 개발자 되기 - 자바편\nhttps://stackoverflow.com/questions/3058/what-is-inversion-of-control/3140#3140\nhttps://velog.io/@virtualplastic/Spring-%EC%A0%9C%EC%96%B4%EC%9D%98-%EC%97%AD%EC%A0%84-IOC-Inversion-of-Control#:~:text=5%2F9-,IOC(Inversion%20of%20Control)%20%EC%A0%9C%EC%96%B4%EC%9D%98%20%EC%97%AD%EC%A0%84,%EB%B3%B8%EC%97%85%EC%97%90%20%EC%A7%91%EC%A4%91%ED%95%A0%20%EC%88%98%20%EC%9E%88%EB%8B%A4.\nhttps://june0122.tistory.com/18\n","permalink":"http://localhost:50666/posts/42/","summary":"\u003chr\u003e\n\u003ch2 id=\"ioc-inversion-of-control-제어의-역전\" ke-size=\"size26\"\u003e1. IoC (Inversion of Control) 제어의 역전\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/42/img.png\"\u003e\u003c/p\u003e\n\u003cp\u003eIoC란 메인 프로그램에서 컨테이너나 프레임워크로 객체와 객체의 의존성에 대한 제어를 넘기는 것을 말한다. 프레임워크 없이 개발할 때는 각 객체에 대한 라이프사이클 (생성, 설정, 초기화, 호출 등)을 개발자가 직접 관리한다. 하지만 프레임워크를 사용하면 객체의 생명주기를 프레임워크에 위임하여, 프레임워크가 개발자의 코드를 호출하고 흐름을 제어하게 할 수 있다.\n \u003c/p\u003e\n\u003cp\u003e이처럼, 제어의 역전은 객체를 직접 생성하거나 제어하는 것이아니라 외부에서 관리하는 객체를 가져와서 사용하는 것으로, 클레스 간의 결합을 느슨하게 하여 테스트와 유지관리를 더 쉽게 설계하는 원칙이다.\n \u003c/p\u003e","title":"[Spring] IoC(제어의 역전) \u0026 DI(의존성 주입)의 개념"},{"content":" 1. JWT의 개념 JWT는 웹에서 사용자 인증/인가에 사용하는 토큰으로 Json Web Token의 줄임말이다. 웹에서 사용되는 JSON 형태의 토큰 표준 규격이며 쿠키와 유사하지만, 서명된 토큰이라는 차이점이 있다. 공개키, 개인키의 쌍으로 사용할 경우 서명된 토큰은 개인키를 보유한 서버에서만 복호화가 가능하다. 보통 Authorization HTTP 헤더를 Bearer \u0026lt;토큰\u0026gt; 형태로 설정하여 클라이언트에서 서버로 전송한다. 서버에서는 토큰에 포함되어 있는 서명정보로 위변조를 검증하며 토큰은 Base64 인코딩 되어있다. 2. JWT 구조 header, payload, signature가 각각 . 으로 구분되어 있다\n\u0026lt;header\u0026gt;.\u0026lt;payload\u0026gt;.\u0026lt;signature\u0026gt; header - 토큰의 타입, 서명 알고리즘 이 저장 { \u0026#34;alg\u0026#34;: \u0026#34;HS256\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;JWT } payload - 사용자의 인증 / 인가 정보를 key-value 형태로 저장 { \u0026#34;sub\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Ado Kukic\u0026#34;, \u0026#34;admin\u0026#34;: true, \u0026#34;iat\u0026#34;: 1464297885 } signature - header와 payload가 비밀키로 서명되어 저장, header를 디코딩한 값, payload를 디코딩한 값을 합치고 서버의 개인키로 암호화를 한 값 3. JWT 표준 스펙 payload의 key의 값은 자유롭게 설정이 가능하지만, 네트워크 상으로 전송되기에 크기가 작아야 유리하기에 JSON 형태로 데이터를 저장할 때 키를 3글자로 줄이는 관행이 있다. 다음은 JWT key의 표준 스펙이다.\nsub(subject) - 인증주체 iss(issue) - 토큰 발급처 typ(type) - 토큰유형 alg(algorithm) - 서명 알고리즘 iat(issued at) - 발급 시각 exp(expiration time) - 만료 시각 aud(audience) - 클라이언트 jti(JWT Id) - JWT 토큰 식별자, iss가 여러 명일 때 구분하기 위한 값 4. JWT를 통한 인증 JWT는 OAuth, OIDC 프로토콜과 함께 API 인증 및 인가를 위해 주로 사용된다. 보통 클라이언트가 서비스 인가 서버를 통해 로그인을 성공하면 JWT 토큰을 획득하고 클라이언트는 해당 서비스의 API를 호출할 때 JWT토큰을 보내서 원하는 자원에 접근하거나 허용된 작업을 수행할 수 있게 된다. 5. 장점 토큰 검증만을 통해 사용자 정보를 확인가능 하여 추가 검증 로직이 필요 없다. 매번 세션이나 데이터베이스 같은 인증 저장소가 필요 없다. 사용자가 늘어나더라도 사용자 인증을 위한 추가 리소스 비용이 없다. 다른 서비스에 공통 스펙으로 사용이 가능하여 확장성이 높다. 6. 한계 base64 인코딩 정보를 전달하기에 전달량이 많다. 토큰이 탈취당할 시 만료될 때까지 대처가 불가능하다. Payload부분은 누구든 디코딩하여 확인할 수 있다. 위의 세 가지 예시로 JWT 토큰을 생성하면 다음과 같다.\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkFkbyBLdWtpYyIsImFkbWluIjp0cnVlLCJpYXQiOjE0NjQyOTc4ODV9.XRt6GrUEcSJiGRIU0jScszrjunhot3l75g6x8ZCbpV0 토큰 자체는 암호화되어있지 않기에 jwt.io 등을 통해 디코딩할 경우 데이터를 바로 확인이 가능하다.\n7. 주의사항 JWT토큰은 누구나 열람이 가능하기에 민감한 사용자 정보를 토큰에 그대로 저장하면 안 된다. 토큰에는 사용자를 식별할 수 있는 ID정도만 저장해야 하며 꼭 사용해야 하는 경우에도 민감 사용자 정보는 반드시 별도 암호화하여 토큰을 디코딩한 후에도 식별 불가하게 해야 한다. JWT토큰을 탈취당할 경우 대처가 힘들기에 만료기간을 적절히 짧게 사용해야 하지만 JWT토큰 유효시간 무작정 짧게 설정 시 사용 편의성이 떨어질 수 있다. 이 경우에는 다음과 같은 방식을 통해 짧은 유효기간을 보완할 수 있다. sliding session - 특정서비스를 계속 사용하는 유저에게 만료 시간을 연장 refresh token - 기존 access 토큰에 추가로 더 긴 만료기간의 refresh 토큰을 발급하여, access토큰 만료시 refresh토큰을 사용하여 새로운 access토큰을 발급 참고\nhttps://inpa.tistory.com/entry/WEB-%F0%9F%93%9A-JWTjson-web-token-%EB%9E%80-%F0%9F%92%AF-%EC%A0%95%EB%A6%AC\nhttps://dzone.com/articles/cookies-vs-tokens-the-definitive-guide\nhttps://brunch.co.kr/@jinyoungchoi95/1\nhttps://www.daleseo.com/jwt/#:~:text=JWT(Json%20Web%20Token)%EB%8A%94,%EC%A3%BC%EA%B3%A0%20%EB%B0%9B%EA%B8%B0%20%EC%9C%84%ED%95%B4%EC%84%9C%20%EC%82%AC%EC%9A%A9%EB%90%A9%EB%8B%88%EB%8B%A4. ","permalink":"http://localhost:50666/posts/41/","summary":"\u003chr\u003e\n\u003ch2 id=\"jwt의-개념\" ke-size=\"size26\"\u003e1. JWT의 개념\u003c/h2\u003e\n\u003cp\u003eJWT는 웹에서 사용자 인증/인가에 사용하는 토큰으로 Json Web Token의 줄임말이다. 웹에서 사용되는 JSON 형태의 토큰 표준 규격이며 쿠키와 유사하지만, 서명된 토큰이라는 차이점이 있다. 공개키, 개인키의 쌍으로 사용할 경우 서명된 토큰은 개인키를 보유한 서버에서만 복호화가 가능하다.\n \u003c/p\u003e\n\u003cp\u003e보통 Authorization HTTP 헤더를 Bearer \u0026lt;토큰\u0026gt; 형태로 설정하여 클라이언트에서 서버로 전송한다. 서버에서는 토큰에 포함되어 있는 서명정보로 위변조를 검증하며 토큰은 Base64 인코딩 되어있다.\n \u003c/p\u003e\n\u003ch2 id=\"2-jwt-구조\"\u003e2. JWT 구조\u003c/h2\u003e\n\u003cp\u003eheader, payload, signature가 각각 . 으로 구분되어 있다\u003c/p\u003e","title":"[WEB] JWT 토큰 인증의 개념과 장단점"},{"content":" 1. 윈도우 함수 (Window Functions)란? 윈도우 함수는 행과 행 간의 관계를 쉽게 정의하기 위해 만든 함수이다. 이 기능은 일반 집계함수의 연산과 유사하지만, 일반 집계함수가 행 각각을 단일 그룹화해서 출력하는 반면에, 윈도우 함수는 각각의 행들이 그룹화되지 않으며 별도의 ID를 가진다. 그렇기에 윈도우 함수는 현재 row의 정보보다 더 많은 정보에 접근이 가능하다. 예를 들면 다음과 같다.\n일반집계함수 : COUNT() + GROUP BY-\u0026gt; 그룹별 1개의 행 출력 (그룹 개수만큼 출력, 자르기 + 집약)\n윈도우집계함수 : COUNT() OVER (PARTITION BY) -\u0026gt; ID개수만큼 행 출력 (행의 개수가 줄어들지 않는다, 자르기)\n다음의 공식문서 예제를 보며 윈도우 함수가 어떻게 작동하는지 알아보자. 임직원의 월급, 부서, 직원번호가 포함된 empsalary 테이블이 있다.\nSELECT depname, empno, salary, avg(salary) OVER (PARTITION BY depname) FROM empsalary; depname | empno | salary | avg -----------+-------+--------+----------------------- develop | 11 | 5200 | 5020.0000000000000000 develop | 7 | 4200 | 5020.0000000000000000 develop | 9 | 4500 | 5020.0000000000000000 develop | 8 | 6000 | 5020.0000000000000000 develop | 10 | 5200 | 5020.0000000000000000 personnel | 5 | 3500 | 3700.0000000000000000 personnel | 2 | 3900 | 3700.0000000000000000 sales | 3 | 4800 | 4866.6666666666666667 sales | 1 | 5000 | 4866.6666666666666667 sales | 4 | 4800 | 4866.6666666666666667 (10 rows) 첫 3개의 컬럼은 테이블의 데이터를 바로 사용하는 것이고, row 당 1개의 값을 가지고 있다. 4번째 컬럼은 같은 부서명의 ROW 끼리의 평균 월급을 나타낸다. (비윈도우 함수의 avg 함수와 동일하지만, over 구문을 사용할 경우 윈도우 함수로 취급받고, window frame 상에서 연산될 수 있게 해 준다.) 윈도우 함수는 함수명, 혹은 변수 뒤에 항상 over를 바로 뒤에 붙여 사용한다. over 구문은 쿼리의 row들이 윈도우 함수에 의해 정확히 어떻게 분할되어 작동하는지에 대한 결정을 내린다. over 내의 partition by 구분은 동일한 값을 공유하는 groups 혹은 partitions으로 행을 분할한다. 이렇게 분할된 파티션 상에서 각 행과 동일한 파티션에 속하는 행끼리 연산하게 된다. over 내에 order by를 통해 윈도우 함수에 통과시킬 row의 순서를 정할 수 있다.\nSELECT depname, empno, salary, rank() OVER (PARTITION BY depname ORDER BY salary DESC) FROM empsalary; depname | empno | salary | rank -----------+-------+--------+------ develop | 8 | 6000 | 1 develop | 10 | 5200 | 2 develop | 11 | 5200 | 2 develop | 9 | 4500 | 4 develop | 7 | 4200 | 5 personnel | 2 | 3900 | 1 personnel | 5 | 3500 | 2 sales | 1 | 5000 | 1 sales | 4 | 4800 | 2 sales | 3 | 4800 | 2 (10 rows) rank 함수는 해당 파티션 당 order by 값에 맞는 숫자 형태의 순위를 나타낸다. rank는 over 절에 의해서만 결정되기에 명시적인 매개 변수가 추가로 필요하지 않다.\n윈도우 함수는 from 절의 테이블에서 where, group by 그리고 having 절로 필터링된 \u0026quot;가상 테이블\u0026quot;의 행을 대상으로 작동하기에 조건에 부합하지 않아 제거된 row는 윈도우 함수 내에서 사용되지 않는다. 쿼리에 다양한 over 절을 사용하여 데이터를 분할할 수 있지만, 이 가상 테이블에 정의된 row를 대상으로 동일하게 작동한다. 행의 순서가 중요하지 않은 경우, order by를 생략해도 되는 것처럼, 단일 파티션이 전체 row를 포함하는 경우 partition by를 생략할 수도 있다.\n1-1. Window frame 윈도우 함수에 관한 중요한 개념 중 하나는 window frame이다. window frame이라고 불리는 row의 집합이 파티션 내에 존재한다. 몇몇 윈도우 함수는 전체 파티션이 아닌, window frame의 row에 대해서만 동작한다. 기본적으로 ORDER BY를 사용하면 frame은 시작 행부터 현재 행까지의 정보로만 구성되며, order by 가 생략되면, 기본 frame은 파티션 내의 전체 row로 이루어진다. 다음 sum의 예제를 보면\nSELECT salary, sum(salary) OVER () FROM empsalary; salary | sum --------+------- 5200 | 47100 5000 | 47100 3500 | 47100 4800 | 47100 3900 | 47100 4200 | 47100 4500 | 47100 4800 | 47100 6000 | 47100 5200 | 47100 (10 rows) over 절에 order by가 없기에, window frame은 파티션 전체와 같고, 각 sum은 전체 테이블을 조회하여 일반 집계 함수와 동일한 결과를 가진다. 하지만 order by 가 들어갈 경우 결과가 달라진다. 아래 쿼리는 월급의 최저값 ROW부터 현재 ROW까지 (파티션의)의 합계이다.\nSELECT salary, sum(salary) OVER (ORDER BY salary) FROM empsalary; salary | sum --------+------- 3500 | 3500 3900 | 7400 4200 | 11600 4500 | 16100 4800 | 25700 4800 | 25700 5000 | 30700 5200 | 41100 5200 | 41100 6000 | 47100 (10 rows) 1-2. 제약조건 위도우 함수는 \bSELECT와 ORDER BY 절에서만 허용된다. group by, having, where 절 \b같은 곳에서는 사용이 불가능하다.\n논리적으로 해당 조건들을 모두 조회한 후에 작동하기 때문이다. 그리고 윈도우 함수는 비윈도우집계함수 이후에 실행된다. 즉 윈도우 함수의 인수에 일반 집합 함수 호출을 포함하는 것은 가능하지만, 그 반대의 경우는 불가능하다. 만약 윈도우 함수의 연산 후에 filter 혹은 group by를 할 경우, 서브쿼리를 사용해야 한다. 아래와 같이 사용하면 내부 쿼리의 순위가 3 이하인 row 들만 보여준다.\nSELECT depname, empno, salary, enroll_date FROM (SELECT depname, empno, salary, enroll_date, rank() OVER (PARTITION BY depname ORDER BY salary DESC, empno) AS pos FROM empsalary ) AS ss WHERE pos \u0026lt; 3; 1-3. WINDOW AS 쿼리가 만약에 다수의 윈도우 함수를 포함한다면, 각각이 OVER문으로 작성하는 것이 가능하지만, 여러 함수에 대해 동일한 윈도우 설정 동작을 하는 경우 중복되고 에러가 발생하기 쉽다. 이럴 경우 WINDOW에 해당하는 레퍼런스를 설정하고 해당 값을 over에서 사용 이 가능하다.\nSELECT sum(salary) OVER w, avg(salary) OVER w FROM empsalary WINDOW w AS (PARTITION BY depname ORDER BY salary DESC); 1-4. 성능 윈도우 함수를 사용할 경우 집계, 순위 등의 쿼리를 편하게 사용할 수 있고, 테이블의 스캔 횟수도 훨씬 줄어든다. 다만 파티션 내 다른 행과 현재행의 관계정보로 다루어지기에, 윈도우 함수를 사용할 시 기본적으로 정렬하는 과정에서 자원이 소모된다. 테이블 및 데이터 정보에 따라 달라지겠지만, 분포율이 5~7%정도 되는 1200만 건의 데이터를 기준으로 윈도우 함수와 group by 정렬을 비교해 보았다.\n윈도우함수 집계\n비윈도우함수 집계\n실제로 윈도우 함수를 포함한 경우 sort 과정에 자원이 많이 소모되어 데이터가 많을 경우 오히려 비윈도우 함수보다 효율이 좋지 않았다. 따라서 기능의 편의성 외에도 데이터의 양이나 테이블 구조에 맞춰 윈도우 함수를 사용하고, 서브쿼리나 조건절 튜닝을 통해 스캔해야할 행의 갯수를 줄인 후 사용하는 방법을 고려해야 한다.\n2. 윈도우 함수의 종류 및 사용법 2-1. 일반집계함수 SUM - 파티션별 윈도우의 합계 SELECT MGR, ENAME, SAL , SUM(SAL) OVER (PARTITION BY MGR ORDER BY SAL RANGE UNBOUNDED PRECEDING) as MGR_SUM FROM EMP; MAX - 파티션별 윈도우의 최댓값 SELECT MGR, ENAME, SAL , MAX(SAL) OVER (PARTITION BY MGR) as MGR_MAX FROM EMP; MIN - 파티션별 윈도우의 최솟값 SELECT MGR, ENAME, HIREDATE, SAL , MIN(SAL) OVER(PARTITION BY MGR ORDER BY HIREDATE) as MGR_MIN FROM EMP; AVG - 파티션별 윈도우의 평균값 SELECT MGR, ENAME, HIREDATE, SAL , ROUND (AVG(SAL) OVER (PARTITION BY MGR ORDER BY HIREDATE ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)) as MGR_AVG FROM EMP; COUNT - 파티션별 윈도우의 카운트 SELECT ENAME, SAL , COUNT(*) OVER (ORDER BY SAL RANGE BETWEEN 50 PRECEDING AND 150 FOLLOWING) as SIM_CNT FROM EMP; 2-2. 그룹 내 행 순서 함수 FIRST_VALUE - 파티션별 윈도우에 가장 먼저 나오는 값 SELECT DEPTNO, ENAME, SAL , FIRST_VALUE(ENAME) OVER (PARTITION BY DEPTNO ORDER BY SAL DESC ROWS UNBOUNDED PRECEDING) as DEPT_RICH FROM EMP; LAST_VALUE - 파티션별 윈도우에 가장 나중에 나오는 값 SELECT DEPTNO, ENAME, SAL , LAST_VALUE(ENAME) OVER ( PARTITION BY DEPTNO ORDER BY SAL DESC ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) as DEPT_POOR FROM EMP; LAG - 파티션별 윈도우의 이전 몇 번째 행의 값 SELECT ENAME, HIREDATE, SAL , LAG(SAL) OVER (ORDER BY HIREDATE) as PREV_SAL FROM EMP WHERE JOB = \u0026#39;SALESMAN\u0026#39;; LEAD - 파티션별 윈도우의 이후 몇번째 행의 값 SELECT ENAME, HIREDATE , LEAD(HIREDATE, 1) OVER (ORDER BY HIREDATE) as \u0026#34;NEXTHIRED\u0026#34; FROM EMP; 2-3. 그룹 내 순위함수 RANK - 파티션 내 전체 윈도우에 대한 순위, 동일 값에 대해서는 동일한 순위, 그 다음 값은 순위는 동일한 순위 만큼 증가된 채로 부여 (ex. 1,1,1,4,5,6,7...) SELECT JOB, ENAME, SAL, RANK( ) OVER (ORDER BY SAL DESC) ALL_RANK, RANK( ) OVER (PARTITION BY JOB ORDER BY SAL DESC) JOB_RANK FROM EMP; DENSE_RANK - 파티션 내 전체 윈도우에 대한 순위, 동일 값에 대해서는 동일한 순위, 그 다음 값은 순위는 동일한 순위에 상관없이 다음값 부여 (ex. 1,1,1,2,3,4,5...) SELECT JOB, ENAME, SAL , RANK( ) OVER (ORDER BY SAL DESC) RANK , DENSE_RANK( ) OVER (ORDER BY SAL DESC) DENSE_RANK FROM EMP; ROW_NUMBER - 파티션 내 전체 윈도우에 대한 순번, 동일한 값이어도 고유한 순위 부여 SELECT JOB, ENAME, SAL , RANK( ) OVER (ORDER BY SAL DESC) RANK , ROW_NUMBER() OVER (ORDER BY SAL DESC) ROW_NUMBER FROM EMP; 2-4. 그룹 내 비율 함수 RATIO_TO_REPORT - 파티션 내 전체 SUM에 대한 컬럼별 백분율 소수점 값 SELECT ENAME, SAL , ROUND(RATIO_TO_REPORT(SAL) OVER (), 2) as R_R FROM EMP WHERE JOB = \u0026#39;SALESMAN\u0026#39;; PERCENT_RANK - 파티션별 윈도우에서 가장 먼저 나오는 것은 0, 제일 마지막에 나오는 것은 1로 나타낸 후 값에 상관없이 행의 순서만으로의 백분율 값 SELECT DEPTNO, ENAME, SAL , PERCENT_RANK() OVER (PARTITION BY DEPTNO ORDER BY SAL DESC) as P_R FROM EMP; CUME_DIST - 파티션별 윈도우의 전체 건수에서 현재 행보다 작거나 같은 건에 대한 누적 백분률 값 SELECT DEPTNO, ENAME, SAL , CUME_DIST() OVER (PARTITION BY DEPTNO ORDER BY SAL DESC) as CUME_DIST FROM EMP; NTITLE - 파티션별 전체 건수를 Argument로 N등분한 값 SELECT ENAME, SAL , NTILE(4) OVER (ORDER BY SAL DESC) as QUAR_TILE FROM EMP ; 참고\n윈도우 함수별 기능 및 예제 - http://www.gurubee.net/lecture/2382\n윈도우 함수 공식 문서 - https://www.postgresql.org/docs/current/tutorial-window.htmlhttps://www.postgresql.org/docs/current/tutorial-window.html ","permalink":"http://localhost:50666/posts/40/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/40/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"윈도우-함수-window-functions란\" ke-size=\"size26\"\u003e1. 윈도우 함수 (Window Functions)란?\u003c/h2\u003e\n\u003cp\u003e윈도우 함수는 행과 행 간의 관계를 쉽게 정의하기 위해 만든 함수이다. 이 기능은 일반 집계함수의 연산과 유사하지만, 일반 집계함수가 행 각각을 단일 그룹화해서 출력하는 반면에, 윈도우 함수는 각각의 행들이 \u003cstrong\u003e그룹화되지 않으며 별도의 ID\u003c/strong\u003e를 가진다. 그렇기에 윈도우 함수는 현재 row의 정보보다 더 많은 정보에 접근이 가능하다. 예를 들면 다음과 같다.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e일반집계함수 :\u003c/strong\u003e COUNT() + GROUP BY-\u0026gt; 그룹별 1개의 행 출력 (그룹 개수만큼 출력, 자르기 + 집약)\u003cbr\u003e\n\u003cstrong\u003e윈도우집계함수 :\u003c/strong\u003e COUNT() OVER (PARTITION BY) -\u0026gt; ID개수만큼 행 출력 (행의 개수가 줄어들지 않는다, 자르기)\u003c/p\u003e","title":"[PostgreSQL] 윈도우 함수(Window Functions)의 개념, 성능 및 사용법 (over, sum/rank/ntitle/cume_dist 등...)"},{"content":" 1. 데이터베이스 상속(Inheritance)이란? 상속은 객체지향 데이터베이스의 개념 중 하나이다. PostgreSQL은 테이블 생성 시 하나 이상의 다른 테이블로부터의 상속 기능을 제공하며, 이를 잘 활용하면 데이터베이스 설계에 새로운 가능성들을 열어준다. 데이터뿐만 아니라 부모 테이블의 컬럼 속성 및 인덱스 등의 특징들도 자식 테이블로 상속되기에 상황에 따라 효율적인 설계가 가능하다.\n2.데이터베이스 상속(Inherits) 방법 다음 예제는 PostgreSQL 공식 문서의 예제이다.\nCapitals - 이름, 인구, 고도, 요약어를 포함한 수도의 정보가 포함된 테이블\nCities - 이름, 인구, 고도를 포함한 도시 정보가 포함된 테이블\n수도는 도시에 포함되기에, 전체 도시 리스트를 보여줄 때, 수도 리스트를 동시에 보여주는 상황이 있을 것이다. 이 경우 상속을 사용하지 않고는 보통 이런 식으로 스키마를 작성한다.\nCREATE TABLE capitals ( name text, population real, elevation int, -- (in ft) state char(2) ); CREATE TABLE non_capitals ( name text, population real, elevation int -- (in ft) ); CREATE VIEW cities AS SELECT name, population, elevation FROM capitals UNION SELECT name, population, elevation FROM non_capitals; 수도(capitals)와 도시(cities) 리스트를 중복 없이 보여주는 기능은 잘 작동하겠지만, 양쪽에 중복된 데이터가 있을 경우, 데이터 변경 시 각각 테이블의 행을 업데이트해야 하기에 효율적이지 못하다. 이 같은 경우 다음과 같이 상속을 사용하면 더 효율적이다.\nCREATE TABLE cities ( name text, population real, elevation int -- (in ft) ); CREATE TABLE capitals ( state char(2) UNIQUE NOT NULL ) INHERITS (cities); 수도(capitals) 테이블의 모든 row는 부모테이블인 도시(cities)의 모든 컬럼들 (name, population, elevation)을 상속받는다.\n예를 들어 다음 쿼리는 500피트 이상의 고도에 존재하는 수도를 포함한 모든 도시의 이름을 찾을 때 Cities 테이블로만 조회가 가능하다.\nSELECT name, elevation FROM cities WHERE elevation \u0026gt; 500; 결과는 name | elevation -----------+----------- Las Vegas | 2174 Mariposa | 1953 Madison | 845 (3 rows) 3. 상속 제외 (Only) 상속받은 테이블에서 특정 테이블의 결과를 제외하고 싶을 때 어떻게 하면 될까? 다음 쿼리는 수도가 아닌 도시 중에서 500 피드 이상의 고도를 가진 도시를 추출하는 쿼리이다.\nSELECT name, elevation FROM ONLY cities WHERE elevation \u0026gt; 500; name | elevation -----------+----------- Las Vegas | 2174 Mariposa | 1953 (2 rows) 테이블 명 앞에 ONLY를 추가하면, 상속계층 상의 다른 테이블이 아닌 해당 테이블에서만 조회를 한다.\nSELECT, UPDATE, DELETE 모두 ONLY 구문을 지원한다.\n4. 테이블 상속의 특징 부모테이블에 대한 조회/수정/삭제는 자식 테이블을 포함하여 동작된다. 상속받은 테이블을 상속받는 또 다른 테이블이 생성될 때, 부모 테이블의 수정은 전계층 상속테이블에 영향을 준다. 부모테이블의 인덱스를 생성 시 자식 테이블에도 동일하게 생성된다. create index cities_test_name_index on cities_test (name); 자식테이블을 타깃으로 부모테이블 컬럼에 인덱스도 생성 가능하다. create index cities_test_population_index on capitals_test (population); 5. 적용 후 성능비교 상속을 받아 테이블을 생성 시, 테이블 자체의 속성을 상속받을 뿐 조회를 위해서는 양쪽 테이블을 다 스캔해야 하기에 성능 자체가 급격하게 증가하진 않는 것으로 예상하였다. 실제로 대량 데이터(500만 건 이상의 테이블) 2개를 union 하여 사용하고 있는 경우가 있어, 상속을 통해 성능비교를 해보았다.\n5-1. 조건 없이 \b조회, 카운트 a. Union (table_a, table_b)\nb. inherit 테이블 (table_parent, table_child1)\n기존 Union은 각 테이블에 seq_scan 후 정렬하고 상속 테이블은 부모, 자식 각 테이블 seq_scan 후 바로 완료한다.\n-\u0026gt; 상속 테이블을 사용 시 정렬 및 중복여부 판단 등에 대한 cost가 생략되기에 효과적이다.\n5-2. 조건을 넣고 조회 시 둘 다 인덱스가 적용/미적용된 상태의 검색을 할 경우, 2개의 테이블을 각각 조회하는 코스트가 소모되며 현재 테스트 데이터에는 중복되는 값들이 극소량이라 차이가 미비하지만, 중복되는 값이 많거나 정렬이 고려되어야 하는 쿼리를 사용 시 상속 테이블 사용 시의 성능이 더 나을 것으로 보인다. 데이터의 분포 및 구조에 따라 성능은 달라질 수 있고, 단순한 조회의 성능이 아니라 실제 데이터의 유지 보수 및 운영상황에 따라 상황에 맞게 상속 테이블을 적용하면 효율적인 데이터베이스 설계가 가능할 것으로 보인다. * 상속 테이블 적용 후 테이블의 성능 지표를 테스트하려면 아래 포스트의 쿼리 플랜 분석법을 확인하면 된다.\n[PostgreSQL] 쿼리 성능향상 (실행계획 보는 법, 상세 확인방법, Explain의 어떤 지표를 봐야 할까?)\n참고\nhttps://www.postgresql.org/docs/current/tutorial-inheritance.html ","permalink":"http://localhost:50666/posts/39/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/39/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"데이터베이스-상속inheritance이란\" ke-size=\"size26\"\u003e1. 데이터베이스 상속(Inheritance)이란?\u003c/h2\u003e\n\u003cp\u003e상속은 객체지향 데이터베이스의 개념 중 하나이다. PostgreSQL은 테이블 생성 시 하나 이상의 다른 테이블로부터의 상속 기능을 제공하며, 이를 잘 활용하면 데이터베이스 설계에 새로운 가능성들을 열어준다. 데이터뿐만 아니라 부모 테이블의 컬럼 속성 및 인덱스 등의 특징들도 자식 테이블로 상속되기에 상황에 따라 효율적인 설계가 가능하다.\u003c/p\u003e\n\u003ch2 id=\"데이터베이스-상속inherits-방법\" ke-size=\"size26\"\u003e2.데이터베이스 상속(Inherits) 방법\u003c/h2\u003e\n\u003cp\u003e다음 예제는 PostgreSQL 공식 문서의 예제이다.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eCapitals -\u003c/strong\u003e 이름, 인구, 고도, 요약어를 포함한 수도의 정보가 포함된 테이블\u003cbr\u003e\n\u003cstrong\u003eCities -\u003c/strong\u003e 이름, 인구, 고도를 포함한 도시 정보가 포함된 테이블\u003c/p\u003e","title":"[PostgreSQL] 데이터베이스 상속(Inheritance)의 개념과 사용법 및 성능비교 (Inherits, Only)"},{"content":" 1. 날짜 형태로 형 변환 데이터 베이스에서 날짜형태로 형 변환을 하는 것은 다음과 같은 방법으로 쉽게 가능하다.\n-- Unix타임(int)형 변환 SELECT to_timestamp(1658792421) -- varchar 타입 변환 SELECT to_timestamp(\u0026#39;20231026\u0026#39;,\u0026#39;yyyymmdd\u0026#39;) -- 날짜형을 char로 변환 SELECT to_char(to_timestamp(1658792421), \u0026#39;DD-MM-YYYY\u0026#39;) 2. 유효한 날짜형태 검증 데이터 정제가 완료되지 않아 조회하려는 데이터에 날짜유형에서 벗어난 데이터 ('20231301',202301', '20231232' 등)가 하나라도 있을 경우 조회 자체가 안된다. 그럴 경우 날짜 규격에 맞지 않는 데이터를 보정 후 연산해야 하는 경우가 있는데 단순 월별 케이스문으로 분리하여 날짜 유형에 어긋나는 경우를 찾을 수도 있지만 row마다 날짜 유형이 다르거나 윤달을 체크할 수 없다.\n그래서 날짜 형태자체를 변환시도하고 성공 여부에 따라 결과값을 추출하는 함수를 생성해야 한다.\nCREATE OR REPLACE FUNCTION VALIDATE_DATE(S VARCHAR) RETURNS INT AS $$ BEGIN IF COALESCE(S, \u0026#39;-\u0026#39;) = \u0026#39;-\u0026#39; THEN RETURN -1; END IF; PERFORM S::DATE; RETURN 0; EXCEPTION WHEN OTHERS THEN RETURN 1; END; $$ LANGUAGE PLPGSQL; 이제 VALIDATE_DATE() 함수를 실행시키면, 날짜유형, 윤달에 상관없이 해당 데이터가 유효한 날짜라면 0, 유효하지 않은 데이터라면 1, null이라면 -1을 리턴하게 된다. 다음과 같이 유효한 날짜 유형의 데이터만 조회하거나\nSELECT date_column from table WHERE VALIDATE_DATE(date_column) = 0; 날짜 유형에 어긋나는 데이터들을 일괄 null 업데이트하여 처리할 수 있다.\nUPDATE table SET date_column = NULL WHERE VALIDATE_DATE(date_column) = 1 ","permalink":"http://localhost:50666/posts/38/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/38/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"날짜-형태로-형변환\" ke-size=\"size26\"\u003e1. 날짜 형태로 형 변환\u003c/h2\u003e\n\u003cp\u003e데이터 베이스에서 날짜형태로 형 변환을 하는 것은 다음과 같은 방법으로 쉽게 가능하다.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e-- Unix타임(int)형 변환\nSELECT to_timestamp(1658792421)\n\n-- varchar 타입 변환\nSELECT to_timestamp(\u0026#39;20231026\u0026#39;,\u0026#39;yyyymmdd\u0026#39;)\n\n-- 날짜형을 char로 변환\nSELECT to_char(to_timestamp(1658792421), \u0026#39;DD-MM-YYYY\u0026#39;)\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"2-유효한-날짜형태-검증\"\u003e2. 유효한 날짜형태 검증\u003c/h2\u003e\n\u003cp\u003e데이터 정제가 완료되지 않아 조회하려는 데이터에 날짜유형에서 벗어난 데이터 \n('20231301',202301', '20231232' 등)가 하나라도 있을 경우 조회 자체가 안된다. 그럴 경우 날짜 규격에 맞지 않는 데이터를 보정 후 연산해야 하는 경우가 있는데 단순 월별 케이스문으로 분리하여 날짜 유형에 어긋나는 경우를 찾을 수도 있지만 row마다 날짜 유형이 다르거나 윤달을 체크할 수 없다.\u003c/p\u003e","title":"[PostgreSQL] 날짜 형태 검증하기 (ERROR: date/time field value out of range)"},{"content":" 1. 가상 스레드 (Virtual Threads)란? 2023.09.20 릴리즈 된 자바 21에 추가된 가상 스레드(Virtual Threads)라는 기능을 살펴보자.\n가상 스레드는 경량 스레드로, 높은 처리량의 동시 어플리케이션을 작성, 유지 및 관찰하는 작업 공수를 크게 줄인다\u001d. OS스레드를 그대로 사용하지 않고 JVM 자체적으로 스케쥴링을 통해 사용할 수 있는 경량 스레드이며, 하나의 프로세스가 수십 - 수백만 스레드를 동시에 실행할수 있도록 설계되었다.\n2. 자바의 전통적인 스레드 자바 개발자들은 근 30년동안 동시성 서버 어플리케이션의 처리를 위해 스레드에 의존해왔다. 모든 메서드의 구문들은 스레드 내부에서 실행되며, 1개의 요청을 1개의 스레드가 처리한다. 대표적으로 스프링은 멀티스레드 구조이기에, 여러 스레드의 실행이 동시에 발생하며 동시 요청이 많아질수록 스레드의 수 역시 증가한다. 각각의 스레드는 지역 변수를 저장하고 메소드 호출을하는 스택을 제공하며, 문제가 생겼을 때의 Context도 제공하는데, 예를들어 Exception은 동일 스레드 내에서의 메소드에 의해 throw/catch 된다. 그렇기 때문에 개발자는 스레드의 Stack trace로 문제를 추적할 수 있는 것이고, 그 외 Debugger (스레드의 메소드 내에서 구문을 차례로 훑어본다), Profiler(JFR) (여러 스레드의 행동을 시각화하여 스레드의 성능을 이해할 수 있도록 도와준다.)도 모두 스레드 기반으로 되어있다.\n2-1. 전통적인 스레드의 한계점 기존 JDK의 스레드는 OS 스레드를 Wrapping한 것으로 사용가능한 스레드의 수가 하드웨어 수준보다 훨씬 적게 제한되어있다. OS 스레드는 생성, 유지 비용이 높고 갯수가 제한적이라 요청량에 비례하여 무한정 늘릴수 없다. 어플리케이션 코드가 플렛폼 스레드를 사용하면 실제로는 OS스레드를 사용하는 것이며, 이 스레드는 비용이 비싸기 때문에 스레드 풀을 사용하여 접근하는 방식으로 사용한다. Spring 같은 어플리케이션의 기본 처리방식은 Thread-per-request이다. [Thread-per-request]\n서버 어플리케이션은 일반적으로 서로 독립적인 유저의 동시 요청들을 처리하기에, 어플리케이션이 전체 요청기간 동안 스레드를 전담하여 요청을 처리해야한다. 이러한 Thread-per-request 스타일은 플랫폼의 동시성 단위가 곧 어플리케이션의 동시성 단위이기 때문에 이해하기 쉽고, 개발 및 디버그, 프로파일링 하기 쉽다. Thread-per-request 방식은 요청을 처리하는 스레드에서 I/O 작업시 Blocking이 발생한다. Blocking 발생시 스레드는 I/O 작업 종료시까지 대기해야하기에 많은 요청을 처리해야 하는 상황이라면 Blocking으로 발생하는 낭비를 줄여야한다. [Reactive Programming]\n- Blocking 방식으로 발생하는 낭비를 줄이기 위해 발전하게된 처리량을 높이기 위한 방법, 비동기 방식 프로그래밍이다. - Non-blocking 방식으로 변경하면서 Blocking을 대기하는데 소요된 자원을 다른 요청에서 사용할 수 있다.\n기존 자바 프로그래밍은 스레드를 기반으로하기에 라이브러리들이 모두 Reactive Programming 방식에 맞게 새로 작성되어야하는 문제가 있다. 3. 가상 스레드의 작동방식 가상 스레드는 OS를 Wrapping한 구조가 아니기에 스레드 풀 없이 사용 가능하고, JVM 자체적으로 OS스레드와 연결하는 스케쥴링을 처리하기에 기존 스타일로 코드를 작성하더라도 내부의 가상 스레드가 효율적인 방법으로 스케줄링 해준다. (가상 스레드를 사용하면 Non-blocking에 대한 처리를 JVM단에서 처리해준다.) 4. 목표 공식 문서에 따르면 가상 스레드의 목표와 목표가 아닌 것 (Goals / Non-Goals)을 확인할 수 있다.\n목표 (Goals)\n기존의 Thread-per-request (요청당 처리) 방식으로 작성된 서버 어플리케이션을 near-optimal(최적화) 하드웨어 사용으로 확장 가능해야한다. java.lang.Thread API를 사용하는 기존 자바 코드를 최소한의 수정으로 가상 스레드를 채택 가능해야 한다. 기존 JDK 툴들을 사용하여 가상 스레드의 쉬운 트러블 슈팅, 디버깅 및 프로파일링이 가능해야 한다. 목표가 아닌 것 (Non-Goals)\n기존 thread의 사용을 제거하는 것이나 기존 어플리케이션이 가상 스레드를 사용하기 위해 은밀하게 마이그레이션 하는 것이 아니다. 자바의 기본 동시성 모델을 바꾸는 것이 아니다. 자바 언어나 자바 라이브러리에 새로운 데이터 병렬구조를 제공하려는 것이 목표가 아니다. Stream API는 큰 데이터를 병렬로 처리하는데 여전히 선호되는 방법이다. 가상 스레드는 자바의 기본 동시성 모델을 바꾸거나, 새로운 데이터 흐름의 병렬 구조를 제시하는 것이 아닌 기존 자바 코드를 최소한으로 수정하는 선에서 동시성을 제어하는 어플리케이션이 기존 Thread-per-request 방식 외에 가상 스레드 풀 없이 Reactive Programming이 추구하는 Non-blocking의 효율적인 자원사용을 지원하는데 목표를 두고 있다. 참고\nhttps://mangkyu.tistory.com/309\nhttps://findstar.pe.kr/2023/04/17/java-virtual-threads-1/\nhttps://openjdk.org/jeps/444https://openjdk.org/jeps/444\nhttps://findstar.pe.kr/2023/04/17/java-virtual-threads-1/\n","permalink":"http://localhost:50666/posts/37/","summary":"\u003chr\u003e\n\u003ch2 id=\"가상-스레드-virtual-threads란\" ke-size=\"size26\"\u003e\u003cstrong\u003e1. 가상 스레드 (Virtual Threads)\u003cstrong\u003e란?\u003c/strong\u003e\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e2023.09.20 릴리즈 된 자바 21에 추가된 가상 스레드(Virtual Threads)라는 기능을 살펴보자.\u003c/p\u003e\n\u003cp\u003e가상 스레드는 경량 스레드로, 높은 처리량의 동시 어플리케이션을 작성, 유지 및 관찰하는 작업 공수를 크게 줄인다\u001d.  OS스레드를 그대로 사용하지 않고 JVM 자체적으로 스케쥴링을 통해 사용할 수 있는 경량 스레드이며, 하나의 프로세스가 수십 - 수백만 스레드를 동시에 실행할수 있도록 설계되었다.\u003c/p\u003e\n\u003ch2 id=\"자바의-전통적인-스레드\" ke-size=\"size26\"\u003e\u003cstrong\u003e2. 자바의 전통적인 스레드\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e자바 개발자들은 근 30년동안 동시성 서버 어플리케이션의 처리를 위해 스레드에 의존해왔다. 모든 메서드의 구문들은 스레드 내부에서 실행되며, 1개의 요청을 1개의 스레드가 처리한다. 대표적으로 스프링은 멀티스레드 구조이기에, 여러 스레드의 실행이 동시에 발생하며 동시 요청이 많아질수록 스레드의 수 역시 증가한다. 각각의 스레드는 지역 변수를 저장하고 메소드 호출을하는 스택을 제공하며, 문제가 생겼을 때의 Context도 제공하는데, 예를들어 Exception은 동일 스레드 내에서의 메소드에 의해 throw/catch 된다. 그렇기 때문에 개발자는 스레드의 Stack trace로 문제를 추적할 수 있는 것이고, 그 외 Debugger (스레드의 메소드 내에서 구문을 차례로 훑어본다), Profiler(JFR) (여러 스레드의 행동을 시각화하여 스레드의 성능을 이해할 수 있도록 도와준다.)도 모두 스레드 기반으로 되어있다.\u003c/p\u003e","title":"[Java] 가상 스레드 (Virtual Threads)란? 자바 21의 가상스레드 (Virtual Thread) 도입"},{"content":" 1. Switch와 if-else 조건에 따라 실행을 분기해야 할 때 우리는 조건문을 사용한다. Java에서는 switch / if-else 두 조건문을 선택적으로 사용 가능하다. 보통 가독성을 기준으로 선택을 많이 하나, 효율성 기준에서 어떤 것을 선택하는 것이 좋을지 비교해보려 한다. 일단 switch 구분에서 Strings를 사용하는 것에 관한 공식문서를 보면, The switch statement compares the String object in its expression with the expressions associated with each case label as if it were using the String.equals method; consequently, the comparison of String objects in switch statements is case sensitive. The Java compiler generates generally more efficient bytecode from switch statements that use String objects than from chained if-then-else statements.\n번역 스위치 문은 각 조건에 있는 String 개체를 String.equals 메서드를 사용하는 것과 같은 방식으로 연관된 조건과 비교한다. 그렇기에 스위치문에서의 String 개체를 비교하는 것은 대소문자를 구분한다. Java 컴파일러는 일반적으로 체인 if-then-else 구문보다 스위치 문에서 더 효율적인 바이트 코드를 생성한다.\n2. Switch 자세한 내용을 확인하기 위해 Switch 구문의 작동원리에 대해 좀 더 자세히 들여다보면, Switch 구문은 컴파일할 때 table switch 혹은 lookup switch 방식을 사용한다.\n2-1. Table switch 스위치의 각 케이스 구문들이 효율적인 (각 스위치의 target 간격 들로 구성된) 테이블의 인덱스로 표현될 수 있을 때 사용된다. Switch 구분의 표현식이 유효한 인덱스 범위를 넘어갈 때에는 스위치의 default 타겟을 사용한다. 테이블을 생성할 충분한 공간이 필요하다.\n[table switch 예제]\nint chooseNear(int i) { switch (i) { case 0: return 0; case 1: return 1; case 2: return 2; default: return -1; } } 를 컴파일하게 되면 다음과 같다.\nMethod int chooseNear(int) 0 iload_1 // Push local variable 1 (argument i) 1 tableswitch 0 to 2: // tableswitch의 유효 색인은 0~2 0: 28 // If i is 0, 28로 이동 1: 30 // If i is 1, 30으로 이동 2: 32 // If i is 2, 32로 이동 default:34 // 범위를 벗어날경우 default 타겟 = 34 로 이동 28 iconst_0 // i was 0; - push int constant 0... 29 ireturn // ...and return it 30 iconst_1 // i was 1; push int constant 1... 31 ireturn // ...and return it 32 iconst_2 // i was 2; push int constant 2... 33 ireturn // ...and return it 34 iconst_m1 // otherwise push int constant -1... 35 ireturn // ...and return it Method 1을 보면, table switch에 유효 인덱스로 0~2가 각각 부여되어 있고, 인덱스 내에서 조회될 경우 해당 target으로, 아닐 경우 default로 설정된 target으로 이동하여 이어서 진행한다. 예를 들어 i가 0일 경우, 인덱스 0에 28로 저장되어 있기에 target = 28로 이동하게 된다. 차례로 일치여부를 확인하며 진행할 필요 없이, 일치하는 target으로 바로 점프하여 이동한다. 다만, 스위치 구분의 cases가 많지 않을 경우, table switch의 테이블 표현 방식은 공간적인 이유로 비효율적이게 된다. 예를 들어 케이스가 -100, 0, 100 일경우 201개의 간격을 만들어놓고 실제 타겟과 매핑된 간격은 3개뿐이라면 공간적 비효율성을 가지게 된다.\n2-2. Lookup switch 이 경우, lookup switch 방식이 대신 사용 될 수 있다. lookup switch 구문은 int형의 key(각 case labels의 값)과 각 타겟들을 테이블에 짝지어 저장한다. lookup switch 방식이 실행되면, 스위치 구문의 expression의 값은 해당 테이블의 key들과 각각 비교된다. 매칭되는 키가 있으면 연결된 타겟으로 이동하여 이어서 진행하게 된다. 키가 발견되지 않는다면, default target로 계속된다.\n[lookup switch 예제]\nint chooseFar(int i) { switch (i) { case -100: return -1; case 0: return 0; case 100: return 1; default: return -1; } } 를 컴파일하게 되면 다음과 같다.\nMethod int chooseFar(int) 0 iload_1 1 lookupswitch 3: -100: 36 0: 38 100: 40 default: 42 36 iconst_m1 37 ireturn 38 iconst_0 39 ireturn 40 iconst_1 41 ireturn 42 iconst_m1 43 ireturn JVM은 lookup switch 테이블이 key를 정렬된 채로 저장하게 하기에 선형 스캔보다 효율적인 검색을 할 수 있다. 그렇더라도, lookup switch는 table switch처럼 단순히 범위 체크와 인덱스를 테이블에 넣는 것과는 다르게 키와 정확히 일치하는 항목을 찾아야 한다 그래서 tableswitch는 공간적인 고려가 충분히 된 상황에서는 lookup switch 보다 효율적일 수 있다 JVM의 table switch와 lookup switch는 int 형에서만 동작한다. byte, char, short 형도 내부적으로는 int로 승격되기 때문에, 해당 타입의 조건도 int 형으로 취급받으며 컴파일된다. 예제의 chooseNear 메서드가 short 타입을 operator로 받아도, JVM은 int 형과 같은 명령을 생성한다. 다른 숫자 타입은 switch를 사용하기 위해선 int형태로 범위를 줄여줘야 한다. 실제로 switch 문 내에 float, long 등을 넣으면 컴파일 에러가 난다. String의 경우 int형태의 hashcode를 변환하여 사용한다. 3. If if는 각 조건문마다 순차적으로 확인하며, 부합하는 조건이 나올 경우 멈춘다. 모든 조건을 각각 확인해야 하기에 조건 개수만큼의 연산이 필요한 선형적 연산 (O(N))에 가깝고 비교 로직을 끝까지 수행해야 하며, 10개의 if-else 구문 중 마지막 조건에 걸리는 값이더라도 항상 9번의 조건을 모두 탐색해야 한다.\n4. Switch와 If-else 중에 어떤 걸 사용해야 할까? If-else : 만족하는 조건이 나올 때까지 탐색하며 조건이 늘어날수록 연산량이 늘어난다. 모든 조건을 각각 확인하기에 O(N)이라고 할 수 있다. Switch : 경우 테이블에서 적절 조건을 조회하여 거의 바로 점프하여 조회가 되기 때문에 O(1)에 가깝다고 할 수 있다. (점프 테이블을 구성하는 오버헤드 발생) 다만 조건에 부합하는 확률을 고려하여 순서를 정한다면 효율적인 로직 작성 가능하고 이 경우에는 switch 구문보다 효율적일 수 있다.\n다음 예제에서 if와 switch 구문이 각각 효율적인 상황을 확인해 보자\n// 다음 배열은 대부분의 연산이 첫번쨰 조건에서 마무리되기에 switch 구문보다 효율적이다. int[] temp = {-1,0,100,100,100,...,100,100}; for (int i = 0; i\u0026lt;temp.length; i++) { if (temp[i] == 100) { return \u0026#34;H\u0026#34;; } else if (temp[i] == 0){ return \u0026#34;Z\u0026#34;; } else if (temp[i] == 1){ return \u0026#34;O\u0026#34;; } else if (temp[i] == 2){ return \u0026#34;T\u0026#34;; } else if (temp[i] == 3){ return \u0026#34;TH\u0026#34;; } else { return \u0026#34;M1\u0026#34;; } } // 다음 배열은 이전 조건문에서 맨마지막 조건에 대부분 걸리기에 switch문이 효율적이다. int[] temp = {-1,-1,-1,-1,-1,...,-1,0,100}; for (int i = 0; i\u0026lt;temp.length; i++) { while (temp[i]) { case 100: return \u0026#34;H\u0026#34;; case 0: return \u0026#34;Z\u0026#34;; case 1: return \u0026#34;O\u0026#34;; case 2: return \u0026#34;T\u0026#34;; case 3: return \u0026#34;TH\u0026#34;; default: return \u0026#34;M1\u0026#34;; } } 다만, 다음 예제와 같은 극단적으로 구현하는 경우는 개발환경에서 잘 나타나지 않고, 조건별 확률이 비슷하거나 예측이 안 되는 경우가 많은데, 이럴 경우 switch 구문이 효율적이다. 하지만, if-else문, switch 문에 조건이 굉장히 많아지는 케이스는 잘 없기에 체감이 될 정도의 차이나 문제가 발생하진 않으며, 단순 성능 외에 가독성 등을 잘 고려하여 선택적으로 사용하여야 한다. 참고\nhttps://docs.oracle.com/javase/specs/jvms/se7/html/jvms-3.html#jvms-3.10\nhttps://docs.oracle.com/javase/7/docs/technotes/guides/language/strings-switch.html\n","permalink":"http://localhost:50666/posts/36/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/36/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"switch와-if-else\" ke-size=\"size26\"\u003e1. Switch와 if-else\u003c/h2\u003e\n\u003cp\u003e조건에 따라 실행을 분기해야 할 때 우리는 조건문을 사용한다. Java에서는 switch / if-else 두 조건문을 선택적으로 사용 가능하다. 보통 가독성을 기준으로 선택을 많이 하나, 효율성 기준에서 어떤 것을 선택하는 것이 좋을지 비교해보려 한다.\n \u003c/p\u003e\n\u003cp\u003e일단 switch 구분에서 Strings를 사용하는 것에 관한 공식문서를 보면,\n \u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe \u003cstrong\u003eswitch\u003c/strong\u003e statement compares the \u003cstrong\u003eString\u003c/strong\u003e object in its expression with the expressions associated with each \u003cstrong\u003ecase\u003c/strong\u003e label as if it were using the \u003cstrong\u003eString.equals\u003c/strong\u003e method; consequently, the comparison of \u003cstrong\u003eString\u003c/strong\u003e objects in \u003cstrong\u003eswitch\u003c/strong\u003e statements is case sensitive. The Java compiler generates generally more efficient bytecode from \u003cstrong\u003eswitch\u003c/strong\u003e statements that use \u003cstrong\u003eString\u003c/strong\u003e objects than from chained \u003cstrong\u003eif-then-else\u003c/strong\u003e statements.\u003c/p\u003e","title":"[Java] Switch와 else-if의 효율성 비교 (Switch와 else-if 중에 어떤 걸 사용해야 할까?)"},{"content":" 1. 클래스 로딩 과정 (Java Class Loading Process) 이란? 자바 클래스 로딩 과정 (Java Class Loading Process)은 클래스 로더가 클래스 파일을 찾아 동적으로 JVM의 메모리 영역인 Runtime Data Areas에 올려놓는 과정을 말한다. 자바에서 객체가 어떻게 형성/관리 되는지 이해하려면. java 파일의 소스코드가 어떻게 JVM 위에 로딩되는지 아는 것이 중요하고, 클래스 로딩할 때 발생하는 문제 (\u0026lsquo;java.lang.ClassNotFoundException'과 같은 에러)를 쉽게 해결하고, 코드상 동적으로 클래스 로딩하는 구문 이해하는데 필요하다.\n2. 클래스 로더 3단계 과정 클래스 로더는 다음 3단계 과정을 거쳐 클래스 파일을 로딩한다.\n1. Loading\n클래스 파일을 읽어 바이너리 코드로 만들고 JVM의 메모리에 로드\n2. Linking\n클래스파일 사용을 위한 검증\n코드 내부의 러퍼런스를 연결\n3. Initialization\nStatic 변수들의 초기화 및 값 할당 3. 클래스 로드 시점 Loading 시점에서 JVM은 실행 시점에 모든 클래스를 메모리에 올려놓지 않고 필요한 클래스를 동적으로 메모리에 적재하여 효율적으로 관리한다.\n1. 인스턴스 생성시\n2. Static 변수 할당\n3. Static 메서드 호출\n4. Static final 상수 호출 (Static 변수, 메소드 호출과 다르게 Outer 클래스가 로딩되지 않는다, JVM의 Method Area에 Constant Pool에 따로 저장되어 관리되기 때문이다.)\n4. 클래스 로더의 종류 JVM을 실행했을 때 각 클래스 로더들은 자신이 호출할 수 있는 클래스들을 호출하여 JVM에 로딩하게 된다.\n1. 부트스트랩 클래스 로더\n$JAVA_HOME/jre/lib/rt.jar 에서 rt.jar에 있는 JVM을 실행시키기 위한 최소한의 핵심 클래스들을 로딩한다.\n-verbose:class JVM 옵션을 주고 자바 애플리케이션을 실행시키면 rt.jar에 있는 파일 로딩되는 것을 확인할 수 있다.\nJava 9 이후로는 rt.jar 등이 없어짐에 따라 로딩할 수 있는 클래스의 범위가 축소되어 정확하게 ClassLoader 내 최상위 클래스들만 로드한다.\n2. 확장 클래스로더\n$JAVA_HOME/jre/lib/ext 경로의 자바 확장 클래스들을 로딩한다\n3. 애플리케이션 클래스 로더\n자바 프로그램 실행 시$CLASSPATH에 설정된 경로의 클래스들을 로딩하게 된다. 이 시점에 개발된. class파일들이 로딩된다.\n클래스 로더들은 계층 구조를 가지도록 생성이 가능하고 각 부모 클래스 로더에서 자식클래스 로더를 가지는 형태로 클래스 로더를 만들 수 있다. 5. 클래스 로더의 작동원칙 위임 원칙 (Delegate Load Request)\nSystem Loader가 A라는 클래스를 로딩할 때 그 요청은 부모로더들로 거슬러 올라가 부트스트랩 로더에 도착한 후 그 밑으로 로딩 요청을 수행한다. 최상위 클래스 로더에 요청을 위임한 후, 파일을 찾으며 자식 클래스 로더에게 넘기며, 클래스로더 중 하나라도 파일 찾는 데 성공하면 자식 로더에게 넘겨준다. 가시성 제약 조건(Have Visibility Constraint)\n부모 로더에서 찾지 못한 클래스는 자식 로더로 찾지 못하고, 자식로더가 찾지 못한 것은 부모로더에 위임해서 찾을 수 있다 언로드 불가 (Cannot unload classes)\n클래스 로더로 로딩된 클래스들은 JVM 상에서 없앨 수 없다 유일성 원칙 (Uniqueness Principle)\n하위 클래스 로더가 상위 클래스 로더에서 로드한 클래스를 다시 로드하지 않아야 한다.\n상위 클래스로만 책임을 위임하기에 고유한 클래스를 보장할 수 있게 해주는 원칙이다. 참고\nhttps://co-no.tistory.com/103\nhttps://engkimbs.tistory.com/entry/Java-Java-%ED%81%B4%EB%9E%98%EC%8A%A4-%EB%A1%9C%EB%94%A9-%EA%B3%BC%EC%A0%95Java-Class-Loading-Process\n클래스 로더 시스템의 3단계 동작 과정 / 출처 :https://dailyheumsi.tistory.com/196\n클래스 로더 동작순서 (출처 : https://engkimbs.tistory.com/)\n","permalink":"http://localhost:50666/posts/35/","summary":"\u003chr\u003e\n\u003ch2 id=\"클래스-로딩-과정-java-class-loading-process-이란\" ke-size=\"size26\"\u003e\u003cstrong\u003e1. 클래스 로딩 과정 (Java Class Loading Process) 이란?\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e자바 클래스 로딩 과정 (Java Class Loading Process)은 클래스 로더가 클래스 파일을 찾아 동적으로 JVM의 메모리 영역인 Runtime Data Areas에 올려놓는 과정을 말한다.\n \u003c/p\u003e\n\u003cp\u003e자바에서 객체가 어떻게 형성/관리 되는지 이해하려면. java 파일의 소스코드가 어떻게 JVM 위에 로딩되는지 아는 것이 중요하고, 클래스 로딩할 때 발생하는 문제 (\u0026lsquo;java.lang.ClassNotFoundException'과 같은 에러)를 쉽게 해결하고, 코드상 동적으로 클래스 로딩하는 구문 이해하는데 필요하다.\u003c/p\u003e\n\u003ch2 id=\"클래스-로더-3단계-과정\" ke-size=\"size26\"\u003e\u003cstrong\u003e2. 클래스 로더 3단계 과정\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e클래스 로더는 다음 3단계 과정을 거쳐 클래스 파일을 로딩한다.\u003c/p\u003e","title":"[Java] 클래스 로딩 과정(Java Class Loading Process)이란?"},{"content":" 1. 리눅스 Swap 메모리 설정 (예제는 EC2 환경에서 진행) 1-1. Swap 메모리 확인 swapon -s free -h shared = 하나의 프로세스에서 다른 프로세스의 데이터에 효율적으로 접근하기 위해 사용하는 메모리\nbuff/cache = 버퍼와 캐시를 위해 사용하는 메모리, 커널이 성능향상을 위해 캐시 영역으로 사용되는 메모리\nbuff = 프로세스가 사용하는 메모리 영역이 아닌 시스템 성능향상을 위해 커널이 사용하고 있는 영역\ncache = 캐시영역의 메모리, I/O 작업을 더 빠르게 하기 위해 커널에서 사용\ntotal - buff/cache - free = 사용중인 메모리\ntotal - used - buff/cache = 실제 사용가능한 메모리\n1-2. Swap 메모리 추가 sudo dd if=/dev/zero of=/swapfile bs=1024 count=200000 bs = 포멧 단위 (bs = 1M 로도 사용 가능)\ncount = 블록수, 1kb를 200000번 /dev/zero로 초기화하기에 총 200MB의 공간을 Swap 파일로 포맷한 것으로 메모리 크기를 의미한다. (count=2000000 -\u0026gt; 2GB swap 메모리)\n1-3. mkswap으로 Swap 파일로 포멧 sudo mkswap swapfile sudo chmod 600 /swapfile 1-4. Swap 메모리 활성화 -- 단일 Swap 메모리 on sudo swapon swapfile -- 전체 Swap 메모리 on swapon -a swapon: /swapfile: swapon failed: Device or resource busy\n에러가 뜬다면 sudo swapoff /swapfile로 Swap 비활성화 후 다시 시도해 주면 된다.\nSwap 메모리가 활성된 후에 다시 \u0026quot;1-1. Swap 메모리 확인\u0026quot;을 해보면, 설정값만큼의 Swap 메모리가 활성화된 것을 확인할 수 있다. 1-5. 시스템 재시작 시에도 Swap 메모리 활성화 sudo vi /etc/fstab 해당 파일을 열어 맨 아랫줄에\n/sawpfile swap swap default 0 0 를 추가하면 된다. 1-6. Swap 메모리 비활성화 -- 단일 Swap 메모리 off sudo swapoff swapfile -- 전체 Swap 메모리 off swapoff -a 1-7. Swap 메모리 삭제 sudo rm -r swapfile swap out -\u0026gt; swap in 되면서 실제 물리 메모리로 이동한다. (다소 시간이 걸리는 작업이다.)\n1-8. Swap 영역 초기화 Swap 파티션 초기화\nsudo mkswap /dev/{swap partition} Swap 파일 초기화\nsudo truncate -s 0 /path/to/swapfile sudo chmod 600 /path/to/swapfile sudo mkswap /path/to/swapfile Swap 초기화 시, Swap 영역의 데이터가 모두 삭제된다. Swap 메모리 초기화 시에는 해당 파티션, 해당 파일을 원마운트하고 비워야 한다.\n1-9. Swap 영역 다시 활성화 Swap 파티션 활성화\nsudo swapon /dev/{swap partition} Swap 파일 활성화\nsudo swapon /path/to/swapfile 2. Swap 메모리란? 주 메모리(RAM)가 모두 사용되어 추가 메모리가 필요할 때 디스크 공간을 활용하여 부족한 메모리를 대체할 수 있는 공간이다. 운영 체제는 일부 데이터를 RAM에서 디스크의 Swap 영역(하드디스크의 특정 파티션 혹은 Swap 파일)으로 옮겨 메모리 부족상태를 해결한다. 하드디스크를 사용하는 것이 아니라 속도 측면에선 아주 떨어지지만 시스템 안정성과 성능 유지에 큰 역할을 한다. 실제 메모리보다 큰 프로그램을 실행하거나 동시에 더 많은 프로세스를 실행하는 데 사용된다. 예를 들어 ec2 프리티어의 경우 t2.micro RAM 은 1G 뿐이지만 임시로 swap 메모리를 설정하면, 여러 개 프로세스 띄울 수 있게 된다. 그렇다면 RAM이 낮은 인스턴스를 사용하고 Swap메모리를 사용하는 것이 비용적으로 효율적일까?\nSwap 메모리는 데이터 전송속도가 느리기에 Swap 메모리 초기화 시에는 해당 파티션, 해당 파일을 원마운트하고 비워야 한다. 영역으로 데이터를 옮기는 작업이 발생할 때 성능이 저하된다. 빈번한 Swap이 발생할 경우 성능향상을 위해서 RAM을 추가해야 한다. 참고\nhttps://reakwon.tistory.com/96\nhttps://jw910911.tistory.com/122\nhttps://serverfault.com/questions/688627/swapon-failed-device-or-resource-busy-on-mounted-disk ","permalink":"http://localhost:50666/posts/34/","summary":"\u003chr\u003e\n\u003ch2 id=\"리눅스-swap-메모리-설정-예제는-ec2-환경에서-진행\" ke-size=\"size26\"\u003e1. 리눅스 Swap 메모리 설정 (예제는 EC2 환경에서 진행)\u003c/h2\u003e\n\u003ch3 id=\"swap-메모리-확인\" style=\"color: #333333; text-align: start;\" ke-size=\"size23\"\u003e1-1. Swap 메모리 확인\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eswapon -s \n\nfree -h\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/34/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202023-10-23%20%EC%98%A4%ED%9B%84%203.25.49.png\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eshared =\u003c/strong\u003e 하나의 프로세스에서 다른 프로세스의 데이터에 효율적으로 접근하기 위해 사용하는 메모리\u003cbr\u003e\n\u003cstrong\u003ebuff/cache =\u003c/strong\u003e 버퍼와 캐시를 위해 사용하는 메모리, 커널이 성능향상을 위해 캐시 영역으로 사용되는 메모리\u003cbr\u003e\n\u003cstrong\u003ebuff =\u003c/strong\u003e 프로세스가 사용하는 메모리 영역이 아닌 시스템 성능향상을 위해 커널이 사용하고 있는 영역\u003cbr\u003e\n\u003cstrong\u003ecache =\u003c/strong\u003e 캐시영역의 메모리, I/O 작업을 더 빠르게 하기 위해 커널에서 사용\u003c/p\u003e","title":"[Linux] 스왑 메모리(Swap Memory)의 개념과 적용방법"},{"content":" 다음은 스프링부트 + 마이바티스 프로젝트에 2개 이상의 데이터소스를 연결하는 예제이다. 기존 스프링 데이터베이스 연동이 되어있다고 가정 후 진행되며, 신규 데이터베이스는 구별이 쉽도록 new 데이터베이스로 명칭 한다.\n1. 신규 데이터베이스 접속정보 추가 추가될 데이터베이스의 속성을 application.properties에 추가\n#기존 DB Setting spring.datasource.url={url} spring.datasource.driverClassName={driverClassName} spring.datasource.username={username} spring.datasource.password={password} spring.datasource.schema={schema} #추가될 DB Setting new.datasource.url={url} new.datasource.driverClassName={driverClassName} new.datasource.username={username} new.datasource.password={password} new.datasource.schema={schema} 2. 데이터베이스 연결 빈 속성 추가 기존 데이터베이스 연결 설정을 하는 DatabaseConfig.java 외에 NewDatabaseConfig.java신규로 추가한다.\n기존 DatabaseConfig.java\n@Bean(name=\u0026#34;sqlSessionFactory\u0026#34;) @Primary public SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception { - sessionFactory 설정 - return sessionFactory.getObject(); } @Bean(name=\u0026#34;sqlSessionTemplate\u0026#34;) @Primary public SqlSessionTemplate sqlSessionTemplate(SqlSessionFactory sqlSessionFactory) throws Exception { -- sessionTemplate 설정 return new SqlSessionTemplate(sqlSessionFactory); } @Bean(name=\u0026#34;dataSource\u0026#34;) @Primary public DataSource dataSource() { - 데이터베이스 연결설정 - return new DataSource(); } 신규 추가된 NewDatabaseConfig.java 설정\n@Bean(name=\u0026#34;newSqlSessionFactory\u0026#34;) public SqlSessionFactory newSqlSessionFactory(@Qualifier(\u0026#34;newDataSource\u0026#34;) DataSource newDataSource) throws Exception { - SqlSessionFactory 설정 - return newSessionFactory.getObject(); } @Bean(name=\u0026#34;newSqlSessionTemplate\u0026#34;) public SqlSessionTemplate newSqlSessionTemplate(@Qualifier(\u0026#34;newSqlSessionFactory\u0026#34;) SqlSessionFactory newSqlSessionFactory) throws Exception { - SqlSessionTemplate 설정 - return new SqlSessionTemplate(newSqlSessionFactory); } @Bean(name=\u0026#34;newDataSource\u0026#34;) public DataSource newDataSource() { - 데이터베이스 연결설정 - return new DataSource(); } 다음 예제대로 SqlSessionFactory, SqlSessionTemplate, DataSource 빈만 별도로 등록해 주면 설정한 데이터베이스 주소로 연결이 가능하다. 다만, NewDatabaseConfig.java의 빈 설정을 보면 public SqlSessionTemplate newSqlSessionTemplate(@Qualifier(\u0026#34;newSqlSessionFactory\u0026#34;) SqlSessionFactory newSqlSessionFactory) 다음과 같이 최초 설정에도 SqlSessionTemplate, SqlSessionFactory 빈 등록 시 새로 생성된 SqlSessionFactory, DataSource를 명시해주지 않는다면 기존 설정의 @Primary 설정을 따라 기존사용 중인 데이터베이스 접속 정보를 사용하게 되기에 의존성 주입을 별도로 해주어야 한다. 별도로 명시하지 않을 경우, 최초 빈 등록시 신규 데이터베이스는 컨넥션만 맺은 채로, 신규 데이터베이스 연결 설정도 1번 데이터베이스 연결 설정 빈을 참조하게 된다.\n3. 데이터소스 설정 추가 데이터베이스 연결 빈 설정이 완료되면 등록된 2개의 데이터 소스 중 접속정보를 선택하여 사용 가능하다.\nprivate final SqlSessionTemplate sqlSessionTemplate; private final SqlSessionTemplate newSqlSessionTemplate; public TestRepository(@Qualifier(\u0026#34;newSqlSessionTemplate\u0026#34;) SqlSessionTemplate sqlSessionTemplate, SqlSessionTemplate sqlSessionTemplate) { this.newSqlSessionTemplate = newSqlSessionTemplate; this.sqlSessionTemplate = sqlSessionTemplate; } --기존 데이터베이스 연결 public Integer connectionTest() throws Exception { log.info(newSqlSessionTemplate.getConnection().get~()); -- 접속 정보 확인 return newSqlSessionTemplate.selectOne(\u0026#34;~.connectionTest\u0026#34;); } --신규 데이터베이스 연결 public Integer connectionTest() throws Exception { log.info(sqlSessionTemplate.getConnection().get~()); -- 접속 정보 확인 return sqlSessionTemplate.selectOne(\u0026#34;~.connectionTest\u0026#34;); } 기존 사용중인 데이터베이스 연결 빈 설정값에 @Primary 지정을 해주면 기존에 사용 중이던 SqlSessionTemplate에 어떤 빈을 사용할지 명시하지 않아도 @Primary 지정된 빈 설정을 따라간다. 예제와 같이 newSqlSessionTemplate에 의존성 주입을 해주어야 하지만, sqlSessionTemplate는 디폴트 설정인 기존 데이터베이스 설정을 사용한다. ","permalink":"http://localhost:50666/posts/33/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/33/img.png\"\u003e\u003c/p\u003e\n\u003cp\u003e다음은 스프링부트 + 마이바티스 프로젝트에 2개 이상의 데이터소스를 연결하는 예제이다. \u003c/p\u003e\n\u003cp\u003e기존 스프링 데이터베이스 연동이 되어있다고 가정 후 진행되며, 신규 데이터베이스는 구별이 쉽도록 new 데이터베이스로 명칭 한다.\u003c/p\u003e\n\u003ch2 id=\"신규-데이터베이스-접속정보-추가\" ke-size=\"size26\"\u003e1. 신규 데이터베이스 접속정보 추가\u003c/h2\u003e\n\u003cp\u003e추가될 데이터베이스의  속성을 \u003cstrong\u003eapplication.properties\u003c/strong\u003e에 추가\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e#기존 DB Setting\nspring.datasource.url={url}\nspring.datasource.driverClassName={driverClassName}\nspring.datasource.username={username}\nspring.datasource.password={password}\nspring.datasource.schema={schema}\n\n#추가될 DB Setting\nnew.datasource.url={url}\nnew.datasource.driverClassName={driverClassName}\nnew.datasource.username={username}\nnew.datasource.password={password}\nnew.datasource.schema={schema}\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"데이터베이스-연결-빈-속성-추가\" ke-size=\"size26\"\u003e2. 데이터베이스 연결 빈 속성 추가\u003c/h2\u003e\n\u003cp\u003e기존 데이터베이스 연결 설정을 하는 \u003cstrong\u003eDatabaseConfig.java\u003c/strong\u003e 외에 \u003cstrong\u003eNewDatabaseConfig.java\u003c/strong\u003e신규로 추가한다.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e기존 DatabaseConfig.java\u003c/strong\u003e\u003c/p\u003e","title":"[Spring] 스프링부트 + Mybatis 데이터소스 여러개 연결 (스프링 다중 데이터베이스 연결)"},{"content":" 1. ItemReader란? 스프링 배치의 ItemReader는 다음과 같은 과정을 거쳐 데이터를 처리한다. 대부분의 데이터 형태는 이미 ItemReader로 제공하고 있기에 ItemReader, ItemStream 인터페이스 자체를 구현할 필요는 없다.\nItemReader는 Chunk 기반 트랜잭션을 다루며 Cursor, Paging 가 대표적인 2가지 방식이다.\n2. Cursor, Paging 형식 2-1. Cursor기반 ItemReader JDBC ResultSet의 기본 기능이다. ResultSet이 Open 될 때마다 데이터베이스의 데이터가 반환된다. 데이터베이스와 연결 맺은 후 데이터를 Streaming 방식으로 I/O이다. 현재 행에서Cursor를 유지하며 다음 데이터를 호출하면 Cursor를 한 칸씩 옮기면서 데이터를 가져온다. 하나의 Connection으로 배치가 끝날때까지 사용되기에 Batch가 끝나기 전에 데이터베이스와 애플리케이션의 연결이 먼저 끊어질 수 있어 데이터베이스와 SocketTimeout을 충분한 값으로 설정하여야 한다. 모든 결과를 메모리에 할당 하기 때문에 메모리 사용량이 많아진다. Chunk 사이즈 만큼의 트랜잭션 단위로 데이터를 처리한다. Cursor 기반 ItemReader 구현체\nJdbcCursorItemReader HibernateCursorItemReader StoredProcedureItemReader MybatisCursorItemReader 2-2. Paging기반 ItemReader Chunk로 데이터베이스에서 데이터를 검색 Page Size 만큼만 한 번에 메모리로 가져오기에 메모리 사용량이 적어진다. 페이지 단위로 컨넥션을 맺고 끊기를 반복하기에 대량 데이터를 처리하기 좋다. 쿼리자체에 반환하고자하는 데이터 범위를 지정하여 사용한다. (offset, limit) 컨넥션 유지시간이 길지 않고 메모리를 효율적으로 사용해야 하는 데이터에 적합하다. Paging 기반 ItemReader 구현체\nJdbcPagingItemReader HibernatePagingItemReader JpaPagingItemReader MybatisPagingItemReader 3. MybatisItemReader 구현 3-1. MybatisCursorItemReader MybatisCursorItemReader로 구현시 간단하다. 한 번에 조회해온 결과를 chunk만큼 트랜잭션을 분할하여 대용량 처리를 한다.\nBatchConfig.java\n@Bean public T jobStep(StepBuilderFactory steps) throws Exception { return stepBuilderFactory.get(\u0026#34;JOB\u0026#34;).\u0026lt;T, T\u0026gt; chunk(1000) -- Chunk 사이즈 조절 .reader(itemReader.reader(sqlSessionFactory)) .processor(new itemProcessor()) .writer(new itemWriter()) .build(); } ItemReader.Java\nMyBatisCursorItemReader\u0026lt;T\u0026gt; databaseReader = new MyBatisCursorItemReader\u0026lt;\u0026gt;(); databaseReader.setSqlSessionFactory(sqlSessionFactory); databaseReader.setQueryId(QueryId); databaseReader.setParameterValues(map); databaseReader.setSaveState(true); return databaseReader; 데이터베이스에서 모든 결과를 메모리에 할당한 후, Chunk 사이즈만큼의 트랜잭션 단위로 데이터를 처리한다.\n3-2. MyBatisPagingItemReader 구현 다음과 같이 조회 쿼리 자체에 OFFSET, LIMIT을 설정하여, 한 페이지당 조회할 데이터 위치를 파악한다.\nMyBatisPagingItemReader에서는 다음 파라미터로 페이징 관련 값들에 바로 접근이 가능하다.\n_page : 읽을 page 수량 (0부터 시작)\n_pagesize : 한번에 읽을 페이지 수량 (리턴 받을 데이터의 수량)\n_skiprows : _page * _pagesize (다음 페이지 시작 포인트, offset)\n해당 값들을 쿼리에서 바로 사용 가능하며 다음과 같이 적용할 수 있다.\n\u0026lt;select resultMap=\u0026#34;employeeBatchResult\u0026#34;\u0026gt; SELECT id, name, job FROM employees ORDER BY id ASC OFFSET #{_skiprows} LIMIT #{_pagesize} \u0026lt;/select\u0026gt; 한번에 가져올 페이지 사이즈 (_pagesize)는 ItemReader.Java에서 setPageSize()**를 통해 설정가능하다. (쿼리의 LIMIT에 해당하는 값)\nMyBatisPagingItemReader\u0026lt;T\u0026gt; databaseReader = new MyBatisPagingItemReader\u0026lt;\u0026gt;(); databaseReader.setSqlSessionFactory(sqlSessionFactory); databaseReader.setQueryId(QueryId); databaseReader.setParameterValues(map); databaseReader.setPageSize(1000); -- Paging에서는 한번에 읽을 Page수량을 추가해야한다. default = 10 databaseReader.setSaveState(true); return databaseReader; 주의사항 매 페이지를 신규 조회할때 데이터의 변경되어 전체 페이징 기준이 달라진다면 누락되거나 중복처리되는 데이터가 있을 수 있다.\n같은 이유로, order by를 적절하게 하지 않을 경우 미처리, 혹은 중복처리 되는 데이터가 발견될 수 있다. 매 Paging마다 그 시점의 페이징 데이터를 조회하기 때문이다. 특히 처리완료 데이터를 마킹하면서 처리하고, 미처리 데이터를 조회조건에 넣는다면, 데이터가 처리될 때마다 특정 페이지의 값들이 달라질 것이다. 이 경우 Cursor를 사용하면 쉽게 해결되지만, 메모리 및 컨넥션 타임 문제로 Paging을 꼭 사용하여야 하는 경우에는 쿼리에서 offset을 제거하거나 _page변수를 항상 0으로 지정해 주면 된다. MybatisPagingItemReader.java의 내부 구조를 확인해보면\n@Override protected void doReadPage() { if (sqlSessionTemplate == null) { sqlSessionTemplate = new SqlSessionTemplate(sqlSessionFactory, ExecutorType.BATCH); } Map\u0026lt;String, Object\u0026gt; parameters = new HashMap\u0026lt;\u0026gt;(); if (parameterValues != null) { parameters.putAll(parameterValues); } parameters.put(\u0026#34;_page\u0026#34;, getPage()); parameters.put(\u0026#34;_pagesize\u0026#34;, getPageSize()); parameters.put(\u0026#34;_skiprows\u0026#34;, getPage() * getPageSize()); if (results == null) { results = new CopyOnWriteArrayList\u0026lt;\u0026gt;(); } else { results.clear(); } results.addAll(sqlSessionTemplate.selectList(queryId, parameters)); } _page는 getPage() 값을 사용하기 때문에\nMyBatisPagingItemReader\u0026lt;T\u0026gt; reader = new MyBatisPagingItemReader\u0026lt;T\u0026gt;(){ @Override public int getPage() { return 0; } }; 다음과 같이 매 Paging 조회마다 페이지 값을 0으로 리셋해주면 매 page를 조회할 때마다 offset = 0인 채로 조회가 가능하다. doReadPage()를 override 하여 페이지 읽는 로직 자체를 커스터마이징 하는 것도 가능하다. 참고\nhttps://mybatis.org/spring/batch.html\nhttps://ojt90902.tistory.com/780\nhttps://junuuu.tistory.com/611\nhttps://jojoldu.tistory.com/336\n","permalink":"http://localhost:50666/posts/32/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/32/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"itemreader란\" ke-size=\"size26\"\u003e1. ItemReader란?\u003c/h2\u003e\n\u003cp\u003e스프링 배치의 ItemReader는 다음과 같은 과정을 거쳐 데이터를 처리한다.\n \u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/32/img_1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e대부분의 데이터 형태는 이미 ItemReader로 제공하고 있기에 ItemReader, ItemStream 인터페이스 자체를 구현할 필요는 없다.\u003c/p\u003e\n\u003cp\u003eItemReader는 Chunk 기반 트랜잭션을 다루며 Cursor, Paging 가 대표적인 2가지 방식이다.\u003c/p\u003e\n\u003ch2 id=\"cursor-paging-형식\" ke-size=\"size26\"\u003e2. Cursor, Paging 형식\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/32/img_2.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"cursor기반-itemreader\" ke-size=\"size23\"\u003e2-1. Cursor기반 ItemReader\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eJDBC ResultSet의 기본 기능이다.\u003c/li\u003e\n\u003cli\u003eResultSet이 Open 될 때마다 데이터베이스의 데이터가 반환된다.\u003c/li\u003e\n\u003cli\u003e데이터베이스와 연결 맺은 후 데이터를 Streaming 방식으로 I/O이다.\u003c/li\u003e\n\u003cli\u003e현재 행에서Cursor를 유지하며 다음 데이터를 호출하면 Cursor를 한 칸씩 옮기면서 데이터를 가져온다.\u003c/li\u003e\n\u003cli\u003e하나의 Connection으로 배치가 끝날때까지 사용되기에 Batch가 끝나기 전에 데이터베이스와 애플리케이션의 연결이 먼저 끊어질 수 있어 데이터베이스와 SocketTimeout을 충분한 값으로 설정하여야 한다.\u003c/li\u003e\n\u003cli\u003e모든 결과를 메모리에 할당 하기 때문에 메모리 사용량이 많아진다.\u003c/li\u003e\n\u003cli\u003eChunk 사이즈 만큼의 트랜잭션 단위로 데이터를 처리한다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eCursor 기반 ItemReader 구현체\u003c/strong\u003e\u003c/p\u003e","title":"[Spring] 스프링 배치 ItemReader의 개념, (MybatisCursorItemReader, MybatisPagingItemReader 구현)"},{"content":" 1. TCP/IP 란? 패킷 전송방식의 인터넷 프로토콜인 IP와 전송 조절 프로토콜인 TCP로 이루어져 있다. IP는 패킷 전달 여부를 보증하지 않고, 패킷을 보낸 순서대로 받는 것을 보장하지 않지만, TCP는 IP 위에서 동작하는 프로토콜로 데이터의 전달을 보증하고 보낸 순서대로 받게 해 준다. IP가 패킷 간의 관계를 이해하지 못하고 목적지를 찾아가는 데만 집중한다면 TCP는 Endpoint 간 통신할 준비가 되어있는지, 데이터 전송이 제대로 되었는지, 데이터가 변질되지 않은지, 데이터 유실은 없는지 등을 점검한다. 즉 IP주소 체계를 따르고 IP Routing을 통해 목적지에 도달하여 TCP의 특성을 활용하여 송신자와 수신자의 논리적 연결을 생성, 신뢰성 유지한다.\n(HTTP, FTP, SFTP 등 TCP를 기반으로 하는 많은 어플리케이션 프로토콜들이 IP 위에서 동작하기 때문에 묶어서 TCP/IP로 부르기도 한다.)\n1-1. IP (Internet Protocol) 클라이언트와 서버는 각 IP주소를 가지고 지정한 IP 주소에 패킷 단위로 데이터를 전송한다. 전송된 데이터는 인터넷 망 내의 노드를 거쳐 목적지에 도달하게 된다. IP프로토콜만으로 통신할 경우 한계가 존재한다. 비연결성 - IP프로토콜만으로는 클라이언트에서 서버가 패킷을 받을 수 있는 상황인지 확인할 수 없다. 따라서 수신 서버가 없거나 서비스가 불가능하더라도 패킷을 전송한다.\n비신뢰성 - 인터넷 망 내 노드에 문제가 생기는 경우 패킷이 손실 없이 안전하게 도달하지 못할 수 있거나 전송순서가 바뀔 수 있다.\n또한, 같은 IP를 사용하는 서버에서 통신하는 애플리케이션이 둘 이상이라면 IP 만으로는 구분할 수 없다. 1-2. TCP (Transmission Control Protocol) IP의 핵심 프로토콜 중 하나로 근거리 통신망이나 인트라넷, 인터넷에 연결된 컴퓨터에서 실행되는 프로그램 간의 일련의 옥텟을\n데이터 패킷에 일련의 번호를 부여함으로써 안정적으로 순서대로 에러 없이 교환하도록 한다. Transport Layer에 위치하며 네트워크 정보전달을 통제하는 프로토콜이다. Endpoint 간 연결을 생성하고 데이터를 얼마나 보냈는지, 받았는지가 TCP 헤더에 담겨있으며 흐름제어, 혼잡제어에 관여할 수 있는 다양한 요소들이 포함되어 있다. TCP는 3-way handshake를 통해 통신을 시도한다.\n[네트워크] 소켓(SOCEKT) 통신, 3-way handshake의 개념\nTCP의 특징\n신뢰성보장 \u0026amp; 흐름제어\n분할된 데이터의 고유번호를 통해 수신자가 어디까지 받았는지 지속적으로 확인가능하며, 이를 통해 데이터의 손실 없이 전송이 가능하다. TCP Header 내의 window size를 사용해 한번에 송/수신할 수 있는 데이터 양을 정하며 수신 측에서 받을 수 있는 양을 기준으로 Window size가 정해진다(3-hand shake 간 결정됨). 송수신시 계속 확인 응답을 받기에 신뢰도가 높지만, 데이터 용량이 증가하여 수신속도가 떨어진다.\n혼잡제어\nEndpoint 간의 흐름제어 외에 네트워크망의 혼잡제어를 한다. 송신자는 연결초기에 데이터 송출량을 낮게 잡고 보내면서 수신자의 수신을 확인하며 데이터 송출량을 조금씩 늘린다. 이를 통해 네트워크에 가장 적합한 데이터 송출량을 확인할 수 있으며 이를 'Slow Start'라고 한다.\n멀티캐스트 불가능\n1:1 전송방식으로 유니캐스트성이다. 단일 송신자와 단일수신자 간의 경로 연결이 설정된다. ","permalink":"http://localhost:50666/posts/31/","summary":"\u003chr\u003e\n\u003ch2 id=\"tcpip-란\" ke-size=\"size26\"\u003e1. TCP/IP 란?\u003c/h2\u003e\n\u003cp\u003e패킷 전송방식의 인터넷 프로토콜인 IP와 전송 조절 프로토콜인 TCP로 이루어져 있다. IP는 패킷 전달 여부를 보증하지 않고, 패킷을 보낸 순서대로 받는 것을 보장하지 않지만, TCP는 IP 위에서 동작하는 프로토콜로 데이터의 전달을 보증하고 보낸 순서대로 받게 해 준다. IP가 패킷 간의 관계를 이해하지 못하고 목적지를 찾아가는 데만 집중한다면 TCP는 Endpoint 간 통신할 준비가 되어있는지, 데이터 전송이 제대로 되었는지, 데이터가 변질되지 않은지, 데이터 유실은 없는지 등을 점검한다.\n \u003c/p\u003e\n\u003cp\u003e즉 IP주소 체계를 따르고 IP Routing을 통해 목적지에 도달하여 TCP의 특성을 활용하여 송신자와 수신자의 논리적 연결을 생성, 신뢰성 유지한다.\u003c/p\u003e","title":"[네트워크] TCP/IP의 개념"},{"content":" 1. 소켓(Socket)이란 소켓은 떨어져 있는 두 호스트를 연결해 주는 도구로써 인터페이스 역할을 한다. TCP/IP 기반 네트워크 통신에서 데이터 송수신의 앤드포인트이며 앤드포인트는 IP, Port조합으로 이루어진 목적지를 나타낸다. 서버-클라이언트 간 데이터를 주고받는 양방향 연결 지향성 통신으로 지속적으로 연결을 유지하면서 실시간 데이터를 주고받는 데 사용된다.\n소켓은 서버 소켓과 클라이언트 소켓으로 이루어지며, 다음은 소켓 간 통신이 이루어지는 과정이다.\n1-1. 서버 소켓 클라이언트에서 연결요청이 오기를 기다렸다가 연결 요청이 들어오면 클라이언트와 연결을 맺고 새로운 소켓을 만든다.\nsocket() - CLOSED 상태의 소켓 인스턴스 생성 bind() - ip, port 할당 (중복, 권한 문제로 주소\u0026amp; 포트 할당에 실패 할 수 있다.) listen() - 서버 소켓 상태는 LISTEN 상태로 변경, 클라이언트 요청을 queue 대기열을 만들어 몇 개의 클라이언트를 대기시킬지 결정 accept() - 각 클라이언트와 통신에 필요한 새로운 연결된 소켓을 획득, 클라이언트와 연결 send() / recv() - 데이터 송수신 close() - 소켓 닫기 listen상태에서 3-way handshaking을 거쳐 클라이언트, 소켓 연결이 ESTABLISH 상태가 되며, 그 후 accept()~close()를 반복하며 데이터를 송수신한다.\n1-2. 클라이언트 소켓 서버로 연결요청 및 데이터 전송을한다.\nsockect() - CLOSED 상태의 소켓 인스턴스 생성 connect() - 소켓에 남는 local port 자동으로 할당(별도 ip, port를 바인딩도 가능) , 서버 ip, port로 연결 요청- 3-way handshake가 일어난다. (CLOSED-\u0026gt;SYN_SENT-\u0026gt;ESTABLISHED 상태로 변경) send() / recv() - 데이터 송수신 close() - 소켓 닫기 서버 소켓이 연결은 맺은 후 \u0026quot;새로운\u0026quot; 소켓을 만든다고 표현한 이유는, accept() 할시 기존 listen()하고 있는 소켓은 데이터 송수신에 사용되지 않고, 클라이언트를 연결하기 위해 새로운 소켓을 획득하기 때문이다.\n따라서 accept() 후에는 서버에 대기상태 (LISTENING) 상태의 소켓과 신규 연결된 소켓 (ESTABLISHED) 소켓 2개가 동시에 존재한다.\n2. 3-way handshake 서버가 클라이언트 요청을 기다리며 listen 상태일 때 3-way handshake를 통해 클라이언트-서버 사이 소켓이 연결된다.\nSYN - synchronize의 약자로 처음 주고받을 데이터의 일련번호 ACK - Acknowledgement의 약자로 어떤 번호까지의 데이터를 정상수신 했는지에 대한 데이터 #1. 클라이언트에서 서버 OS에 가상경로 오픈을 의뢰하며 SYN 패킷 전송. 클라이언트는 SYN을 보내고 응답을 기다리는 SYN_SENT 상태로 변경\n#2. 서버는 listen 상태이기에 ACK+SYN 패킷 응답 발송, 클라이언트에서 다시 ACK로 응답하기를 기다리며 SYN_RECEIVED 상태로 변경\n#3. 클라이언트도 다시 ACK패킷으로 응답하며 서버의 새로운 소켓이 생성되며 연결. 서버의 상태가 ESTABLISHED로 변경\n#2이 완료되는 시점에 클라이언트소켓은 send() (데이터송신)이 가능하다.\n서버는 ACK를 받지 못하고 SYN_RCV 상태이지만 클라이언트 소켓은 이미 ESTABLISHED 상태이기 때문이다.\n3. 소켓(Socket)의 종류 3-1. TCP (스트림 소켓) 양방향 바이트 스트림 전송, 연결 지향방식의 소켓 송수신자의 연결을 보장하여 신뢰성 있는 데이터 송신 가능 오류수정, 전송처리, 흐름제어 송신되는 순서에 따라 중복되지 않게 데이터 수신 소량의 데이터보다 대량의 데이터 전송에 적합 3-2. UDP (데이터그램 소켓) 비연결형 소켓 데이터 크기에 제한이 있음 데이터의 순서와 신뢰성을 보장하지 않음 점대점뿐만 아니라 일대다 연결도 가능 확실하게 전달이 보장되지 않음, 데이터가 손실돼도 오류가 발생하지 않음 실시간 멀티미디어 정보를 위해 주로 사용 accept 과정 없이 소켓 생성 후 바로 데이터 송수신 참고\nhttps://devkly.com/network/3-way-handshake-with-c/\nhttps://velog.io/@newdana01/%EC%86%8C%EC%BC%93%EC%9D%B4%EB%9E%80-%EC%A2%85%EB%A5%98-%ED%86%B5%EC%8B%A0-%ED%9D%90%EB%A6%84-HTTP%ED%86%B5%EC%8B%A0%EA%B3%BC%EC%9D%98-%EC%B0%A8%EC%9D%B4 ","permalink":"http://localhost:50666/posts/30/","summary":"\u003chr\u003e\n\u003ch2 id=\"소켓socket이란\" ke-size=\"size26\"\u003e1. 소켓(Socket)이란\u003c/h2\u003e\n\u003cp\u003e소켓은 떨어져 있는 두 호스트를 연결해 주는 도구로써 인터페이스 역할을 한다. TCP/IP 기반 네트워크 통신에서 데이터 송수신의 앤드포인트이며 앤드포인트는 IP, Port조합으로 이루어진 목적지를 나타낸다. 서버-클라이언트 간 데이터를 주고받는 양방향 연결 지향성 통신으로 지속적으로 연결을 유지하면서 실시간 데이터를 주고받는 데 사용된다.\u003c/p\u003e\n\u003cp\u003e소켓은 서버 소켓과 클라이언트 소켓으로 이루어지며, 다음은 소켓 간 통신이 이루어지는 과정이다.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/30/img.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"서버-소켓\" ke-size=\"size23\"\u003e\u003cstrong\u003e1-1. 서버 소켓\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e클라이언트에서 연결요청이 오기를 기다렸다가 연결 요청이 들어오면 클라이언트와 연결을 맺고 새로운 소켓을 만든다.\u003c/p\u003e","title":"[네트워크] 소켓(SOCEKT) 통신, 3-way handshake의 개념"},{"content":" 트랜잭션이란? [Postgresql] - [PostgreSQL] 트랜잭션(Transaction)의 개념 및 사용\n스프링에서 제공하는 트랜잭션 ◆ 동기화 (Synchronization)\n트랜잭션을 시작하기 위한 Connection 객체를 특별 저장소에 보관하고 필요할 때 쓸 수 있도록 한다.\n작업 쓰레드마다 Connection 객체를 독립적으로 관리하기에 멀티 스레드 환경에서도 충돌이 발생하지 않는다. 하지만 JDBC가 아닌 Hiberate 같은 기능을 사용한다면 JDBC 종속적인 트랜잭션 동기화 코드들은 문제가 발생한다. 대표적으로 Hibernate는 Connection이 아니라 Session 객체를 사용하기 때문이다. 이를 해결하기 위해 트랜잭션 관리 부분을 추상화한 기술을 제공하고 있다. ◆ 추상화 (Abstraction)\n어플리케이션에 각 기술(JDBC, Hibernate 등) 종속된 코드를 사용하지 않고 일관된 트랜잭션 처리를 할 수 있다.\n(스프링의 트랜잭션 경계 설정을 위한 추상 인터페이스는 PlatformTransactionManager) ◆ 트랜잭션 분리\n트랜잭션과 비즈니스 로직 코드가 얽혀 있는 경우 깔끔한 분리를 위해 @Transactional 어노테이션을 지원한다.\n@Transactional 어노테이션을 통한 트랜잭션 제어 해당 어노테이션이 붙으면 스프링은 트랜잭션 관리 대상으로 등록하고 설정된 트랜잭션 속성을 부여한다. 어노테이션을 사용하면 메서드 단위로 세밀하게 설정이 가능하며 직관적이어서 사용하는 것을 권장한다.\n트랜잭션 전파 (Propagation) 트랜잭션 경계에서 이미 진행중이거나 이미 진행 중인 트랜잭션이 있거나 없을 때 어떻게 상호작용 할 것인가를 결정한다.\nA작업에 대한 트랜잭션이 진행중이고 B작업이 이어서 시작될 때 각 옵션별로 다음과 같이 수행된다. ◆ PROPAGATION_REQUIRED (기존 트랜잭션에 참여)\n기본설정으로 모든 트랜잭션 매니저가 지원한다. B 작업은 새로운 트랜잭션을 만들지 않고 A에 트랜잭션에 참여한다. 이미 시작된 트랜잭션이 없다면 새로 생성한다. A,B가 같은 트랜잭션상에서 진행되기 때문에 A작업을 진행 중 B작업을 수행하고 다시 남은 A의 작업을 처리할 때 예외처리가 되면 A, B가 모두 롤백된다. ◆ PROPAGATION_REQUIRES_NEW (신규 트랜잭션 생성)\n항상 새로운 트랜잭션을 시작한다. B 작업을 위한 독립적인 트랜잭션을 생성한다. 이미 시작된 트랜잭션이 있으면 잠시 보류시키고 새로운 트랜잭션을 생성한다. B 트랜잭션 경계를 나오는 순간 B 트랜잭션은 독자적으로 커밋 혹은 롤백되며 A작업에 영향을 주지 않는다. A의 작업 간 예외가 발생되어 롤백되어도 B작업엔 영향을 주지 않는다. ◆ PROPAGATION_SUPPORTED (기존 트랜잭션 참여 혹은 트랜잭션 없이 동작)\nB작업은 트랜잭션을 새로 만들지 않고 A트랜잭션에 참여한다. 이미 시작된 트랜잭션이 없을 경우 트랜잭션 없이 진행된다. ◆ PROPAGATION_NOT_SUPPORTED (트랜잭션 없이 동작)\nB작업에 대한 트랜잭션이 없이 진행된다. 이미 시작된 트랜잭션이 있는경우 보류시키고, 트랜잭션을 사용하지 않은 채로 B작업을 수행한다. B가 단순 조회라면 트랜잭션이 필요없을 수 있다. ◆ PROPAGATION_MANDATORY (선행 트랜잭션 필수)\nB작업은 A작업의 트랜잭션에 참여한다. 이미 시작된 트랜잭션이 없을 경우 예외를 발생시킨다. 독립적으로 트랜잭션이 진행되면 안되는 경우에 사용된다. ◆ PROPAGATION_NEVER (트랜잭션 미사용)\n이미 시작된 트랜잭션이 있을경우 예외를 발생시키고, 트랜잭션을 사용하지 않도록 한다. ◆ PROPAGATION_NESTED (자식 트랜잭션 생성)\n이미 시작된 트랜잭션이 있을 경우 중첩(자식) 트랜 잭션을 생성한다. 먼저시작된 부모 트랜잭션의 커밋, 롤백은 자식 트랜잭션에 영향을 주지만, 자식 트랜잭션의 커밋,롤백은 부모 트랜잭션에 영향을 주지 않는다. 예를 들어 중요작업의 마무리 단계에 로그를 DB에 저장할 경우, 로그 저장에 대한 에러는 중요작업을 롤백하지 않고, 중요 작업 자체에 예외가 발생하면 로그는 저장되지 않아야 할 때 사용할 수 있다. JDBC3.0의 SavePoint를 지원하는 드라이버를 사용할 경우 사용가능하다. 사용가능한 트랜잭션 매니저를 확인 후 사용해야 한다.\n격리 수준 (Isolation) 모든 DB 트랜잭션은 격리 수준을 가지고 있다. 서버에서는 여러 개의 트랜잭션이 동시에 진행될 수 있고, 모든 트랜잭션을 독립적으로 격리시킨 후 순차적으로 처리하면 안전하긴 하지만 성능이 크게 떨어질 수밖에 없다. 그러므로 적절한 격리 수준을 통해 가능한 많은 트랜잭션을 동시에 실행시키며 제어해야한다. JDBC 드라이버나 DataSource 등을 통해 설정변경 가능하며, 트랜잭션 단위로 격리도 가능하다.\nDefaultTransactionDefinition의 격리수준 설정은 ISOLATION_DEFAULT로 DataSource에 정의된 격리 수준을 따른다.\n◆ DEFAULT\n드라이버 기본설정을 따른다. 대부분 READ_committed 기본 격리 수준을 가진다. ◆ READ_uncommitted (level 0)\n가장 낮은 격리 수준 커밋되지 않은 데이터에 대한 읽기가 가능하다. 하나의 트랜잭션이 커밋되기 전에 그 변화가 다른 트랜잭션에 그대로 노출되는 문제가 있다. 가장 빠르기 때문에 데이터의 무결성이 조금 떨어지더라도 성능을 극대화할 때 의도적으로 사용한다. ◆ READ_committed (level 1)\n스프링 기본 속성이다 트랜잭션이 커밋 확정된 데이터만 읽기가 가능하다. 하나의 트랜잭션이 읽은 로우를 다른 트랜잭션이 수정할 수 있어서 처음 트랜잭션이 같은 로우를 다시 읽을 때 내용이 달라져 있을 수 있다. ◆ REAPEATABLE_READ (level 2)\n하나의 트랜잭션이 읽은 ROW를 다른 트랜잭션이 수정할 수 없도록 막아준다. 새로운 ROW 추가는 막지 않는다 SELECT로 ROW를 다시 가져오는 경우 변경은 없지만 신규 추가된 데이터는 있을 수 있다. ◆ SERIALIZABLE (level 3)\n가장 강력한 트랜잭션 격리 수준 트랜잭션을 순차적으로 진행시켜 준다. 트랜잭션 완료 시까지 SELECT 문장이 사용하는 모든 데이터에 shared lock이 걸리므로 여러 트랜잭션이 동시에 테이블을 참조할 수 없다. 가장 안전하지만 가장 성능이 떨어지기 때문에 극단적인 상황이 아니라면 사용하면 안 된다 데이터의 일관성 및 동시성 제어를 위해 MVCC를 사용하지 않는다. [Postgresql] - [PostgreSQL] MVCC (Multi-Version Concurrency Control)\n제한시간 (timeout) 트랜잭션을 수행하는 제한시간을 설정할 수 있다. INT 타입 초단위로 설정할 수 있다. timeoutString을 설정하면 문자열로 설정 가능 기본값은 트랜잭션 시스템 제한시간 트랜잭션 매니저가 이 기능을 지원하지 않는다면 예외 발생 트랜잭션을 직접 실행하는 PROPAGATION_REQUIRED, PROPAGATION_REQUIRED_NEW에 사용해야지만 의미가 있다. 읽기 전용 (READ_ONLY) 성능최적화와 쓰기 방지의 목적으로 읽기 전용을 설정할 수 있다. 읽기 전용 설정은 트랜잭션 내에 데이터를 조작하는 시도를 막아주고, 데이터 엑세스 성능을 최적화한다.\n일반적으로 읽기전용 트랜잭션이 시작된 후 INSERT, UPDATE, DELETE 등의 데이터 변경 작업이 시작되면 예외가 발생되지만,\n일부 트랜잭션 매니저의 경우 읽기전용 속성을 무시하는 경우가 있기에 주의하여야 한다.\n롤백/커밋 예외 체크 예외란 Exception 클래스 하위 클래스, 언체크 예외란 Exception 하위 RuntimeException의 하위 클래스이다. @Transactional 내에 예외발생 시 언체크 예외(RuntimeException) 면 자동으로 롤백하고 체크 예외가 발생되면 커밋한다.\n스프링에서는 데이터 접근 관련 에러는 런타임 익셉션으로 전환하여 throw 하기 때문에 런타임 익셉션만 롤백 대상이기 때문이다.\n롤백/커밋 동작방식의 변경을 원하면,\n커밋 대상이지만 롤백을 발생시킬 예외나 클래스 이름은 각각 rollbackFor, rollbackForClassName으로 지정할 수 있고, 롤백 대상인 런타임 예외를 트랜잭션 커밋대상으로 지정하기 위해서는 noRollbackFor 또는 noRollbackForClassName을 사용할 수 있다. 참고\nhttps://www.ibm.com/docs/ko/wxs/8.6.1?topic=framework-managing-transactions-spring\nhttps://mangkyu.tistory.com/170\nhttps://goddaehee.tistory.com/167\n","permalink":"http://localhost:50666/posts/29/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/29/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"트랜잭션이란\" ke-size=\"size26\"\u003e트랜잭션이란?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://junhkang.tistory.com/20\"\u003e[Postgresql] - [PostgreSQL] 트랜잭션(Transaction)의 개념 및 사용\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"스프링에서-제공하는-트랜잭션\" ke-size=\"size26\"\u003e스프링에서 제공하는 트랜잭션\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e◆ 동기화 (Synchronization)\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e트랜잭션을 시작하기 위한 Connection 객체를 특별 저장소에 보관하고 필요할 때 쓸 수 있도록 한다.\u003c/p\u003e\n\u003cp\u003e작업 쓰레드마다 Connection 객체를 독립적으로 관리하기에 멀티 스레드 환경에서도 충돌이 발생하지 않는다. 하지만 JDBC가 아닌 Hiberate 같은 기능을 사용한다면 JDBC 종속적인 트랜잭션 동기화 코드들은 문제가 발생한다. 대표적으로 Hibernate는 Connection이 아니라 Session 객체를 사용하기 때문이다. 이를 해결하기 위해 트랜잭션 관리 부분을 추상화한 기술을 제공하고 있다.\n \u003c/p\u003e","title":"[Spring] 스프링 트랜잭션의 개념 및 적용 (@Transactional 사용법)"},{"content":" 두 개의 실행 스레드를 가진 프로세스가 하나의 프로세서 위에서 실행 중인 모습\n1. Thread란? CPU 수행의 기본단위이며 특히 프로세스 안의 흐름의 단위이다. 스레드가 수행되는 환경을 Task라고 하며 Thread ID, Program counter, register set, Stack space로 구성된다. 각각의 스레드는 레지스터 상태와 스택을 갖는다. Code, Data 섹션이나 운영체제 자원들은 스레드끼리 공유한다. 스레드의 종류 스레드는 지원 주체에 따라 2가지로 나눌 수 있다.\nUser Threads\n유저 스레드는 사용자 수준의 스레드 라이브러리가 관리하는 스레드 라이브러리는 스레드의 생성 및 스케쥴링 등 관리 기능을 제공한다. 동일 메모리에서 스레드가 생성 및 관리되므로 속도가 빠르다. 여러 개의 사용자 스레드 중 하나의 스레드가 시스템 호출 등으로 중단되면 나머지 스레드가 같이 종료된다. (커널이 프로세스 내부 스레드를 인식하지 못하여 해당 프로세스를 대기상태로 전환시키기 때문) 스레드 라이브러리에는 POSIX, Pthreads, Win32 threads, Java threads 대표적이다 Kernel Threads\n커널 스레드는 커널이 지원하는 스레드 커널 스레드를 사용하면 안정적이지만 유저모드에서 커널모드로 계속 바꿔줘야 하기에 성능이 저하된다. 반대로 유저 스레드를 사용하면 안정성은 떨어지지만 성능이 저하되지는 않는다. 스레드가 시스템 호출 등으로 중단되어도 다른 스레드를 중단시키지 않고 계속 실행시킨다. Thread Group (스레드 그룹) Thread Group (스레드 그룹)이란 관련 있는 스레드를 그룹으로 묶어 다루는 장치이다. 쓰레드 그룹은 다른 스레드그룹에 포함될 수 있으며, 트리형태로 연결된다. 스레드는는 자신이 포함된 스레드 그룹이나 하위 그룹에는 접근가능 하지만, 다른 그룹에는 접근할 수 없다.\nDeamon Thread(데몬 스레드) 다른 일반 스레드의 작업을 돕는 보조 쓰레드 일반 스레드가 모두 종료되면 자동으로 종료 일정시간마도 자동수행되는 저장/ 화면 갱신등에 사용 Thread Pools 스레드를 요청할 때마다 매번 새로 생성하고, 수행하고, 지우고 반복하면 성능저하로 이어진다.\n그래서 미리 스레드 풀에 여러 개의 스레드를 만들어두고 요청이 오면 스레드풀에서 스레드를 할당해 주는 방법을 사용한다. 2. 멀티스레드란? 한 번에 하나의 작업만 수행하면 싱글 스레드, 하나의 프로세스가 둘 이상의 스레드가 동시에 작업을 수행하면 멀티스레드라 한다.\n멀티프로세싱 시스템이 여러 개의 완전한 처리 장치들을 포함하는 반면 멀티스레딩은 스레드 수준뿐 아니라 명령어 수준의 병렬 처리에까지 신경을 쓰면서 하나의 코어에 대한 이용성을 증가하는 것에 초점을 두고 있다. 멀티스레드의 장점 두 프로세스가 하나의 데이터를 공유하려면 공유메모리 또는 파이프를 사용해야 하지만, 효율이 떨어지고 구현/관리하기 힘들다.\n프로세스사이 콘텍스트 스위치가 지속적으로 일어나면 성능저하 발생 (스레드 전환 시에도 일어나지만 속도가 더 빠르다) 응답성 : 대화형 프로그램을 멀티스레드화 하면 일부 스레드가 중단되거나 긴 작업을 수행하더라도 [ 다른 스레드가 별도의 작업을 할 수 있어 응답성이 좋다]{style=\u0026ldquo;color: #333333; text-align: start;\u0026rdquo;}. 자원공유 : 프로세스 내의 자원과 메모리를 공유함으로 시스템 자원의 낭비가 적다. 또한 같은 주소 공간 내에 여러 개의 활동성 스레드를 가질 수 있다는 장점이 있다.\n경제성 : 메모리와 자원할당은 많인 비용이 소모된다. 스레드는 프로세스 내 자원을 공유하기에 스레드생성과 Context Switching을 하는 것이 효율적이다.\n멀티프로세서 활용 : 각각의 스레드가 다른 프로세스에서 병렬로 수행 가능하다. 단일 쓰레드 프로세스는 CPU가 많아도 1개의 CPU에서만 실행되지만, 다중 스레드화 하면 다중 CPU에서 병렬성이 증간된다.\n프로세스와 비교 두 프로세스가 하나의 데이터를 공유하려면 공유메모리 또는 파이프를 사용해야 하지만, 효율이 떨어지고 구현/관리하기 힘들다.\n스레드, 프로세스 사이 콘텍스트 스위치가 지속적으로 일어나면 성능저하 발생하나 스레드의 Context Switching의 속도가 더 빨라서 효율적이다.\n멀티스레드의 단점 캐시, 변환 생인 버퍼(TLB) 등의 하드웨어 리소스를 공유할 때 서로 간섭할 수 있다. Context Switching 시간이 길수록 멀티 쓰레딩의 효율은 저하되어 단순 계산은 싱글 스레드 보다 실행시간이 개선되지 않고 오히려 지연될 수 있다. 멀티 쓰레딩의 하드웨어 지원을 위해 애플리케이션, 운영체제 모두에 최적화 변경이 필요하다. 각 스레드 중 어떤 것이 먼저 실행될지 그 순서를 알 수 없다. 예를 들어 스레드 1, 스레드 2로 다음 작업을 수행할 때,\n공유되는 변수 i의 값을 레지스터에 저장 레지스터의 값을 1 증가 변수 i에 그 값을 저장 쓰레드 동작 i 스레드 1의 레지스터 스레드 2의 레지스터 스레드 1 i의 값을 레지스터에 저장 0 0 스레드 1 레지스터 값을 1 증가 0 1 스레드 1 i에 값 저장 1 1 스레드 2 i의 값을 레지스터에 저장 1 1 1 스레드 2 레지스터 값을 1 증가 1 1 2 스레드 2 i에 값 저장 2 1 2 스레드 순서가 정상적으로 처리된다면 다음과 같이 최종적으로 i = 2가 되지만, 스레드 실행 순서가 달라진다면 스레드 동작 i 스레드 1의 레지스터 스레드 2의 레지스터 스레드 1 i의 값을 레지스터에 저장 0 0 스레드 2 i의 값을 레지스터에 저장 0 0 0 스레드 1 레지스터 값을 1 증가 0 1 0 스레드 2 레지스터 값을 1 증가 0 1 1 스레드 1 i에 값 저장 1 1 1 스레드 2 i에 값 저장 1 1 1 i = 1 이 되기에 의도와 다른 수행이 일어나며, 스레드의 실행조건에 따라 다른 결과를 나타내기에 원인 파악이 힘들다.\n이러한 문제를 경쟁조건이라고 하며 세마포어 같은 방법으로 공유데이터에 접근하는 스레드의 개수를 한 개 이하로 유지하여 해결할 수 있다.\nContext Switching 컴퓨터가 동시에 처리할 수 있는 작업 수는 CPU의 코어 수량과 같다. CPU 코어보다 많은 스레드가 동시에 실행되면 각 코어별로 정해진 시간만큼 번갈아가며 작업을 수행한다. 각 스레드가 서로 번갈아가며 교체될때 쓰레드간 현재까지의 작업상태나 다음 작업에 필요한 데이터를 저장하고 읽는 작업을 하는데 이를 Context Switching라고 한다. Context Switching 시간이 길수록 멀티 쓰레딩의 효율은 저하된다. 그래서 많은 양의 단순계산은 싱글 쓰레드로 처리하는 것이 효율적인 경우가 있기에 쓰레드 수가 많은 게 항상 고성능은 아니다.\nMultithreaded Server Architecture 서버와 클라이언트 사이에도 멀티 스레드를 구현한다. 클라이언트가 새로운 요청을 하면 서버는 스레드를 새로 생성해서 요청을 수행한다. 프로세스 보다 스레드를 생성하는 것이 더 빠르기 때문에 효율적이다.\nMulticore Programming 동시성(Concurrency)\n동시성은 싱글 프로세스에서 사용되는 방식으로 프로세서가 여러 개의 스레드를 번갈아가면 수행하며 동시에 실행되는 것처럼 보이게 한다.\n병렬성(Parallelism)\n병렬성은 멀티코어 방식에서 사용되는 방식으로 여러 개의 코어가 스레드를 동시에 수행한다.\n3. Multithreading Models 유저 스레드와 커널 쓰레드 관계를 정의하는 방식이다.\nMany-to-One Model 하나의 커널 스레드에 여러 개의 유저 스레드 연결 사용자 수준에서의 스레드 관리 주로 커널 스레드를 지원하지 않는 시스템에서 사용 한 번에 하나의 유저스레드만 커널에 접근가능 멀티코어 시스템에서 병렬적인 수행이 불가능 One-to-One Model 하나의 유저 스레드에 하나의 커널 스레드가 대응하는 모델 Many-to-One방식에서 시스템 호출 시 다른 스레드들이 중단되는 문제를 해결할 수 있어 동시성 향상 멀티프로세서 시스템에서는 동시에 여러 개 쓰레드 수행 가능 유저 스레드 증가분만큼 커널 스레드가 증가. 커널 스레드를 생성하는 것은 오버헤드가 큰 작업이기에 성능저하 발생가능 Many-to-Many Model 여러 유저스레드가 더 적거나 같은 수의 커널 스레드에 대응하는 모델 운영체제에 충분한 수의 커널 스레드를 생성가능 커널 스레드의 구체적 개수는 프로그램이나 작동기기에 따라 상이 멀티프로세서 프로그램에서는 싱글프로세서 보다 더 많은 커널 스레드가 생성 커널이 사용자 스레드와 커널 스레드의 매핑을 적절하게 조절 Two-level Model Many-to-Many 모델과 유사 특정 유저 스레드를 위한 커널 스레드를 따로 제공하는 모델 점유율이 높아야 하는 유저 스레드를 더 빠르게 처리 가능 참고\nhttps://ko.wikipedia.org/wiki/스레드_(컴퓨팅)\nhttps://rebro.kr/174\nhttp://www.tcpschool.com/java/java_thread_multi\nhttps://ko.wikipedia.org/wiki/%EB%A9%80%ED%8B%B0%EC%8A%A4%EB%A0%88%EB%94%A9\n","permalink":"http://localhost:50666/posts/28/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/28/img.png\"\u003e\u003c/p\u003e\n\u003cp\u003e두 개의 실행 스레드를 가진 프로세스가 하나의 프로세서 위에서 실행 중인 모습\u003c/p\u003e\n\u003ch2 id=\"thread란\" ke-size=\"size26\"\u003e1. Thread란?\u003c/h2\u003e\n\u003cp\u003eCPU 수행의 기본단위이며 특히 프로세스 안의 흐름의 단위이다. 스레드가 수행되는 환경을 Task라고 하며 Thread ID, Program counter, register set, Stack space로 구성된다. 각각의 스레드는 레지스터 상태와 스택을 갖는다. Code, Data 섹션이나 운영체제 자원들은 스레드끼리 공유한다. \u003c/p\u003e\n\u003ch3 id=\"스레드의-종류\" ke-size=\"size23\"\u003e스레드의 종류\u003c/h3\u003e\n\u003cp\u003e스레드는 지원 주체에 따라 2가지로 나눌 수 있다.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUser Threads\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e유저 스레드는 사용자 수준의 스레드 라이브러리가 관리하는 스레드\u003c/li\u003e\n\u003cli\u003e라이브러리는 스레드의 생성 및 스케쥴링 등 관리 기능을 제공한다.\u003c/li\u003e\n\u003cli\u003e동일 메모리에서 스레드가 생성 및 관리되므로 속도가 빠르다.\u003c/li\u003e\n\u003cli\u003e여러 개의 사용자 스레드 중 하나의 스레드가 시스템 호출 등으로 중단되면 나머지 스레드가 같이 종료된다. (커널이 프로세스 내부 스레드를 인식하지 못하여 해당 프로세스를 대기상태로 전환시키기 때문)\u003c/li\u003e\n\u003cli\u003e스레드 라이브러리에는 POSIX, Pthreads, Win32 threads, Java threads 대표적이다\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eKernel Threads\u003c/strong\u003e\u003c/p\u003e","title":"[운영체제(OS)] 스레드 (Thread), 멀티스레드(Multithreaded Programming)란?"},{"content":" 1. RDS란? RDS는 클라우드에서 데이터베이스를 쉽게 설정, 운영 및 확장할 수 있는 완전관리형 오픈소스 관계형 데이터베이스이다. 온디맨드, 혹은 예약형 인스턴스 구매로 유연한 데이터베이스 관리가 가능하며 스토리지 및 메모리 등의 설정에 따라 금액이 달라진다.\n2. RDS 비용을 결정하는 요소 DB 인스턴스 가용시간 - 1초 단위로 청구되며 1회 최소 10분 과금 스토리지 (월별 GB당) - DB인스턴스에 프로비저닝 한 스토리지 용량 월별 I/O - 총 스토리지 I/O 요청 수 백업 스토리지 - 자동 데이터베이스 백업 및 모든 데이터베이스 스냅샷과 연결된 스토리지 데이터 전송 - RDS에서 인스턴스를 통한 데이터 송수신 티어별 계산은 여기서 가능 (PostgreSQL 기준)\nhttps://calculator.aws/#/addService/RDSPostgreSQLhttps://calculator.aws/#/addService/RDSPostgreSQL\n초기 스타트업 및 신규 프로젝트의 경우 적정 스펙을 선택하는 것이 중요하며, 시스템 사양, 트래픽 (평균 유입량, 피크 지점의 유입량), IOPS 등 운영환경에 따라 비용이 급격하게 달라질 수 있다. 성능에 전혀 문제없는 스펙 차이인데도 10배 이상의 과금이 생기는 경우가 발생하기에 다양한 시뮬레이션을 통해 검증하고, 적용 후에도 모니터링을 통해 지속적인 튜닝이 필요하다.\n3. RDS 주요 옵션 3-1. Deploy options RDS의 비용을 결정하는 것 중 가장 큰 옵션 중 하나는 배포 옵션이다.\nMulti-AZ DB Cluster\n2개의 읽기 가능한 대기 인스턴스가 있는 다중 AZ배포로 메인 DB인스턴스를 생성한다. 서로 다른 AZ에서 동일 인스턴스를 프로비저닝 하고 유지관리 한다. 메인 DB 인스턴스에 영향을 미치는 계획이 생성되거나 중단되는 경우 대기 DB인스턴스 중 하나로 자동 장애복구를 수행한다. Multi-AZ DB Instance\n다른 AZ에 1개의 대기 DB인스턴스와 메인 DB 인스턴스를 동시에 생성한다. 1대의 대기 인스턴스가 장애 대응을 하지만, Multi-AZ DB Cluster 보다 read workload 처리를 덜하고, write 지연이 더 길다는 단점이 있다.\nSingle DB Instance\n대기 인스턴스 없이 메인 DB인스턴스만을 생성한다.\n설명 상으로는 Multi-AZ DB Cluster가 가장 좋지만, 그만큼 비용이 비싸다.\nSingle DB Instance에 비해 Multi-AZ DB Cluster는 약 30배가 넘는 과금을 부여하여 차이가 굉장히 크다. 실제 운영서버의 경우 실시간 장애대응 및 데이터백업이 필수이기에 최소 Multi-AZ DB Instance를 사용하여 무중단 운영 및 장애대응을 권장한다.\n3-2. 인스턴스 유형 RDS Instance 티어는 크게 T, M, R로 나뉘며 용도 및 금액이 다르다.\nT - 개발용, 특정 상황에 버스팅 되어 사용하는 시스템\nM - 일반적인 운영 상황\nR - 트래픽이 많고, 메모리 사용량이 많은 시스템\n보통 일반 웹사이트, 시스템 운영에서는 M타입 내에서 사이즈, IOPS 선택 후 모니터링을 권장한다. 테스트 서버 혹은 사용자가 적은 단계에서는 T티어로 운영이 가능하지만, T티어 (T4g, T3 제외)의 인스턴스는 일정 크레딧을 보유한 채로 시작하여 인스턴스가 사용될 경우에 크레딧을 차감하는 방식으로 운영되며, 크레딧이 모두 소모될 경우 DB성능이 급격하게 저하되기 때문에 크레딧, CPU, 메모리 모니터링 및 Cloud watch로 크레딧 리밋을 설정하며 관제하는 것을 권장한다.\nRDS-\u0026gt;Databases-\u0026gt;Monitoring에서 인스턴스 현재 밸런스 확인 가능\n실제 크레딧이 0인 채로 10분 이상 지속된 경우 1초 이내 수행되던 쿼리가 8초 이상 지연 (연결 자체가 끊기진 않았다.)되며 시스템에 큰 문제를 일으킨 적이 있다. 4. RDS 인스턴스 선택 현재 DB의 잉여자원이 많지 않은지 지속적인 모니터링을 하는 것이 중요하다.\n또한 단순히 DB 속도가 느려졌기에 인스턴스를 스케일 업 하는 것이 아닌, 정확한 원인파악 후 그에 맞는 튜닝을 하는 것이 중요하다. (스케일 업보다 쿼리 최적화, 주요 로직 개선으로 성능 문제를 해결할 수 있는 경우도 있다.)\n예를 들어 m5.large 티어를 사용 중 스케일업을 고려하고 있다면 m5.xlarge 외에 r6i.large도 같이 고려 대상이 될 수 있다.\n메모리 최적화 r모델의 경우, CPU는 유지한 채 메모리만 16 GiB로 증가시킨 인스턴스로, 현재 데이터베이스에서 메모리 증가의 니즈만 있다면 m5.xlarge (시간당 $0.494) 대신 r6i.large (시간당 $0.30)으로 비용 절감 효과를 볼 수 있다. 또한, 컨넥션 풀을 늘려야 하는 상황에서도, 무조건 다음 단계의 인스턴스를 사용하는 것보다는 필요한 자원만을 최적화하여 스케일업 하는 것이 좋다. Postgresql 기준으로 최대 동시 연결 숫자는\nLEAST({DBInstanceClassMemory/9531392}, 5000)\n로 기본 설정되어 있으며, 이는 인스턴스 메모리의 영향을 받는다. 그렇기에 메모리를 증가시키는 인스턴스 스케일업을 해야 컨넥션 풀이 늘어난다. 신규 인스턴스의 최적화 자원을 찾는 것만큼, 기존 인스턴스가 최적화되어 있는지 모니터링이 필수이다.\nRDS-\u0026gt;Databases의 Monitoring탭에서 상세 자원현황을 확인가능하다. (다음은 주요 지표들이 모니터링되는 현황이다. IOPS, Latency, Memory, CPU 등)\n마지막 캡처의 CPU 같은 경우 70~80% 이상(25.95% 바의 빨간색 선)이 넘을 경우 붉은 그래프로 변하며 위험 수준을 알려준다. 현재 시스템이 사용자가 많고 불안정하다면 기본 수치보다 낮은 수치로 Cloud Watch와 연결하여 모니터링하는 것을 권장한다.\n5. 비용 최적화 5-1. T티어 사용 시 T 티어를 사용 중인데 크레디트가 바닥을 치는 경우가 자주 있다면 AWS 공식 가격표에서 보이는 가격이 M티어가 더 싸더라도 동일 환경에서 시뮬레이션을 돌려보는 것이 좋다. T티어의 경우 24시간 평균 CPU 사용률이 인스턴스 기본 사용률을 초과하면 추가 요금이 과금되기 때문이다.\n5-2. 인스턴스 유동적 운용 DB를 상시 운영할 필요가 없다면, RDS 인스턴스는 가용 중인 시간을 1초 단위로 (1회 최소 10분 요금) 청구되기에 인스턴스를 유동적으로 관리하는 것도 비용 측면에서 고려해 볼 만하다. (운영 DB인스턴스를 부분적으로 사용하는 것은 부담이 크니 테스트 및 개발 시스템에서만 적용하길 권장)\n5-3. 예약형 인스턴스 운영 인프라가 안정화되고, 향후 예상 트래픽이 어느 정도 확정되고 나면, 온디멘드가 아닌 예약형 인스턴스로 변경해서 사용하면 비용 절감효과를 볼 수 있다. 일정 기간 동안 정해진 규격의 인스턴스를 예약해서 사용하는 기능으로 비용이 약 25~41% (1년 예약, Seoul Region 기준) 저렴하다. 다만 인스턴스 스케일업, 스케일 아웃은 가능하지만 스케일 다운은 불가능하기에 예상 사용량을 정확히 파악하여 사용하는 것을 권장한다. 1년 혹은 3년 선택가능하고 3년 선택 시 비용절감 폭이 50% 이상으로 대폭 증가하지만, 3년 동안 고정된 스펙의 인스턴스를 사용해도 문제 되지 않는 상황인지 정확한 파악이 필요하다.\n5-4. Deploy Options 개발, 테스트 데이터 베이스이고 즉각적인 백업이 중요하지 않다면, Single DB Instance로 설정을 변경하면 Multi-AZ DB Instance 대비 절반, Multi-AZ DB cluster 대비 30배 이상의 비용절감 효과를 볼 수 있다. (운영에는 적어도 Multi-AZ DB Instance를 사용하는 것을 권장)\n5-5. 모니터링 RDS 인스턴스는 트래픽, 데이터 양, 운영 중인 시스템 등의 상황에 맞춰 최적화된 인스턴스를 선택 후, 주요 자원들에 대한 모니터링을 통해 성능 및 비용 최적화를 지속적으로 하는 것이 중요하다. 참고\nhttps://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/CHAP_Limits.html#RDS_Limits.MaxConnections\nhttps://aws.amazon.com/ko/rds/instance-types/\nhttps://aws.amazon.com/rds/postgresql/pricing/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.html\nhttps://aws.amazon.com/ko/rds/mysql/pricing/\n","permalink":"http://localhost:50666/posts/27/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/27/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"rds란\" ke-size=\"size26\"\u003e1. RDS란?\u003c/h2\u003e\n\u003cp\u003eRDS는 클라우드에서 데이터베이스를 쉽게 설정, 운영 및 확장할 수 있는 완전관리형 오픈소스 관계형 데이터베이스이다. \u003c/p\u003e\n\u003cp\u003e온디맨드, 혹은 예약형 인스턴스 구매로 유연한 데이터베이스 관리가 가능하며 스토리지 및 메모리 등의 설정에 따라 금액이 달라진다.\u003c/p\u003e\n\u003ch2 id=\"rds-비용을-결정하는-요소\" ke-size=\"size26\"\u003e2. RDS 비용을 결정하는 요소\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDB 인스턴스 가용시간\u003c/strong\u003e - 1초 단위로 청구되며 1회 최소 10분 과금\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e스토리지 (월별 GB당)\u003c/strong\u003e - DB인스턴스에 프로비저닝 한 스토리지 용량\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e월별 I/O\u003c/strong\u003e - 총 스토리지 I/O 요청 수\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e백업 스토리지\u003c/strong\u003e - 자동 데이터베이스 백업 및 모든 데이터베이스 스냅샷과 연결된 스토리지\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e데이터 전송\u003c/strong\u003e - RDS에서 인스턴스를 통한 데이터 송수신\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e티어별 계산은 여기서 가능 (PostgreSQL 기준)\u003c/p\u003e","title":"[AWS] RDS 적정 인스턴스 선택, 비용 최적화"},{"content":" 1. 실행 계획 확인 --Synopsis EXPLAIN [ ( option [, ...] ) ] 쿼리문 EXPLAIN [ ANALYZE ] [ VERBOSE ] 쿼리문 option 자리에 사용할 수 있는 것들: ANALYZE [ boolean ] VERBOSE [ boolean ] COSTS [ boolean ] BUFFERS [ boolean ] TIMING [ boolean ] FORMAT { TEXT | XML | JSON | YAML } * 이후 설명에서 사용될 테스트 데이터는 아래의 \u0026quot;7. 테스트 데이터 생성\u0026quot; 부분 쿼리 확인\n2. EXPLAIN이란? EXPLAIN을 통한 실행 계획 확인은 PostgreSQL PLANNER가 만든 쿼리 플랜을 보여준다. 조건에 부합하는 자료를 찾기 위해 어떤 테이블 및 인덱스를 조회하는지, 어떤 SCAN 방식을 사용하는지, 각 테이블별 조인 알고리즘은 어떤 것을 사용하는지를 보여준다.\nEXPLAIN ANALYZE SELECT * from TEST_EXPLAIN; 테스트 테이블의 100만개의 행을 조건 없이 조회했을 때의 플랜 조회이다.\n3. EXPLAIN 항목별 의미 EXPLAIN ANALYZE를 통해 단순히 실행 시간만으로 쿼리 튜닝을 진행하는 것은 옳지 않다. 동일 쿼리를 반복적으로 실행할 경우 캐쉬로 인해 속도가 향상되고, ANALYZE를 위한 코스트가 추가되기에(6-2. 주의사항 참고) 실제 스캔방식이나 인덱싱 등 세부 노드별 상세 지표를 확인하여 튜닝을 진행하여야 한다.\n3-1. COST 전체 COST 계산을 위해, 테이블의 ROWS 수와 BLOCK수를 확인한다.\n-- PAGE(BLOCK) 수 조회 (테이블명 소문자로만 조회 가능) SELECT relpages FROM pg_class WHERE relname = \u0026#39;test_explain\u0026#39;; -- 전체 ROW 수 조회 SELECT COUNT(*) FROM test_explain; 이 수치에서 14,213개의 PAGE를 로드하는 COST와 1,000,000개 ROW를 연산하는 COST를 계산하면 된다.\n(# of pages) * (page load cost) + (# of rows) * (row 처리 cost)\nPAGE를 데이터베이스의 저장장치로 부터 읽어서 로드하는 작업보다, 이미 로드되어 정렬된 ROW DATA를 처리하는 과정이 훨씬 빠르고 가볍다. PAGE 로드 과정과 ROW 처리하는 비용을 100:1이라고 가정하면\n14213 * 1 + 1000000*0.01 = 24213\n24,213은 먼저 확인한 쿼리플랜의 COST와 동일한 값이다. 100:1로 가정한 수치가 어떻게 COST와 정확히 일치할 수 있을까? PLANNER의 COST 자체도 실제 비용이 아닌 다른 작업들을 수행하는데 드는 COST의 상대적인 값이기 때문이다. 메모리 및 CPU의 정확한 수치 계산이 아닌 다른 작업에 비해 몇 배가 되는가에 대한 COST이다.\n3-2. ROWS COST 계산식에서 확인한 대로 전체 ROW의 개수이다. 현재 샘플 테이블에서는 1,000,000개 ROW가 있기에 해당 수치가 그대로 나오게 된다.\n3-3. WIDTH 컬럼별 평균 byte 수이다. ANALYZE 옵션을 통해 실행을 시키지 않아도 해당 값은 확인할 수 있는데, 그 이유는 PostgreSQL에서 각 테이블에 대한 상세 통계를 별도 보관하고 있기 때문이다. 그래서 테이블을 크게 수정한 경우에는 Vacuum을 통해 통계정보를 재집계해야지 올바른 플랜을 획득할 수 있다.\nSELECT * FROM pg_stats WHERE tablename = \u0026#39;test_explain\u0026#39;; 4+10+13+6+8은 통계값에서 확인한 41과 일치한다.\n3-4. [ANALYZE] ACTUAL TIME ANALYZE 옵션을 사용할 때만 확인가능한 파라미터이다. 실제 쿼리를 수행하는데 소모된 시간을 보여주며 [값 1.. 값 2] 형태로 표현된다. 앞부분(값 1)은 작업 결과로 첫 번째 row를 리턴하기 전까지 비용이고, 뒷부분(값 2)은 마지막 로우를 리턴할 때까지의 비용이다.\n대부분의 쿼리는 실행하는데 첫 번째부터 마지막까지 전체 소요시간을 파악해야 하지만\n3-4-1. EXISTS EXISTS 구문을 사용하는 서브쿼리 같은 경우 총비용(값 2) 대신 첫 번째 시작비용(값 1)으로 쿼리 플랜을 짠다. EXISTS 구분은 존재여부만 파악되면 이후 데이터의 작업을 중지하기 때문에 총비용(값 2)에 대한 비용을 확인할 필요가 없기 때문이다.\n3-4-2. LIMITS LIMIT의 경우 출력 범위가 전체에서 어느 부분인지를 파악한 후 총비용과 최적화된 COST를 고려해 계산한다. 3-5. [ANALYZE] ROWS 통계에서 나온 값이 아닌 실제 조회한 ROWS 수이다.\n3-6. [ANALYZE] LOOPS 서브플랜의 노드가 1회 이상 조회되는 경우들이 있다. 예를 들어 inner 인덱스 스캔은 nested-loop플랜에서 outer row당 한 번씩 실행된다. 이 경우 ROW당 조회되는 카운트를 표현한 값이다. 해당 예제에서는 단일 테이블 조회로 LOOP가 1회 발생하였기에 1로 조회되었다.\n4. EXPLAIN ANALYZE ANALYZE 옵션을 사용하면 실제 쿼리가 실행되고, 추정비용과 함께 소요비용, 소요시간, 실제 처리된 계획 노드별 전체 로우 수도 보여준다. 추정비용과 실제비용을 같이 보여주기에 Planner가 짐작하는 작업이 실작업에 비해 얼마나 정확한지 보는데도 유용하다 실제로 실행되기 때문에 운영 데이터베이스에 사용할 때는 주의해야 한다. 오래 걸리는 SELECT문은 물론 UPDATE, DELETE, INSERT, CREATE TABLE AS, EXCUTE의 경우 운영데이터베이스에 직접적인 영향을 주기에 트랜잭션 블락에서 처리 후 바로 롤백하는 방법으로 처리하는 것을 권장한다.\nBEGIN; EXPLAIN ANALYZE ...; ROLLBACK; 5. EXPLAIN 추가옵션 (상세 정보 확인) 5-1. VERBOSE EXPLAIN (VERBOSE ) SELECT * from TEST_EXPLAIN TE join test_explain_detail TED on TE.name = TED.name 실행계획 추가정보를 보여준다. 단계별 모든 컬럼 목록을 보여준다 테이블, 함수에는 해당 스키마 명을 보여준다. 조건절에서 사용한 컬럼들도 테이블명과 함께 보여준다. 통계 정보가 출력되기 위한 각 트리거의 이름을 보여준다. 초기값은 FALSE이다. 5-2. BUFFERS EXPLAIN (ANALYSE, buffers true ) SELECT * from TEST_EXPLAIN TE join test_explain_detail TED on TE.name = TED.name; 버퍼 사용량을 보여준다. 특히 공유 BLOCK, 로컬 BLOCK, 임시 BLOCK의 HIT, 읽기, 쓰기, 변경 내용을 포함한다.\n(여기서 Hit 이란 필요한 시점에 해당 블럭이 캐쉬에 있기에 읽기가 생략된 단계이다.)\n공유 블럭 - 일반 테이블, 일반 인덱스에 대한 데이터를 저장한다. 로컬 블럭 - 임시 테이블, 임시 인덱스에 대한 데이터를 저장한다. 임시 블럭 - 정렬, 해쉬, 구체화된 뷰, 실행계획 세부 단위자료와 같은 단기 작업에 사용된 데이터를 저장한다. 결과에 나오는 항목 중 dirted 블럭은 해당쿼리로 변경된 블록의 수이고, written 블럭은 해당 쿼리가 실행되며 세션이 공유버퍼에서 변경된 블록을 디스크에 기록한 수이다. 실행이 필수이므로 ANALYZE 옵션과 같이 사용해야 하며 초기값은 FALSE이다.\n5-3. TIMING EXPLAIN (ANALYSE, TIMING TRUE ) SELECT * from TEST_EXPLAIN TE join test_explain_detail TED on TE.name = TED.name; 실제 각 노드별 소모 시간을 보여준다. 반복적으로 시스템 시간을 확인하는데서 오는 부하는 쿼리의 성능을 굉장히 저하시킬 수 있기 때문에 정확한 시간이 필요한 것이 아니라면 FALSE로 두는 것이 좋다. FALSE 상태로 노드 단위의 시간을 보지 않는다 해도 해당 쿼리의 총 실행시간은 항상 계산된다. 실행이 필수이므로 ANALYZE 옵션과 같이 사용해야 한다. 초기값은 FALSE이다. 5-4. FORMAT EXPLAIN (ANALYSE, FORMAT JSON ) SELECT * from TEST_EXPLAIN TE join test_explain_detail TED on TE.name = TED.name 실행계획 값을 TEXT, XML, JSON, YAML의 형식으로 출력한다. 초기값은 TEXT이다. 6. EXPLAIN 정리 및 주의사항 6-1. VACUUM PostgreSQL 쿼리 플래너가 유효한 실행계획을 만들기 위해 쿼리에 포함된 모든 테이블에 대한 pg_statistic 데이터는 최신화되어야 한다. 보통 Auto Vacuum을 통해 관리되지만 (Vacuum은 이 포스트에서 상세 내용을 확인할 수 있다.)\n[PostgreSQL] Vacuum 개념 및 적절한 사용\n테이블이 최근에 크게 변경되었다면 Auto Vacuum을 기다리거나, 수동 Vacuum으로 즉시 통계 재집계를 실행할 수 있다.\n6-2. ANALYZE 4. ANALYZE 에서 언급한 주의사항 외에도, 실행계획의 노드별 실행 시간을 측정하기 위해 EXPLAIN ANALYZE를 사용하면 시간을 측정하는 부하도 같이 측정된다. 결국 EXPLAIN ANALYZE가 실제 쿼리보다 현저하기 오래 걸리는 경우다 있다. 이 비용은 쿼리의 복잡도 및 사용하는 플랫폼에 따라 달라진다. 최악의 경우 실행시간이 아주 짧은 간단한 쿼리가 시스템이 실행시간을 계산하는데 더 오랜 시간이 걸어 실행계획이 이상하게 나오는 경우도 있다.\n7. 테스트 데이터 생성 100만 유저와 유저별 2개의 child를 가지고 있는 디테일 테이블을 생성한다.\n-- 테스트 테이블 생성 CREATE TABLE test_explain ( id serial PRIMARY KEY, name varchar(255), company varchar(255), address varchar(255), created timestamp ); -- 100만 ROW 인서트 DO $$ DECLARE i INTEGER := 1; BEGIN WHILE i \u0026lt;= 1000000 LOOP INSERT INTO test_explain(name, company, address, created ) VALUES(CONCAT(\u0026#39;name\u0026#39;, i), CONCAT(\u0026#39;company\u0026#39;, i), CONCAT(\u0026#39;addr\u0026#39;, i % 100), now() + i * INTERVAL \u0026#39;1 second\u0026#39;); i := i + 1; END LOOP; END $$; -- 테스트 연관 테이블 생성 CREATE TABLE test_explain_detail ( id serial PRIMARY KEY, name varchar(255), child varchar(255), created timestamp ); -- 200만 ROW 인서트 (한 name당 2개의 child) DO $$ DECLARE i INTEGER := 1; BEGIN WHILE i \u0026lt;= 1000000 LOOP INSERT INTO test_explain_detail(name, child, created ) VALUES (CONCAT(\u0026#39;name\u0026#39;, i), CONCAT(\u0026#39;child\u0026#39;, i), now() + i * INTERVAL \u0026#39;1 second\u0026#39;), (CONCAT(\u0026#39;name\u0026#39;, i), CONCAT(\u0026#39;child\u0026#39;, i+1), now() + i * INTERVAL \u0026#39;1 second\u0026#39;); i := i + 1; END LOOP; END $$; 참고\nhttps://morningcoffee.io/the-postgresql-query-cost-model.html\nhttps://www.postgresql.kr/docs/9.6/sql-explain.html\nhttps://stackoverflow.com/questions/49733675/what-does-loop-in-explain-analyze-statement-mean\n","permalink":"http://localhost:50666/posts/26/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/26/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"실행-계획-확인\" ke-size=\"size26\"\u003e1. 실행 계획 확인\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e--Synopsis\nEXPLAIN [ ( option [, ...] ) ] 쿼리문\nEXPLAIN [ ANALYZE ] [ VERBOSE ] 쿼리문\n\noption 자리에 사용할 수 있는 것들:\n\n    ANALYZE [ boolean ]\n    VERBOSE [ boolean ]\n    COSTS [ boolean ]\n    BUFFERS [ boolean ]\n    TIMING [ boolean ]\n    FORMAT { TEXT | XML | JSON | YAML }\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003e* 이후 설명에서 사용될 테스트 데이터는 아래의 \u0026quot;7. 테스트 데이터 생성\u0026quot; 부분 쿼리 확인\u003c/strong\u003e\u003c/p\u003e","title":"[PostgreSQL] 쿼리 성능향상 (실행계획 보는 법, 상세 확인방법, Explain의 어떤 지표를 봐야할까?)"},{"content":" 1. 뷰(VIEW) 테이블의 사용 (생성, 삭제, 수정) -- 기본 생성 CREATE VIEW comedies AS SELECT * FROM films WHERE kind = \u0026#39;Comedy\u0026#39;; -- 삭제 DROP VIEW comedies --Synopsis CREATE [ OR REPLACE ] VIEW name [ ( column_name [, ...] ) ] AS query -- or CREATE VIEW name [ ( column [, ...] ) ] AS query [ WITH [ CASCADE | LOCAL ] CHECK OPTION ] 2. 뷰(VIEW)의 개념 및 특징 정의된 쿼리를 실행시켜 가상의 테이블 형태로 보여주며 테이블을 조회하는 것과 같은 방식으로 조회가 가능하다. VIEW는 물리적으로 생성되지 않는다. 복잡한 쿼리를 단순화시키거나 반복된 쿼리 작업을 효율적으로 처리할 수 있게 해 준다. VIEW에 참조된 쿼리는 호출 시 매번 새로 실행되기에 실시간 결과물을 조회할 수 있다. CREATE OR REPLACE VIEW로 VIEW를 수정할 시, 완전히 일치하는 컬럼 셋을 조회하는 쿼리로만 대체가 가능하다. (같은 컬럼명과 데이터타입) Schema 명을 명시적으로 작성하면 해당 Schema에, 아니라면 현재 Schema에 생성된다. View, Table, Sequence, Index는 한 스키마에 중복된 명칭을 가질 수 없다. VIEW 결과물은 수정이 불가능하다. 테이블의 전체 컬럼 및 정보를 직접적으로 노출시키지 않은 채로 사용이 가능하다. 3. 주의사항 3-1. READ-ONLY VIEW 자체에 insert, update, delete를 실행할 수 없다.\n(RULE 생성을 적절하게 사용하여 업데이트 가능한 뷰의 효과를 볼 수는 있지만 설정이 까다롭기에 새로 생성하는 것이 효율적이다.)\n3-2. 데이터 타입/ 컬럼명 선언 뷰를 생성할 시 데이터타입과 컬럼명을 선언해주어야한다. 그렇지 않을 경우 기본 컬럼명은 ?column?, 데이터 타입은 unknown으로 선언된다. 예를들어 다음 쿼리로 VIEW를 생성하면 컬럼명 = ?column?, 데이터타입 = unknown으로 생성된다.\nCREATE VIEW vista AS SELECT \u0026#39;Hello World\u0026#39;; 이를 방지하기 위해서는 다음과 같이 SQL문을 작성해야 한다.\nCREATE VIEW vista AS SELECT text \u0026#39;Hello World\u0026#39; AS hello; 추가로, VIEW를 수정할 때 정확히 일치하는 데이터타입 및 컬럼명 세트를 가진 쿼리로만 수정이 가능하다.\n3-3. 권한 테이블을 참고하는 권한은 VIEW 소유자의 권한을 따른다. 그러나 view 내에서 호출되는 함수는 쿼리에서 직접 호출된 것과 동일한 권한을 가진다. 그래서 VIEW user는 VIEW에서 사용 중인 모든 함수들에 대한 권한을 보유하고 있어야 한다. 참고\nhttps://www.postgresql.org/docs/8.0/sql-createview.html ","permalink":"http://localhost:50666/posts/25/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/25/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"뷰view-테이블의-사용-생성-삭제-수정\" ke-size=\"size26\"\u003e1. 뷰(VIEW) 테이블의 사용 (생성, 삭제, 수정)\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e-- 기본 생성\nCREATE VIEW comedies AS\n    SELECT *\n    FROM films\n    WHERE kind = \u0026#39;Comedy\u0026#39;;\n\n-- 삭제\nDROP VIEW comedies\n\n--Synopsis\nCREATE [ OR REPLACE ] VIEW name [ ( column_name [, ...] ) ] AS query\n-- or\nCREATE VIEW name [ ( column [, ...] ) ]\n    AS query\n    [ WITH [ CASCADE | LOCAL ] CHECK OPTION ]\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"뷰view의-개념-및-특징\" ke-size=\"size26\"\u003e2. 뷰(VIEW)의 개념 및 특징\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e정의된 쿼리를 실행시켜 가상의 테이블 형태로 보여주며 테이블을 조회하는 것과 같은 방식으로 조회가 가능하다.\u003c/li\u003e\n\u003cli\u003eVIEW는 물리적으로 생성되지 않는다.\u003c/li\u003e\n\u003cli\u003e복잡한 쿼리를 단순화시키거나 반복된 쿼리 작업을 효율적으로 처리할 수 있게 해 준다.\u003c/li\u003e\n\u003cli\u003eVIEW에 참조된 쿼리는 호출 시 매번 새로 실행되기에 실시간 결과물을 조회할 수 있다.\u003c/li\u003e\n\u003cli\u003eCREATE OR REPLACE VIEW로 VIEW를 수정할 시, 완전히 일치하는 컬럼 셋을 조회하는 쿼리로만 대체가 가능하다. (같은 컬럼명과 데이터타입)\u003c/li\u003e\n\u003cli\u003eSchema 명을 명시적으로 작성하면 해당 Schema에, 아니라면 현재 Schema에 생성된다.\u003c/li\u003e\n\u003cli\u003eView, Table, Sequence, Index는 한 스키마에 중복된 명칭을 가질 수 없다.\u003c/li\u003e\n\u003cli\u003eVIEW 결과물은 수정이 불가능하다.\u003c/li\u003e\n\u003cli\u003e테이블의 전체 컬럼 및 정보를 직접적으로 노출시키지 않은 채로 사용이 가능하다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"주의사항\" ke-size=\"size26\"\u003e3. 주의사항\u003c/h2\u003e\n\u003ch3 id=\"read-only\" ke-size=\"size23\"\u003e          3-1. READ-ONLY\u003c/h3\u003e\n\u003cp\u003eVIEW 자체에 insert, update, delete를 실행할 수 없다.\u003c/p\u003e","title":"[PostgreSQL] 뷰(VIEW) 테이블 개념 및 사용, 생성(CREATE), 수정(CREATE OR REPLACE), 삭제(DROP)"},{"content":" 1. CREATE TABLE AS 사용 -- 기본 CREATE TABLE films_recent AS SELECT * FROM films WHERE date_prod \u0026gt;= \u0026#39;2002-01-01\u0026#39;; --Synopsis CREATE [ [ GLOBAL | LOCAL ] { TEMPORARY | TEMP } ] TABLE table_name [ (column_name [, ...] ) ] [ [ WITH | WITHOUT ] OIDS ] AS query 2. CREATE TABLE AS 옵션 - TEMPORARY / TEMP\n임시 테이블로 생성되며 세션이 종료될 시 삭제된다.\n- WITH OIDS / WITHOUT OIDS\nCREATE TABLE AS로 생성된 테이블이 OID를 포함하는지 여부이다. 사용하기 위해서는 default_with_oids 설정이 필요하다.\n3. CREATE TABLE AS란? 3-1. 장점 데이터 작업을 하다 보면 종종 특정 쿼리의 결과물로 별도 테이블을 생성하는 경우가 있다. 일반 테이블을 생성 후 SELECT-INSERT를 사용해도 되지만, 컬럼 명이나 data type을 별도로 맞춰 넣는 작업을 해야 한다. 하지만 CREATE TABLE AS는 테이블을 생성 후 SELECT 쿼리의 결과물을 바로 채워 넣고 테이블의 각 컬럼들은 SELECT 결과의 명칭 (alias를 통해 명시적으로 컬럼명을 재정의 할 수 있다.)과 data type을 자동으로 생성한다. 3-2. VIEW, SELECT INTO 와 다른 점 VIEW를 만드는 것과 유사지만 다르다. VIEW는 매 조회 시 데이터를 새로 조회하지만, CREATE TABLE AS는 테이블을 만들기 위해 SELECT 쿼리를 최초 1회만 수행한다. 그러므로 신규 테이블은 기존 테이블들의 변화를 참조하지 않는다.\nSELECT INTO와도 유사한 기능이지만, SELECT INTO 문법보다 직관적이며 더 많은 기능을 제공한다.\n3-3. 주의사항 PostgreSQL 8.0 이전에는 CREATE TABLE AS는 OID를 항상 포함했고, 8.0 이후부터 OID 포함여부를 선택할 수 있게 되었다.\nOID 포함 여부를 명시적으로 지정하지 않은 경우 default_with_oids 파라미터를 사용한다. default_with_oids는 기본값이 true이다.\nCREATE TABLE AS에서 생성한 테이블에 OID를 필요로 하는 응용 프로그램은 PostgreSQL 향후 버전과의 호환성을 보장하기 위해 WITH OID를 명시적으로 지정해야 한다. 참고\nhttps://www.postgresql.org/docs/8.0/sql-createtableas.html ","permalink":"http://localhost:50666/posts/24/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/24/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"create-table-as-사용\" ke-size=\"size26\"\u003e1. CREATE TABLE AS 사용\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e-- 기본\nCREATE TABLE films_recent AS\n  SELECT * FROM films WHERE date_prod \u0026gt;= \u0026#39;2002-01-01\u0026#39;;\n\n--Synopsis\nCREATE [ [ GLOBAL | LOCAL ] { TEMPORARY | TEMP } ] TABLE table_name [ (column_name [, ...] ) ] [ [ WITH | WITHOUT ] OIDS ]\n    AS query\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"create-table-as-옵션\" ke-size=\"size26\"\u003e2. CREATE TABLE AS 옵션\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e- TEMPORARY / TEMP\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e        임시 테이블로 생성되며 세션이 종료될 시 삭제된다.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e- WITH OIDS / WITHOUT OIDS\u003c/strong\u003e\u003c/p\u003e","title":"[PostgreSQL] CREATE TABLE AS (결과물을 테이블로)"},{"content":" 1. 시퀀스(Sequence)의 사용 1-1. 생성, 삭제, 조회 -- 101부터 시작하는 기본 시퀀스 생성 CREATE SEQUENCE serial START 101; -- 시퀀스 다음값 조회 SELECT nextval(\u0026#39;serial\u0026#39;); -- 시퀀스 현재값 조회 select currval(\u0026#39;serial\u0026#39;); -- 시퀀스 삭제 DROP SEQUENCE serial; -- 시퀀스로 INSERT하기 INSERT INTO distributors VALUES (nextval(\u0026#39;serial\u0026#39;), \u0026#39;nothing\u0026#39;); -- COPY FROM 후에 시퀀스 시작값 변경하기 BEGIN; COPY distributors FROM \u0026#39;input_file\u0026#39;; SELECT setval(\u0026#39;serial\u0026#39;, max(id)) FROM distributors; END; -- Synopsis CREATE [ TEMPORARY | TEMP ] SEQUENCE [ IF NOT EXISTS ] 이름 [ AS 자료형 ] [ INCREMENT [ BY ] 증가값 ] [ MINVALUE 최소값 | NO MINVALUE ] [ MAXVALUE 최대값 | NO MAXVALUE ] [ START [ WITH ] 시작값 ] [ CACHE 캐시 ] [ [ NO ] CYCLE ] [ OWNED BY { 테이블이름.칼럼이름 | NONE } ] 1-2. 사용 중인 시퀀스 확인 select n.nspname as sequence_schema, c.relname as sequence_name, u.usename as owner from pg_class c join pg_namespace n on n.oid = c.relnamespace join pg_user u on u.usesysid = c.relowner where c.relkind = \u0026#39;S\u0026#39; and u.usename = current_user; 2. 시퀀스 생성시 상세 옵션 - TEMPORARY or TEMP\n현재 세션에서만 사용하는 시퀀스를 생성하며, 세션이 종료되면 시퀀스는 자동 삭제된다.\n- IF NOT EXISTS\n동일명의 시퀀스가 있다면 알림만 보여주고 작업은 생략한다.\n- AS 자료형\n시퀀스의 자료형을 설정한다. smallint, integer, and bigint 세 종류로, bigint형이 기본값이다.\n- INCREMENT BY 증가값\n시퀀스 채번식 증가값을 더하여 구한다. 양수라면 증가 시퀀스, 음수면 감소시퀀스이다. default는 1이다.\n- MINVALUE / NO MINVALUE\n해당 시퀀스의 최솟값을 설정한다. 기본 값은 1이며, 감소 시퀀스의 경우 해당 자료형의 최솟값이다.\n- MAXVALUE / NO MAXVALUE\n해당 시퀀스의 최댓값을 설정한다. 기본값은 해당 자료형의 최댓값이다. 감소 시퀀스의 경우 -1이다.\n- START WITH\n시퀀스의 시작값을 설정한다. 기본값은 증가시퀀스의 경우 최솟값이며, 감소 시퀀스는 최댓값이다.\n- CACHE\n시퀀스 채번을 빠르게 하기 위해 메모리에서만 처리하는 캐시값이다.\n기본값은 1이며 캐시를 사용하지 않고 매번 디스크를 사용함을 뜻한다.\n- CYCLE / NO CYCLE\n시퀀스가 최댓값/최솟값에 도달했을 때 순환하며 다시 시작한다.\nNO CYCLE의 경우 최솟값/최댓값에 도달하면 에러로 처리한다.\n기본 설정은 NO CYCLE이다.\n- OWNED BY / OWNED BY NONE\n해당 칼럼과 시퀀스의 의존관계를 생성한다.\n테이블/칼럼이 삭제되면 시퀀스는 자동으로 삭제되며, 테이블/시퀀스의 소유자가 같아야 한다.\nOWNED BY NONE이 기본 옵션이며 어떠한 의존관계도 없는 상태이다.\n3. 시퀀스의 개념 CREATE SEQUENCE는 일련번호 생성기인 SEQUENCE를 생성한다. 시퀀스를 생성하면, 내부적으로 지정한 명칭으로 단일 로우의 특수 테이블을 만들고, 그 로우의 값을 초기화한다. 시퀀스는 특수 용도의 \u0026quot;테이블\u0026quot; 이기 때문에 다음과 같은 쿼리를 사용할 수 있지만,\nSELECT * FROM seq_name; 이 테이블을 직접 조작하면 안되며, 결과에서 last_value (nextval)은 \u0026quot;실행 시점\u0026quot;의 최신 값이다. (그 후로는 다른 세션에서 호출되어 바뀌었을 수도 있는 값이다.)\n4. 시퀀스의 특징 시퀀스명은 시퀀스 / 테이블 / 인덱스 / 뷰 명과 겹칠 수 없다. 시퀀스는 기본적으로 bigint형으로 계산한다. (-9223372036854775808 ~ 9223372036854775807) nextval, setval은 취소가 되지 않는다. 때문에 연속되지 않는 일련번호를 처리하는 용도로 사용할 수 없고, 락을 통해 구현은 가능하지만 시퀀스를 사용하지 않는 것보다 더 높은 코스트가 소모된다. 특히 동시 접속 많은 서비스라면 더 비효율적이다. 5. CACHE 옵션 5-1. CACHE 옵션이란? CACHE 옵션을 사용하는 경우 다중 세션에서 시퀀스가 순차적으로 채번 되지 않는다. 각세션 별로 캐시값만큼 생략 후 그다음 세션시작번호를 채번 한다. (last_Value +캐시-1) 즉 시퀀스 캐시는 한 세션 내의 캐시를 의미한다. 이를 통해 다른 세션에서 이전 세션에서 미사용 한 일련번호를 사용할 수 없도록 하고, 그렇기 때문에 시퀀스가 연속적이지 않는 경우가 종종 발생한다.\n5-2. CACHE 옵션 사용시 주의 사항 캐시를 사용하면 시퀀스가 크다는 의미가 꼭 나중에 생성된 데이터라는 보장이 없다. 그렇기에 순차적 시퀀스를 사용해야 하는 경우라면 캐시값을 1로 설정해야 만한다. 예를 들어\n캐시값은 10으로 설정할 경우\\\nA 세션이 1~10 시퀀스를 선점 B 세션이 11~20 시퀀스를 선점 이 상태에서 B세션에서 더 빨리 시퀀스를 호출하더라도 A세션의 1~10 시퀀스보다 낮은 값을 가질 수 없기 때문이다. (CACHE를 사용하더라도 유니크한 시퀀스를 채번함에는 전혀 지장이 없기에, 순차적 의미로써 시퀀스를 사용할 것이 아니라면 사용하여도 무관하다.) 참고\nhttps://www.postgresql.kr/docs/10/sql-createsequence.html ","permalink":"http://localhost:50666/posts/23/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/23/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"시퀀스sequence의-사용\" ke-size=\"size26\"\u003e1. 시퀀스(Sequence)의 사용\u003c/h2\u003e\n\u003ch3 id=\"생성-삭제-조회\" ke-size=\"size23\"\u003e          1-1. 생성, 삭제, 조회\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e-- 101부터 시작하는 기본 시퀀스 생성\nCREATE SEQUENCE serial START 101;\n-- 시퀀스 다음값 조회\nSELECT nextval(\u0026#39;serial\u0026#39;);\n-- 시퀀스 현재값 조회\nselect currval(\u0026#39;serial\u0026#39;);\n-- 시퀀스 삭제\nDROP SEQUENCE serial;\n\n-- 시퀀스로 INSERT하기\nINSERT INTO distributors VALUES (nextval(\u0026#39;serial\u0026#39;), \u0026#39;nothing\u0026#39;);\n-- COPY FROM 후에 시퀀스 시작값 변경하기\nBEGIN;\nCOPY distributors FROM \u0026#39;input_file\u0026#39;;\nSELECT setval(\u0026#39;serial\u0026#39;, max(id)) FROM distributors;\nEND;\n\n-- Synopsis\nCREATE [ TEMPORARY | TEMP ] SEQUENCE [ IF NOT EXISTS ] 이름\n    [ AS 자료형 ]\n    [ INCREMENT [ BY ] 증가값 ]\n    [ MINVALUE 최소값 | NO MINVALUE ] [ MAXVALUE 최대값 | NO MAXVALUE ]\n    [ START [ WITH ] 시작값 ] [ CACHE 캐시 ] [ [ NO ] CYCLE ]\n    [ OWNED BY { 테이블이름.칼럼이름 | NONE } ]\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"사용-중인-시퀀스-확인\" ke-size=\"size23\"\u003e          1-2. 사용 중인 시퀀스 확인\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eselect n.nspname as sequence_schema, \n          c.relname as sequence_name,\n          u.usename as owner\nfrom pg_class c \n     join pg_namespace n on n.oid = c.relnamespace\n     join pg_user u on u.usesysid = c.relowner\nwhere c.relkind = \u0026#39;S\u0026#39;\n     and u.usename = current_user;\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"시퀀스-생성시-상세-옵션\" ke-size=\"size26\"\u003e2. 시퀀스 생성시 상세 옵션\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e- TEMPORARY or TEMP\u003c/strong\u003e\u003c/p\u003e","title":"[PostgreSQL] 시퀀스(Sequence)의 개념과 사용법(생성, 삭제, 조회 등)"},{"content":" 1. ROLE 1-1. ROLE 생성 -- 기본 CREATE ROLE jonathan LOGIN; -- 비밀번호 포함 CREATE USER davide WITH PASSWORD \u0026#39;jw8s0F4\u0026#39;; -- 권한 포함 CREATE ROLE admin WITH CREATEDB CREATEROLE; -- 사용 기한 포함 CREATE ROLE miriam WITH LOGIN PASSWORD \u0026#39;jw8s0F4\u0026#39; VALID UNTIL \u0026#39;2005-01-01\u0026#39;; -- 삭제 DELETE ROLE miriam; -- Synopsis CREATE ROLE name [ [ WITH ] option [ ... ] ] where option can be: SUPERUSER | NOSUPERUSER | CREATEDB | NOCREATEDB | CREATEROLE | NOCREATEROLE | INHERIT | NOINHERIT | LOGIN | NOLOGIN | REPLICATION | NOREPLICATION | BYPASSRLS | NOBYPASSRLS | CONNECTION LIMIT connlimit | [ ENCRYPTED ] PASSWORD \u0026#39;password\u0026#39; | PASSWORD NULL | VALID UNTIL \u0026#39;timestamp\u0026#39; | IN ROLE role_name [, ...] | IN GROUP role_name [, ...] | ROLE role_name [, ...] | ADMIN role_name [, ...] | USER role_name [, ...] | SYSID uid 1-2. ROLE 이란? CREATE ROLE은 PostgreSQL database cluster에 새로운 ROLE을 추가한다. ROLE은 데이터베이스 object, 권한을 가질 수 있는 엔티티이다. ROLE은 사용방법에 따라 USER, GROUP 혹은 둘다로 간주될 수 있다. CREATEROLE 권한이 있어야지만 사용 가능하다. ALTER ROLE, DELETE ROLE을 통해 권한을 수정, 삭제 가능하다. 1-3. ROLE 권한별 특징 SUPERUSER - 로그인을 제외한 모든 권한 포함 (ex. Role 생성 및 권한 부여) LOGIN - 데이터베이스에 로그인하기 위한 권한 PASSWORD - 로그인 비밀번호 설정 CREATEDB - 데이터베이스 생성 CREATEROLE - ROLE 생성/삭제/수정 REPLICATION - REPLICATION 권한 CONNECTIONLIMIT - 데이터베이스 접속 카운트 INHERIT - ROLE 권한들 상속 2. USER 2-1. USER 생성 -- 기본 CREATE USER jonathan; -- 비밀번호 추가 CREATE USER davide WITH PASSWORD \u0026#39;jw8s0F4\u0026#39;; -- 만료기한 추가 CREATE USER miriam WITH PASSWORD \u0026#39;jw8s0F4\u0026#39; VALID UNTIL \u0026#39;2005-01-01\u0026#39;; -- 권한 추가 CREATE USER manuel WITH PASSWORD \u0026#39;jw8s0F4\u0026#39; CREATEDB; --Synopsis CREATE USER name [ [ WITH ] option [ ... ] ] where option can be: SYSID uid | CREATEDB | NOCREATEDB | CREATEUSER | NOCREATEUSER | IN GROUP groupname [, ...] | [ ENCRYPTED | UNENCRYPTED ] PASSWORD \u0026#39;password\u0026#39; | VALID UNTIL \u0026#39;abstime\u0026#39; 2-2. USER란? CREATE ROLE은 PostgreSQL database cluster에 새로운 User을 추가한다. CREATEUSER 권한이 있어야지만 사용 가능하다. 3. GROUP 3-1. GROUP 생성 --기본 CREATE GROUP staff; --유저 추가 CREATE GROUP marketing WITH USER jonathan, david; --그룹 삭제 DROP GROUP staff; -- Synopsis CREATE GROUP name [ [ WITH ] option [ ... ] ] where option can be: SYSID gid | USER username [, ...] 3-2. GROUP이란? CREATE GROUP은 USER 그룹을 생성한다. SUPERUSER 권한이 있어야지만 생성가능하다. 데이터베이스의 cluster 레벨에 접근 가능하기 위해 GROUP, USER, ROLE은 모두 cluster단에서 정의되어 있다. 4. ROLE, USER, GROUP 차이 ROLE은 Postgresql Database 관련 권한들을 모아 놓은 것으로, 8.1버전부터 USER와 GROUP의 개념이 ROLE로 통합되었다. 현재 버전에서는 USER와 ROLE의 기능은 동일하며, USER는 login 권한이 default, ROLE은 login 권한을 별도로 부여해야 하는 차이점만 있다. CREATE GROUP의 경우 PostgreSQL의 SQL 표준에는 존재하지 않으며, ROLE과 비슷한 개념을 가지고 있다. 참고\nhttps://www.postgresql.org/docs/8.0/sql-creategroup.html\nhttps://www.postgresql.org/docs/8.0/sql-createuser.html\nhttps://www.postgresql.org/docs/current/sql-createrole.html#:~:text=A%20role%20is%20an%20entity,on%20how%20it%20is%20used.\nhttps://docs.aws.amazon.com/ko_kr/AmazonRDS/latest/UserGuide/Appendix.PostgreSQL.CommonDBATasks.Roles.\nhttps://blog.ex-em.com/1655\n","permalink":"http://localhost:50666/posts/22/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/22/img.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"role\" ke-size=\"size26\"\u003e1. ROLE\u003c/h2\u003e\n\u003ch3 id=\"role-생성\" ke-size=\"size23\"\u003e        1-1. ROLE 생성\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e-- 기본\nCREATE ROLE jonathan LOGIN;\n-- 비밀번호 포함\nCREATE USER davide WITH PASSWORD \u0026#39;jw8s0F4\u0026#39;;\n-- 권한 포함\nCREATE ROLE admin WITH CREATEDB CREATEROLE;\n-- 사용 기한 포함\nCREATE ROLE miriam WITH LOGIN PASSWORD \u0026#39;jw8s0F4\u0026#39; VALID UNTIL \u0026#39;2005-01-01\u0026#39;;\n-- 삭제\nDELETE ROLE miriam;\n\n-- Synopsis\n\nCREATE ROLE name [ [ WITH ] option [ ... ] ]\n\nwhere option can be:\n\n      SUPERUSER | NOSUPERUSER\n    | CREATEDB | NOCREATEDB\n    | CREATEROLE | NOCREATEROLE\n    | INHERIT | NOINHERIT\n    | LOGIN | NOLOGIN\n    | REPLICATION | NOREPLICATION\n    | BYPASSRLS | NOBYPASSRLS\n    | CONNECTION LIMIT connlimit\n    | [ ENCRYPTED ] PASSWORD \u0026#39;password\u0026#39; | PASSWORD NULL\n    | VALID UNTIL \u0026#39;timestamp\u0026#39;\n    | IN ROLE role_name [, ...]\n    | IN GROUP role_name [, ...]\n    | ROLE role_name [, ...]\n    | ADMIN role_name [, ...]\n    | USER role_name [, ...]\n    | SYSID uid\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"role-이란\" ke-size=\"size23\"\u003e        1-2. ROLE 이란?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCREATE ROLE은 PostgreSQL database cluster에 새로운 ROLE을 추가한다. \u003c/li\u003e\n\u003cli\u003eROLE은 데이터베이스 object, 권한을 가질 수 있는 엔티티이다.\u003c/li\u003e\n\u003cli\u003eROLE은 사용방법에 따라 USER, GROUP 혹은 둘다로 간주될 수 있다.\u003c/li\u003e\n\u003cli\u003eCREATEROLE 권한이 있어야지만 사용 가능하다.\u003c/li\u003e\n\u003cli\u003eALTER ROLE, DELETE ROLE을 통해 권한을 수정, 삭제 가능하다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"role-권한별-특징\" ke-size=\"size23\"\u003e        1-3. ROLE 권한별 특징\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSUPERUSER - 로그인을 제외한 모든 권한 포함 (ex. Role 생성 및 권한 부여)\u003c/li\u003e\n\u003cli\u003eLOGIN - 데이터베이스에 로그인하기 위한 권한\u003c/li\u003e\n\u003cli\u003ePASSWORD - 로그인 비밀번호 설정\u003c/li\u003e\n\u003cli\u003eCREATEDB - 데이터베이스 생성\u003c/li\u003e\n\u003cli\u003eCREATEROLE - ROLE 생성/삭제/수정\u003c/li\u003e\n\u003cli\u003eREPLICATION - REPLICATION 권한\u003c/li\u003e\n\u003cli\u003eCONNECTIONLIMIT - 데이터베이스 접속 카운트\u003c/li\u003e\n\u003cli\u003eINHERIT - ROLE 권한들 상속\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"user\" ke-size=\"size26\"\u003e2. USER\u003c/h2\u003e\n\u003ch3 id=\"user-생성\" ke-size=\"size23\"\u003e        2-1. USER 생성\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e-- 기본\nCREATE USER jonathan;\n-- 비밀번호 추가\nCREATE USER davide WITH PASSWORD \u0026#39;jw8s0F4\u0026#39;;\n-- 만료기한 추가\nCREATE USER miriam WITH PASSWORD \u0026#39;jw8s0F4\u0026#39; VALID UNTIL \u0026#39;2005-01-01\u0026#39;;\n-- 권한 추가\nCREATE USER manuel WITH PASSWORD \u0026#39;jw8s0F4\u0026#39; CREATEDB;\n\n--Synopsis\nCREATE USER name [ [ WITH ] option [ ... ] ]\n\nwhere option can be:\n    \n      SYSID uid \n    | CREATEDB | NOCREATEDB\n    | CREATEUSER | NOCREATEUSER\n    | IN GROUP groupname [, ...]\n    | [ ENCRYPTED | UNENCRYPTED ] PASSWORD \u0026#39;password\u0026#39;\n    | VALID UNTIL \u0026#39;abstime\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"user란\" ke-size=\"size23\"\u003e        2-2. USER란?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCREATE ROLE은 PostgreSQL database cluster에 새로운 User을 추가한다.\u003c/li\u003e\n\u003cli\u003eCREATEUSER 권한이 있어야지만 사용 가능하다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"group\" ke-size=\"size26\"\u003e3. GROUP\u003c/h2\u003e\n\u003ch3 id=\"group-생성\" ke-size=\"size23\"\u003e        3-1. GROUP 생성\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e--기본\nCREATE GROUP staff;\n--유저 추가\nCREATE GROUP marketing WITH USER jonathan, david;\n--그룹 삭제\nDROP GROUP staff;\n\n-- Synopsis\nCREATE GROUP name [ [ WITH ] option [ ... ] ]\n\nwhere option can be:\n\n     SYSID gid\n   | USER  username [, ...]\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"group이란\" ke-size=\"size23\"\u003e        3-2. GROUP이란?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCREATE GROUP은 USER 그룹을 생성한다. \u003c/li\u003e\n\u003cli\u003eSUPERUSER 권한이 있어야지만 생성가능하다.\u003c/li\u003e\n\u003cli\u003e데이터베이스의 cluster 레벨에 접근 가능하기 위해 GROUP, USER, ROLE은 모두 cluster단에서 정의되어 있다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"role-user-group-차이\" ke-size=\"size26\"\u003e4. ROLE, USER, GROUP 차이\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eROLE은 Postgresql Database 관련 권한들을 모아 놓은 것으로, 8.1버전부터 USER와 GROUP의 개념이 ROLE로 통합되었다.\u003c/li\u003e\n\u003cli\u003e현재 버전에서는 USER와 ROLE의 기능은 동일하며, USER는 login 권한이 default, ROLE은 login 권한을 별도로 부여해야 하는 차이점만 있다. \u003c/li\u003e\n\u003cli\u003eCREATE GROUP의 경우 PostgreSQL의 SQL 표준에는 존재하지 않으며, ROLE과 비슷한 개념을 가지고 있다.\n \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e참고\u003c/p\u003e","title":"[PostgreSQL] 역할 및 권한 (ROLE, USER, GROUP) 개념 및 설정"},{"content":" PostgreSQL 제약조건 (Constrant)란? 데이터베이스는 데이터 타입 외에 제약조건들을 통해 데이터의 무결성을 유지한다.\n제약조건에는 여러 가지 종류가 있으며 DMBS에 마다 다양하지만, 이번 포스트는 PostgreSQL의 5가지 제약 조건들을 설명하겠다. 1. Primary Keys(PK)\n2. Foreign Keys(FK)\n3. Check\n4. Not-null\n5. Unique\n1. Primary Keys (PK) Primary Keys는 테이블의 각 ROW를 구분하는 유니크한 컬럼 혹은 컬럼의 조합이다. Not null, Unique Constraints의 조합이다. 테이블인 단 1개의 PK만 가질 수 있다. PK 생성 시 Postgresql은 B-tree 인덱스를 자동으로 부여한다. B-tree 인덱스를 사용하기 때문에 컬럼의 조합으로 PK를 설정 시 순서가 중요하다. (상세 내용은 다음 포스트에서 확인이 가능하다.) [PostgreSQL] B-tree 인덱스의 원리 및 특징\n1-1. 테이블 생성 시 PK 부여 -- 단일 설정 CREATE TABLE po_headers ( po_no INTEGER PRIMARY KEY, vendor_no INTEGER, description TEXT, shipping_address TEXT ); -- 복합설정 CREATE TABLE TABLE ( column_1 data_type, column_2 data_type, … PRIMARY KEY (column_1, column_2) ); 1-2. 기존 테이블에 PK 속성 부여 ALTER TABLE table_name ADD PRIMARY KEY (column_1, column_2); -- 자동 증가하는 PK 설정 ALTER TABLE vendors ADD COLUMN ID SERIAL PRIMARY KEY; 1-3. PK 삭제 ALTER TABLE table_name DROP CONSTRAINT primary_key_constraint; 2. Foreign Keys 외래키(Foreign Keys)는 다른 테이블의 Primary Key에 참조된 컬럼 혹은 컬럼의 조합이다.\n다른 테이블과의 관계에 따라 다양한 FK를 가질 수 있다. 외래키 설정 후 parent 컬럼의 상태에 따라 다음 액션을 지정할 수 있다.\na. SET NULL b. SET DEFAULT c. RESTRICT d. NO ACTION e. CASCADE\nPostgresql에서는 다음 5가지 parent데이터 변경에 대한 옵션을 제공한다. 다음 FK 설정 예제는 parent데이터가 삭제될 경우 종속된 데이터를 null로 업데이트한다. Cascade의 경우 parent 데이터가 삭제될 경우 종속된 데이터들도 같이 전체 삭제된다.\n2-1. FK 생성 CREATE TABLE customers( customer_id INT GENERATED ALWAYS AS IDENTITY, customer_name VARCHAR(255) NOT NULL, PRIMARY KEY(customer_id) ); CREATE TABLE contacts( contact_id INT GENERATED ALWAYS AS IDENTITY, customer_id INT, contact_name VARCHAR(255) NOT NULL, phone VARCHAR(15), email VARCHAR(100), PRIMARY KEY(contact_id), CONSTRAINT fk_customer FOREIGN KEY(customer_id) REFERENCES customers(customer_id) -- 다음 설정은 parent 데이터가 삭제될시 참조데이터를 null로 업데이트한다. ON DELETE SET NULL ); 3. Check Boolean 타입으로 컬럼에 제약을 줘서 insert 혹은 update 전에 테이블에 유효한 데이터인지를 검증한다.\n(맞지 않는다면 Constraint violation error를 발생시킨다.)\n3-1. Check Constraint 부여한 채로 테이블 생성 CREATE TABLE employees ( id SERIAL PRIMARY KEY, first_name VARCHAR (50), last_name VARCHAR (50), birth_date DATE CHECK (birth_date \u0026gt; \u0026#39;1900-01-01\u0026#39;), joined_date DATE CHECK (joined_date \u0026gt; birth_date), salary numeric CHECK(salary \u0026gt; 0) ); 다음 테이블에는 2가지 Constraint이 걸려있다. birth_date는 1900-01-01 이후 날짜여야 하며, joined_date는 birth_date 이후 날짜여야만 한다.\n3-2. 기존에 테이블에 Check Constraint 추가 ALTER TABLE prices_list ADD CONSTRAINT price_discount_check CHECK ( price \u0026gt; 0 AND discount \u0026gt;= 0 AND price \u0026gt; discount ); 4. Not null 특정 컬럼에 Null 제약을 줘서 insert 혹은 update시 해당 값이 null이 아닌지를 검증한다.\n4-1. Not null Constraint 부여 CREATE TABLE table_name( ... column_name data_type NOT NULL, ... ); check와 Not null을 동시에 적용 가능하다.\nCREATE TABLE invoices( id SERIAL PRIMARY KEY, product_id INT NOT NULL, qty numeric NOT NULL CHECK(qty \u0026gt; 0), net_price numeric CHECK(net_price \u0026gt; 0) ); 4-2. 기존 테이블에 not null 속성을 추가 해당 컬럼에 null 값이 없어야 적용 가능하다.\nALTER TABLE table_name ALTER COLUMN column_name SET NOT NULL; -- 여러개 ALTER TABLE table_name ALTER COLUMN column_name_1 SET NOT NULL, ALTER COLUMN column_name_2 SET NOT NULL, ...; 종종 두 컬럼 중 적어도 1개는 null이 아니게 설정해야 할 경우가 있다.\nCREATE TABLE users ( id serial PRIMARY KEY, username VARCHAR (50), password VARCHAR (50), email VARCHAR (50), CONSTRAINT username_email_notnull CHECK ( NOT ( ( username IS NULL OR username = \u0026#39;\u0026#39; ) AND ( email IS NULL OR email = \u0026#39;\u0026#39; ) ) ) ); 5. Unique insert 혹은 update 시 해당 컬럼에 유니크한 값이 들어있는지를 확인한다. 단일 컬럼 혹은 컬럼의 조합으로 설정이 가능하며 Unique index가 자동으로 부여된다.\n5-1. Unique Constraint 적용한 테이블 생성 CREATE TABLE person ( id SERIAL PRIMARY KEY, first_name VARCHAR (50), last_name VARCHAR (50), email VARCHAR (50) UNIQUE ); 컬럼의 조합에 설정하고 싶을 때는 CREATE TABLE table ( c1 data_type, c2 data_type, c3 data_type, UNIQUE (c2, c3) ); 5-2. 기존 테이블에 Unique Constraint 추가 CREATE UNIQUE INDEX CONCURRENTLY equipment_equip_id ON equipment (equip_id); ALTER TABLE equipment ADD CONSTRAINT unique_equip_id UNIQUE USING INDEX equipment_equip_id; 참고\nhttps://www.postgresqltutorial.com/postgresql-tutorial/postgresql-unique-constraint/\nhttps://www.postgresqltutorial.com/postgresql-tutorial/postgresql-check-constraint/\n","permalink":"http://localhost:50666/posts/21/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/21/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"postgresql-제약조건-constrant란\" ke-size=\"size26\"\u003ePostgreSQL 제약조건 (Constrant)란?\u003c/h2\u003e\n\u003cp\u003e데이터베이스는 데이터 타입 외에 제약조건들을 통해 데이터의 무결성을 유지한다.\u003c/p\u003e\n\u003cp\u003e제약조건에는 여러 가지 종류가 있으며 DMBS에 마다 다양하지만, 이번 포스트는 PostgreSQL의 5가지 제약 조건들을 설명하겠다.\n \u003c/p\u003e\n\u003cp\u003e1. Primary Keys(PK)\u003c/p\u003e\n\u003cp\u003e2. Foreign Keys(FK)\u003c/p\u003e\n\u003cp\u003e3. Check\u003c/p\u003e\n\u003cp\u003e4. Not-null\u003c/p\u003e\n\u003cp\u003e5. Unique\u003c/p\u003e\n\u003ch2 id=\"primary-keys-pk\" ke-size=\"size26\"\u003e1. Primary Keys (PK)\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePrimary Keys는 테이블의 각 ROW를 구분하는 유니크한 컬럼 혹은 컬럼의 조합이다.\u003c/li\u003e\n\u003cli\u003eNot null, Unique Constraints의 조합이다. 테이블인 단 1개의 PK만 가질 수 있다.\u003c/li\u003e\n\u003cli\u003ePK 생성 시 Postgresql은 B-tree 인덱스를 자동으로 부여한다.\u003c/li\u003e\n\u003cli\u003eB-tree 인덱스를 사용하기 때문에 컬럼의 조합으로 PK를 설정 시 순서가 중요하다. (상세 내용은 다음 포스트에서 확인이 가능하다.)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://junhkang.tistory.com/6\"\u003e[PostgreSQL] B-tree 인덱스의 원리 및 특징\u003c/a\u003e\u003c/p\u003e","title":"[PostgreSQL] 제약조건 (Constraint) 개념 및 설정 (Primary Keys, Foreign Keys, Unique, Not null, Check)"},{"content":" 1. 트랜잭션(Transaction)이란? 트랜잭션은 데이터베이스에서 실행되는 일련의 작업들이다. 트랜잭션은 데이터베이스의 무결성 및 작업 간 충돌방지, 데이터 검증을 위해 필수적인 요소이다. 단순한 DML 작업의 롤백 용도뿐 아니라, 대용량 데이터 처리의 무결성, 에러발생 시, 여러 유저의 동시작업 등에서 사용된다.\n2. 트랜잭션 적용 트랜잭션을 사용하는 커맨드 예제이다.\n--COMMIT 혹은 ROLLBACK으로 트랜잭션을 종료하지 않으면, 해당 업데이트 건은 데이터베이스에 적용되지 않는다. BEGIN; UPDATE accounts SET balance = balance - 100.00 WHERE name = \u0026#39;Alice\u0026#39;; COMMIT; 트랜잭션 COMMIT 전에 다른 유저가 동일한 row의 balance를 업데이트하려고 한다면, 그전 트랜잭션이 commit 혹은 rollback 되는 것을 대기해야하며 이를 lock 상태라고 한다. (Lock의 개념 및 상세는 다음 포스트에 상세하게 정리되어 있다.)\n[PostgreSQL] Postgresql Lock이란? (조회 및 kill, Dead lock)\nPostgreSQL에서는 모든 SQL 구문이 트랜잭션 안에서 실행되며, BEGIN을 명시적으로 실행하지 않아도 SQL 명령어를 실행시키면 BIGIN이 되었다고 간주한다. Postgresql의 툴로 사용하는 PgAdmin, Dbeaver, DataGrip 등의 툴에는 각각 트랜잭션을 Manual or Auto로 설정할 수 있다. Manual의 경우 트랜잭션을 commit, rollback을 명시적으로 선언해주지 않을 경우 트랜잭션이 계속 유지되며, Auto의 경우 명시적으로 선언하지 않아도 SQL이 성공적으로 실행된 후 즉시 commit이 실행된 것으로 간주한다.\n3. 트랜잭션의 4가지 특성 3-1. 원자성 Atomic 작업이 최종적으로는 하나로 취급된다. 동일 트랜잭션 내의 작업은 전부 취소 혹은 전부 작업된다. 트랜잭션 내의 작업에 오류가 발생하면 해당 트랜잭션 내의 모든 작업이 취소된다. 3-2. 내구성 durability 트랜잭션이 정상적으로 끝났을 경우, 변경완료된 자료에는 어떠한 간섭도 없이 저장되어야 하고, 손상이 되면 안 된다. 트랜잭션 완료까지 간섭을 없애기 위해, 데이터베이스에서는 트랜잭션이 정상종료됨을 전달받기 전에 트랜잭션에서 발생하는 모든 작업들을 영구저장장치(예, 하드디스크)에 기록해 둔다. 3-3. 고립성 isolation 트랜잭션은 다른 트랜잭션에 의해 간섭받지 않아야 한다. 여러 개의 트랜잭션 발생 시, 각각의 트랜잭션은 다른 트랜잭션의 변경 중인 데이터를 참조, 간섭할 수 없어야 한다. 3-4. 정합성 consistency 트랜잭션은 각강의 명령을 데이터베이스 원데이터에 영향을 주는 것이 아니라 트랜잭션 영역 안에 있는 모든 작업이 끝났을 때, 한 번에 그 변경사항이 데이터베이스에 적용된다. 4. SAVEPOINT 성공/실패에 따라 트랜잭션 내의 작업은 일괄 처리되지만 SAVEPOINT 명령을 사용하여 부분 커밋을 하여 좀 더 유연하게 처리가 가능하다.\nsavepoint로 취소작업을 진행한 뒤에도 트랜잭션 내 작업을 계속 진행할 수 있다. savepoint가 필요 없다고 판단될 시 삭제하여 시스템 자원을 늘릴 수 있다. savepoint로 돌아갈 경우 그지점 이후의 savepoint들도 모두 롤백된다. BEGIN; UPDATE accounts SET balance = balance - 100.00 WHERE name = \u0026#39;Alice\u0026#39;; SAVEPOINT my_savepoint; UPDATE accounts SET balance = balance + 100.00 WHERE name = \u0026#39;Bob\u0026#39;; ROLLBACK TO my_savepoint; UPDATE accounts SET balance = balance + 100.00 WHERE name = \u0026#39;Wally\u0026#39;; COMMIT; 참고 https://www.postgresql.kr/docs/9.6/tutorial-transactions.html ","permalink":"http://localhost:50666/posts/20/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/20/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"트랜잭션transaction이란\" ke-size=\"size26\"\u003e1. 트랜잭션(Transaction)이란?\u003c/h2\u003e\n\u003cp\u003e트랜잭션은 데이터베이스에서 실행되는 일련의 작업들이다. 트랜잭션은 데이터베이스의 무결성 및 작업 간 충돌방지, 데이터 검증을 위해 필수적인 요소이다. 단순한 DML 작업의 롤백 용도뿐 아니라, 대용량 데이터 처리의 무결성, 에러발생 시, 여러 유저의 동시작업 등에서 사용된다.\u003c/p\u003e\n\u003ch2 id=\"트랜잭션-적용\" ke-size=\"size26\"\u003e2. 트랜잭션 적용\u003c/h2\u003e\n\u003cp\u003e트랜잭션을 사용하는 커맨드 예제이다.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e--COMMIT 혹은 ROLLBACK으로 트랜잭션을 종료하지 않으면, 해당 업데이트 건은 데이터베이스에 적용되지 않는다.\n\nBEGIN;\nUPDATE accounts SET balance = balance - 100.00\n    WHERE name = \u0026#39;Alice\u0026#39;;\nCOMMIT;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e트랜잭션 COMMIT 전에 다른 유저가 동일한 row의 balance를 업데이트하려고 한다면, 그전 트랜잭션이 commit 혹은 rollback 되는 것을 대기해야하며 이를 lock 상태라고 한다. (Lock의 개념 및 상세는 다음 포스트에 상세하게 정리되어 있다.)\u003c/p\u003e","title":"[PostgreSQL] 트랜잭션(Transaction)의 개념 및 사용"},{"content":" 1. AMI(Amazon Machine Image)란? 인스턴스를 생성하는데 필요한 정보를 가지고 있는 이미지이다. 한 AMI로 동일 속성을 가진 인스턴스를 여러 개 생성할 수 있으며 인스턴스의 설정값을 가지고 있는 템플릿으로 보면 된다. AMI 생성 시 소프트웨어 구성이 기재된 템플릿 (예: 운영 체제, 애플리케이션 서버, 애플리케이션)을 추출하고, AMI를 사용하여 신규 인스턴스를 바로 복제 가능하다.\n2. 주의사항 AMI 생성시 기존 인스턴스가 정지 또는 최소전력 상태로 전환되니 주의해야 한다. AMI 생성 시 인스턴스 재부팅 안 함 옵션이 선택가능하지만, 재부팅을 하지 않을 경우 시스템의 무결성을 보장할 수 없다.\n3. 적용 (AMI 생성) 3-1. 인스턴스 리스트에서 AMI를 복제할 인스턴스에 마우스 우클릭 3-2. Image and templates -\u0026gt; create image 선택 3-3. 이미지명을 설정 후 \u0026quot;Create Image\u0026quot; 선택 No reboot을 선택할 경우, 인스턴스가 재시작 하진 않지만, 시스템의 무결성을 보장할 수 없다.\n(AMI가 생성되는데 몇분정도 소요된다.) 4. 적용2 (AMI로 인스턴스 생성) 4-1. EC2 레프트메뉴에서 Images -\u0026gt; AMIs 선택 4-2. 복제를 원하는 AMI를 선택 후 상세에서 \u0026quot;Launch Instance from AMI\u0026quot;를 통해 인스턴스 생성 5. 용도 WAS, OS 설정을 그대로 보유한 동일 스펙의 서버 증설 시 사용, 특히 트래픽 급증 등으로 인한 긴급 서버 증설이 필요할 시 AMI를 생성해 놓으면 즉각적인 대응이 가능하다. 참고\nhttps://docs.aws.amazon.com/ko_kr/toolkit-for-visual-studio/latest/user-guide/tkv-create-ami-from-instance.html ","permalink":"http://localhost:50666/posts/19/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/19/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"amiamazon-machine-image란\" ke-size=\"size26\"\u003e1. AMI(Amazon Machine Image)란?\u003c/h2\u003e\n\u003cp\u003e인스턴스를 생성하는데 필요한 정보를 가지고 있는 이미지이다. 한 AMI로 동일 속성을 가진 인스턴스를 여러 개 생성할 수 있으며 인스턴스의 설정값을 가지고 있는 템플릿으로 보면 된다. AMI 생성 시 소프트웨어 구성이 기재된 템플릿 (예: 운영 체제, 애플리케이션 서버, 애플리케이션)을 추출하고, AMI를 사용하여 신규 인스턴스를 바로 복제 가능하다.\u003c/p\u003e\n\u003ch2 id=\"주의사항\" ke-size=\"size26\"\u003e2. 주의사항\u003c/h2\u003e\n\u003cp\u003eAMI 생성시 기존 인스턴스가 정지 또는 최소전력 상태로 전환되니 주의해야 한다. AMI 생성 시 인스턴스 재부팅 안 함 옵션이 선택가능하지만, 재부팅을 하지 않을 경우 시스템의 무결성을 보장할 수 없다.\u003c/p\u003e","title":"[AWS] AMI(Amazon Machine Image) 개념 및 적용"},{"content":" 1. Elastic IP (탄력적 IP) 란? EIP(Elastic Ip Address)란 인터넷으로 접속이 가능한 공인 IP를 할당하여, 인스턴스에 탈부착할 수 있는 서비스이다. 인스턴스 혹은 네트워크 인터페이스에 연결이 가능하며 삭제 전까지 해당 IP를 유지할 수 있다. EC2 인스턴스 생성 시 공인 IP 사용 설정을 Enable로 변경 (default는 Disable)할 경우 인스턴스 자체에 공인 IP를 할당받을 수 있는데 왜 굳이 Elastic IP를 사용하는 것일까?\n인스턴스가 stop 후 재시작될 경우 공인 IP가 변경되는 경우가 발생한다. 인스턴스 자체의 공인 IP가 변경될 경우 큰 문제로 이어질 수 있어\nEIP를 인스턴스에 연결함으로써 인스턴스의 공인IP를 고정시켜 준다.\n2. Elastic IP 개념 및 특징 탄력적 IP 주소는 정적이며 시간이 지남에 따라 변경되지 않는다. 탄력적 IP 주소는 특정 리전에서만 사용할 수 있으며 다른 리전으로 이전할 수 없다. IPv4 주소의 Amazon 풀 또는 AWS 계정으로 가져온 사용자 지정 IPv4 주소 풀에서 탄력적 IP 주소를 할당할 수 있다. (3-3. EIP 옵션 설정 시 선택 가능) 탄력적 IP 주소를 사용하려면 먼저 계정에 주소를 할당한 후 인스턴스 또는 네트워크 인터페이스와 연결해야 한다.(3-6. 인스턴스 할당) 탄력적 IP 주소는 리소스에서 연결 해제했다가 다른 리소스와 다시 연결할 수 있다. 예기치 않은 동작을 방지하려면 변경하기 전에 기존 연결에 이름이 지정된 리소스에 대한 모든 활성 연결이 닫혀 있는지 확인해야 한다. 탄력적 IP 주소를 다른 리소스에 연결한 후 새로 연결된 리소스에 대한 연결을 다시 열 수 있다. 연결 해제한 Elastic IP 주소는 명시적으로 릴리스(삭제)할 때까지 계정에 할당되어 있기 때문에 실행 중인 인스턴스와 연결되지 않은 탄력적 IP 주소에 대해서는 소액의 시간당 요금이 부과된다 (2. 요금 항목 참고). 탄력적 IP 주소를 이전에 퍼블릭 IPv4 주소가 있던 인스턴스와 연결하면 인스턴스의 퍼블릭 DNS 호스트 이름이 탄력적 IP 주소에 맞게 변경된다. Amazon은 퍼블릭 DNS 호스트 이름을 인스턴스 네트워크 외부에서는 인스턴스의 퍼블릭 IPv4 주소 또는 탄력적 IP 주소로 변환하고, 인스턴스 네트워크 내부에서는 인스턴스의 프라이빗 IPv4 주소로 변환한다. AWS 계정으로 가져온 IP 주소 풀에서 탄력적 IP 주소를 할당하는 경우 해당 IP 주소는 탄력적 IP 주소 한도에 포함되지 않는다. 탄력적 IP 주소는 특정 네트워크 경계 그룹에서만 사용할 수 있다. 퍼블릭(IPv4) 인터넷 주소는 흔치 않은 퍼블릭 리소스 이기 때문에 지역당 5개로 제한되며, 인스턴스 장애 시 주소를 다른 인스턴스로 다시 매핑하는 기능이 필요할때는 EIP를 주로 사용하고, 다른 모든 노드 간 통신은 DNS 호스트명을 사용하는 것을 권장한다. (사용개수 제한은 AWS에 별도 문의하여 최대 사용량을 증가시킬 수 있다.)\n3. 요금 실행 중인 인스턴스와 연결된 각 추가 IP 주소에 대해 시간당 0.005 USD(비례 할당으로 계산) 실행 중인 인스턴스와 연결되지 않은 각 탄력적 IP 주소에 대해 시간당 0.005 USD(비례 할당으로 계산) 매달 처음 100개의 재매핑에 대해 탄력적 IP 주소 재 매핑당 0.00 USD 매달 100개 이후의 재매핑에 대해 탄력적 IP 주소 재 매핑당 0.10 USD 상세 요금은 다음 공식 링크에서 확인 가능하다.\nhttps://aws.amazon.com/ec2/pricing/on-demand/\n적용 4-1. EC2 메뉴에서 Elastic IPs 선택 4-2. Allocate Elastic IP address 선택 4-3. 네트워크 경계그룹, IP주소 풀 선택 후 \u0026quot;Allocate\u0026quot;선택 4-3-1. 네트워크 경계 그룹 AWS가 Public 주소를 알리는 가용영역, Local Zone 또는 Wavelength Zone의 집합이다. Local Zone 및 Wavelength Zone은 AWS 네트워크와 해당 영역의 리소스에 액세스 하는 고객 간의 지연 시간 또는 물리적 거리를 최소화하기 위해 리전의 AZ와 다른 네트워크 경계 그룹을 가질 수 있다.\n[주의 : EIP와 연결될 AWS 리소스는 동일한 네트워크 경계 그룹에 할당되어야 한다.] 4-3-2. Public IPv4 address pool [Amazon의 IP 주소 풀(Amazon's pool of IPv4 addresses)] - 특별한 경우가 아니라면 해당 옵션을 선택하여 공인 IP를 할당받으면 된다. IPv4 주소를 Amazon의 IPv4 주소 풀에서 할당하려는 경우이다. AWS 계정으로 가져오는 퍼블릭 IPv4 주소 - AWS 계정으로 가져온 IP 주소 풀에서 IPv4 주소를 할당하려는 경우. IP 주소 풀이 없는 경우에는 이 옵션을 사용할 수 없다. [고객 소유 IPv4 주소 풀(Customer owned pool of IPv4 addresses)] - AWS Outpost를 통해 사용할 온프레미스 네트워크에서 생성된 풀에서 IPv4 주소를 할당하려는 경우. AWS Outposts가 없는 경우 이 옵션이 비활성화된다. 4-4. EIP 생성이 완료되면 리스트에서 확인이 가능하며, 인스턴스 할당을 위해서 해당 항목을 클릭하여 상세 페이지로 들아간다. 4-5. EIP 상세에서 우측 상단 \u0026quot;Associate Elastic IP address\u0026quot;를 선택한다. 4-6. AWS 인스턴스와 연결하고 싶다면 인스턴스를 선택, 특정 사설(private) ip와 연결하고 싶다면 IP주소를 입력하면 된다. \u0026quot;Allow this Elastic IP address to be reassociated\u0026quot; 옵션을 선택하면 해당 EIP를 할당 후 다른 인스턴스로 변경이 가능하다.\n참고\nhttps://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html ","permalink":"http://localhost:50666/posts/18/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/18/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"elastic-ip-탄력적-ip-란\" ke-size=\"size26\"\u003e1. Elastic IP (탄력적 IP) 란?\u003c/h2\u003e\n\u003cp\u003eEIP(Elastic Ip Address)란 인터넷으로 접속이 가능한 공인 IP를 할당하여, 인스턴스에 탈부착할 수 있는 서비스이다. 인스턴스 혹은 네트워크 인터페이스에 연결이 가능하며 삭제 전까지 해당 IP를 유지할 수 있다.\n \u003c/p\u003e\n\u003cp\u003eEC2 인스턴스 생성 시 공인 IP 사용 설정을 Enable로 변경 (default는 Disable)할 경우 인스턴스 자체에 공인 IP를 할당받을 수 있는데 왜 굳이 Elastic IP를 사용하는 것일까?\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/18/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202023-10-09%20%EC%98%A4%ED%9B%84%201.56.51.png\"\u003e\u003c/p\u003e\n\u003cp\u003e인스턴스가 stop 후 재시작될 경우 공인 IP가 변경되는 경우가 발생한다. 인스턴스 자체의 공인 IP가 변경될 경우 큰 문제로 이어질 수 있어\u003c/p\u003e","title":"[AWS] Elastic IP (탄력적 IP)의 개념 및 적용"},{"content":" 1. Vacuum 이란? Vacuum은 postgresql에서 dead tuple이 차지하는 저장공간을 회수한다. 일반적으로 Postgresql에서 update, delete tuple 은 물리적으로 삭제되지 않으며 vacuum이 완료될 때까지 계속 존재한다. (update, delete 시 tuple의 순환은 MVCC 개념에서 확인할 수 있다.)\n[PostgreSQL] MVCC (Multi-Version Concurrency Control)\n그렇기 때문에 특히 자주 업데이트되는 테이블의 경우 주기적인 Vacuum 수행이 필요하다. Vacuum은 특정 테이블에 한해서도 실행이 가능하고, 테이블을 지정하지 않는다면 전체 테이블 (권한을 보유한)에 대해서 실행된다.\n2. Vacuum 명령어 -- DB 전체 full vacuum vacuum full analyze; -- DB 전체 간단하게 실행 vacuum verbose analyze; -- 해당 테이블만 간단하게 실행 vacuum analyze [테이블 명]; -- 특정 테이블만 full vacuum vacuum full [테이블 명]; 3. Vacuum 상세 옵션 VACUUM [ ( option [, ...] ) ] [ table_and_columns [, ...] ] VACUUM [ FULL ] [ FREEZE ] [ VERBOSE ] [ ANALYZE ] [ table_and_columns [, ...] ] where option can be one of: FULL [ boolean ] FREEZE [ boolean ] VERBOSE [ boolean ] ANALYZE [ boolean ] DISABLE_PAGE_SKIPPING [ boolean ] SKIP_LOCKED [ boolean ] INDEX_CLEANUP { AUTO | ON | OFF } PROCESS_MAIN [ boolean ] PROCESS_TOAST [ boolean ] TRUNCATE [ boolean ] PARALLEL integer SKIP_DATABASE_STATS [ boolean ] ONLY_DATABASE_STATS [ boolean ] BUFFER_USAGE_LIMIT [ size ] and table_and_columns is: table_name [ ( column_name [, ...] ) ] 4. Vacuum analyze Vacuum analyze 는 vacuum 후 테이블 별로 analyze를 수행(통계정보 수집)하기에 유지보수에 원활하다.\nPostgresql 버전업을 수행한 후, 인덱스 및 테이블 튜닝이 완료된 쿼리 (평균 소요시간 0.8초 이내)가 5분 이상 소모되어 서비스에 문제를 일으키는 상황이 발생하였다. Vacuum analyze를 사용해 통계 정보를 조정 후 플랜이 정상적으로 작동하는 것을 확인하였다.\n5. Vacuum without Full Full 옵션 없이 vacuum 은 단순히 tuple을 삭제후 공간을 확보하여 재사용 가능하게 한다. 이 작업은 일반적은 read/write와 동시에 실행될 수 있으며 배타적 lock이 발생하지 않는다. 그러나 일반적으로 추가 공간이 OS로 반환되지 않고, 동일 테이블에 재사용가능 상태로 반환된다. 인덱스를 처리하기위해 여러 개의 CPU를 사용할 수 있으며, 이를 parallel Vacuum이라고 한다.\n6. Vacuum Full vacuum full은 테이블 전체 내용을 추가 공간 없이 새로운 디스크파일에 다시 작성한다. 미사용 space는 OS로 반환된다. 일반적인 vacuum 보다 훨씬 시간이 오래 걸리며, 해당 테이블에 lock 이 발생하기에 운영 중인 테이블에 실행 시 주의하여야 한다. Vacuum은 transaction 블럭 내에서는 사용이 불가능하다. Gin 인덱스의 경우 대기중인 인덱스 생성까지 완료된다.\n(Gin 인덱스에 대한 개념은 다음 포스트에서 확인 가능하다.)\n[PostgreSQL] GIN인덱스의 원리 및 특징\n7. Vacuum의 적절한 사용 Vacuum의 수동 실행이아니더라도 postgresql에는 autovacuum launcher가 상시 수행되고 있어 dead tuple이 임계치에 도달하거나 table, tuple의 age가 누적되어 임계치에 도달하였을 때 auto vacuum이 실행된다. Postgresql에서는 Autovacuum 활성화를 권장하고 있다.\nVacuum Full의 경우 테이블 자체의 락이 발생하기에 사용에 주의하여야 하며, 보통 사용자가 없는 시간대나 시스템 영향도가 낮은 시간대에 실행하나, 해당 지표만으로 Vacuum 주기를 설정하는 것이 정답은 아니다. Vacuum이 너무 잦은 것도 Vacuum 부하로 쿼리성능을 저하시키기에 좋지 않고, 영향도가 너무 낮은 시간을 찾기 위해 너무 드물게 수행하면 삭제할 tuple 및 작업이 많아져 오히려 큰 부하를 유발할 수 있다. 그러므로 상세 파라미터 튜닝을 통해 현재 운영 중인 시스템에 맞는 Vacuum주기를 찾는 것이 중요하다. 참고\nhttps://www.postgresql.org/docs/current/sql-vacuum.html\nhttps://techblog.woowahan.com/9478/ ","permalink":"http://localhost:50666/posts/17/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/17/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"vacuum-이란\" ke-size=\"size26\"\u003e1. Vacuum 이란?\u003c/h2\u003e\n\u003cp\u003eVacuum은 postgresql에서 dead tuple이 차지하는 저장공간을 회수한다. 일반적으로 Postgresql에서 update, delete tuple 은 물리적으로 삭제되지 않으며 vacuum이 완료될 때까지 계속 존재한다. \u003c/p\u003e\n\u003cp\u003e(update, delete 시 tuple의 순환은 MVCC 개념에서 확인할 수 있다.)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://junhkang.tistory.com/15\"\u003e[PostgreSQL] MVCC (Multi-Version Concurrency Control)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e그렇기 때문에 특히 자주 업데이트되는 테이블의 경우 주기적인 Vacuum 수행이 필요하다. Vacuum은 특정 테이블에 한해서도 실행이 가능하고, 테이블을 지정하지 않는다면 전체 테이블 (권한을 보유한)에 대해서 실행된다.\u003c/p\u003e\n\u003ch2 id=\"vacuum-명령어\" style=\"color: #000000; text-align: start;\" ke-size=\"size26\"\u003e2. Vacuum 명령어\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e-- DB 전체 full vacuum\nvacuum full analyze;\n\n-- DB 전체 간단하게 실행\nvacuum verbose analyze;\n\n-- 해당 테이블만 간단하게 실행\nvacuum analyze [테이블 명];\n\n-- 특정 테이블만 full vacuum\nvacuum full [테이블 명];\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"vacuum-상세-옵션\" ke-size=\"size26\"\u003e3. Vacuum 상세 옵션\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eVACUUM [ ( option [, ...] ) ] [ table_and_columns [, ...] ]\nVACUUM [ FULL ] [ FREEZE ] [ VERBOSE ] [ ANALYZE ] [ table_and_columns [, ...] ]\n\nwhere option can be one of:\n\n    FULL [ boolean ]\n    FREEZE [ boolean ]\n    VERBOSE [ boolean ]\n    ANALYZE [ boolean ]\n    DISABLE_PAGE_SKIPPING [ boolean ]\n    SKIP_LOCKED [ boolean ]\n    INDEX_CLEANUP { AUTO | ON | OFF }\n    PROCESS_MAIN [ boolean ]\n    PROCESS_TOAST [ boolean ]\n    TRUNCATE [ boolean ]\n    PARALLEL integer\n    SKIP_DATABASE_STATS [ boolean ]\n    ONLY_DATABASE_STATS [ boolean ]\n    BUFFER_USAGE_LIMIT [ size ]\n\nand table_and_columns is:\n\n    table_name [ ( column_name [, ...] ) ]\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"vacuum-analyze\" ke-size=\"size26\"\u003e4. Vacuum analyze\u003c/h2\u003e\n\u003cp\u003eVacuum analyze 는 vacuum 후 테이블 별로 analyze를 수행(통계정보 수집)하기에 유지보수에 원활하다.\u003c/p\u003e","title":"[PostgreSQL] Vacuum 개념 및 적절한 사용"},{"content":" 1. 심볼릭 링크 (Symbolic link)란? 링크를 걸어 원본 파일을 직접 사용하는 것과 같은 효과를 낸다. 특정 폴더에 링크를 걸어 NAS, library 원본 파일을 사용하거나 톰캣 빌듯이 상위경로의 파일을 사용하고자 할 때 사용한다. 심볼릭 링크는 단순히 원본파일을 가리키도록 링크만 연결시켜둔 것으로 원본파일을 가리키기만 하고 있으므로 원본파일의 크기와 무관하며 원본파일이 삭제되어 존재하지 않을 경우에 빨간색으로 링크파일의 원본파일이 없다는 것을 알려준다. 2. 심볼릭 링크 설정하기 ln -s [대상 원본 파일] [새로 만들 파일 이름] -- 파일을 생성 후 링크를 거는 것이 아니라 새로 만들 파일/directory가 없는 채로 링크를 생성을 해야한다. 2-1. ln 옵션 s : 심볼릭링크 생성한다. b : 링크파일 생성 시에 대상파일이 이미 존재하면 백업파일을 만든 후에 링크파일을 생성한다. d : 디렉토리에 대한 하드링크파일생성을 가능하게 한다. f : 대상파일이 존재할 경우에 대상파일을 지우고 링크파일을 생성한다. i : 대상파일이 존재할 경우에 대상파일을 지울건인가를 확인요청한다. t : 링크파일을 생성할 디렉토리를 지정한다. 2-2. 심볼릭 링크 생성(directory) 2-3. 심볼릭 링크 생성(file) 원본 파일을 못 찾을 경우 빨간색으로 알려준다.\n2-4. 심볼릭 링크 삭제 rm [링크 파일] rm -r [링크 디렉토리] ","permalink":"http://localhost:50666/posts/16/","summary":"\u003chr\u003e\n\u003ch2 id=\"심볼릭-링크-symbolic-link란\" ke-size=\"size26\"\u003e1. 심볼릭 링크 (Symbolic link)란?\u003c/h2\u003e\n\u003cp\u003e링크를 걸어 원본 파일을 직접 사용하는 것과 같은 효과를 낸다. 특정 폴더에 링크를 걸어 NAS, library 원본 파일을 사용하거나 톰캣 빌듯이 상위경로의 파일을 사용하고자 할 때 사용한다. 심볼릭 링크는 단순히 원본파일을 가리키도록 링크만 연결시켜둔 것으로 원본파일을 가리키기만 하고 있으므로 원본파일의 크기와 무관하며 원본파일이 삭제되어 존재하지 않을 경우에 빨간색으로 링크파일의 원본파일이 없다는 것을 알려준다.\n \u003c/p\u003e\n\u003ch2 id=\"2-심볼릭-링크-설정하기\"\u003e2. 심볼릭 링크 설정하기\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eln -s [대상 원본 파일] [새로 만들 파일 이름]\n\n-- 파일을 생성 후 링크를 거는 것이 아니라 새로 만들 파일/directory가 없는 채로 링크를 생성을 해야한다.\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"ln-옵션\" style=\"background-color: #ffffff; color: #000000; text-align: start;\" ke-size=\"size23\"\u003e    2-1. ln 옵션\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003es : 심볼릭링크 생성한다.\u003c/li\u003e\n\u003cli\u003eb : 링크파일 생성 시에 대상파일이 이미 존재하면 백업파일을 만든 후에 링크파일을 생성한다.\u003c/li\u003e\n\u003cli\u003ed : 디렉토리에 대한 하드링크파일생성을 가능하게 한다.\u003c/li\u003e\n\u003cli\u003ef : 대상파일이 존재할 경우에 대상파일을 지우고 링크파일을 생성한다.\u003c/li\u003e\n\u003cli\u003ei : 대상파일이 존재할 경우에 대상파일을 지울건인가를 확인요청한다.\u003c/li\u003e\n\u003cli\u003et : 링크파일을 생성할 디렉토리를 지정한다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"심볼릭-링크-생성directory\" ke-size=\"size23\"\u003e     2-2. 심볼릭 링크 생성(directory)\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/16/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202023-10-09%20%EC%98%A4%EC%A0%84%2011.35.53.png\"\u003e\u003c/p\u003e","title":"[Linux] 심볼릭 링크 (Symbolic link) 설정하기"},{"content":" 1. MVCC란? 동시성 제어를 위해 lock을 사용하는 대부분의 다른 데이터베이스 시스템과 달리 Postgres는 다중 버전 모델(multiversion model)을 사용하여 데이터 일관성을 유지한다. 각 트랜잭션이 데이터베이스를 쿼리 하는 동안 데이터의 현재 상태에 관계없이 얼마 전의 데이터 스냅샷을 볼 수 있음을 의미한다. 데이터를 쿼리 하기 위해 트랜잭션을 만들었다면 해당 Transaction은 데이터의 스냅샷을 보고 있는 것이다.\n동일한 행에 서로 다른 트랜잭션이 동시에 업데이트를 시도할 때, 일관성 없는 데이터가 조회되지 않도록 트랜잭션을 보호하여 각 데이터베이스 세션에 대한 트랜잭션 격리를 제공한다. Multiversion과 Lock model의 주요 차이점은 MVCC에서 데이터 read를 위해 획득한 lock과 데이터 쓰기를 위해 획득한 lock이 충돌하지 않는다는 것이다. (따라서 read와 write는 서로 block 하지 않는다.) 이러한 방식을 통해서 Reading 하는 작업에 대해서 Lock을 걸지 않기에 높은 성능을 얻을 수 있게 된다.\n1-1. Postgresql Lock에 대한 상세 설명 Postgresql Lock이란? (조회 및 kill, Dead lock)\n2. PostgreSQL의 MVCC PostgreSQL에서는 record를 tuple이라고 한다. PostgreSQL에서는 멀티버전에 대한 정보를 하나의 Page ( Table )에서 관리하고 있다. 모든 테이블에는 System Columns을 가지고 있고 이들은 미리 정의된 컬럼들로 내부 동작에 사용된다. 이 컬럼 중 mvcc를 구현하게 해주는 것이 xmin, xmax 컬럼이다.\nxmin \u0026ndash; Tuple을 insert 하거나 update 하는 시점의 Transaction ID를 갖는 메타데이터\nxmax \u0026ndash; Tuple을 delete 하거나 update 하는 시점의 Transaction ID를 갖는 메타데이터\n신규 insert, update시 xmin에 현재 transaction id를 넣고 xmax에는 null 값을 넣는다. delete, update시 이전 tuple의 xmax에는 작업을 수행한 transaction id 값을 넣는다. 이를 통해 트랜잭션이 시작된 시점의 Transaction ID와 같거나 작은 Transacion ID를 가지는 데이터를 읽는다. (xmin과 xmax의 범위를 통해 해당 트랜잭션이 조회할 수 있는 데이터인지를 판단한다.)\nxmin | xmax | value -------+-------+----- 100 | 120 | A 102 | 120 | B 110 | 134 | C 115 | 0 | D 115 | 120 | E [Transaction ID 별 조회 가능한 데이터]\nTransaction 101에서는 A\nTransaction 109에서는 A, B\nTransaction 112에서는 A, B, C\nTransaction 117에서는 A, B, C, D, E\n하나의 page에 이전 tuple들이 그대로 존재하기 때문에, row가 삭제되어도 용량은 그대로 차지하는 경우가 있다. 쿼리 성능 또한 지속적으로 떨어지게 된다. 따라서 PostgreSQL에서는 Vacuum 작업을 진행해주어야 한다. Vacuum에 대한 상세 개념은 해당 포스트에서 확인할 수 있다.\nPostgresql Vacuum 개념 및 적절한 사용\n[참고]\nhttps://techblog.woowahan.com/9478/\nhttps://www.postgresql.org/docs/9.1/ddl-system-columns.html\nhttps://www.postgresql.org/docs/7.1/mvcc.html\n","permalink":"http://localhost:50666/posts/15/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/15/img.png\"\u003e\n \u003c/p\u003e\n\u003ch2 id=\"1-mvcc란\"\u003e1.  MVCC란?\u003c/h2\u003e\n\u003cp\u003e동시성 제어를 위해 lock을 사용하는 대부분의 다른 데이터베이스 시스템과 달리 Postgres는 다중 버전 모델(multiversion model)을 사용하여 데이터 일관성을 유지한다. 각 트랜잭션이 데이터베이스를 쿼리 하는 동안 데이터의 현재 상태에 관계없이 얼마 전의 데이터 스냅샷을 볼 수 있음을 의미한다. 데이터를 쿼리 하기 위해 트랜잭션을 만들었다면 해당 Transaction은 데이터의 스냅샷을 보고 있는 것이다.\u003c/p\u003e\n\u003cp\u003e동일한 행에 서로 다른 트랜잭션이 동시에 업데이트를 시도할 때, 일관성 없는 데이터가 조회되지 않도록 트랜잭션을 보호하여 각 데이터베이스 세션에 대한 트랜잭션 격리를 제공한다. Multiversion과 Lock model의 주요 차이점은 MVCC에서 데이터 read를 위해 획득한 lock과 데이터 쓰기를 위해 획득한 lock이 충돌하지 않는다는 것이다. (따라서 read와 write는 서로 block 하지 않는다.) 이러한 방식을 통해서 Reading 하는 작업에 대해서 Lock을 걸지 않기에 높은 성능을 얻을 수 있게 된다.\u003c/p\u003e","title":"[PostgreSQL] MVCC (Multi-Version Concurrency Control)"},{"content":" 1. 인덱스(INDEX) 상세 개념 Postgresql 인덱스(INDEX)개념 및 생성, 삭제, 분석, 설계 방법\n2. 미사용 인덱스 간단히 말해, 인덱스는 지정 컬럼에 매핑된 정보를 별도로 저장하고 있다. 보통 플랜 확인을 통해 효율적으로 인덱스를 추가하여 쿼리 최적화를 진행하게 된다. 오래되고 변경이 잦은 어플리케이션일수록 미사용 인덱스는 늘어나고, 인덱스가 사용되지 않는 경우를 매번 모니터링하여 삭제하는 것은 힘든 일이다. 하지만 불필요 인덱스는 디비 성능저하 및 vacuum 코스트를 증가시키기에, 최적화된 인덱스 생성만큼 최적화된 인덱스 삭제도 중요하다.\n2-1. 미사용 인덱스 검색 쿼리 SELECT schemaname AS schema_name, relname AS table_name, indexrelname AS index_name, pg_size_pretty(pg_relation_size(indexrelid::regclass)) AS index_size, idx_scan, idx_tup_read, idx_tup_fetch FROM pg_stat_user_indexes ORDER BY idx_scan ASC 2-2. 인덱스 삭제 쿼리 DROP INDEX [ CONCURRENTLY ] [ IF EXISTS ] name [, ...] [ CASCADE | RESTRICT ] --simple DROP INDEX IDX_NAME -- 옵션 IF EXISTS 해당 인덱스가 없어도 오류를 내지 않고, 알림 메시지만 보여줌 CASCADE 해당 인덱스와 의존성 관계가 있는 모든 객체도 함께 삭제한, 삭제될 다른 객제와 관계된 또 다른 객체들도 함께 삭제 RESTRICT 해당 인덱스와 의존성 관계가 있는 객체가 있으면 작업을 중지 (기본값) 2-3. 인덱스 삭제 시 주의사항 검색된 인덱스가 실제 미사용 인덱스인지 재검토가 필요하다. \\ 애초에 사용주기가 긴 인덱스의 경우, 사용되는 인덱스이지만 검색되는 경우도 있다.\\ 인덱스는 해당 소유자만 삭제 가능하다. ","permalink":"http://localhost:50666/posts/14/","summary":"\u003chr\u003e\n\u003cp\u003e \u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/14/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"인덱스index-상세-개념\" ke-size=\"size26\"\u003e1. 인덱스(INDEX) 상세 개념\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://junhkang.tistory.com/5\"\u003ePostgresql 인덱스(INDEX)개념 및 생성, 삭제, 분석, 설계 방법\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"미사용-인덱스\" ke-size=\"size26\"\u003e2. 미사용 인덱스\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e간단히 말해, 인덱스는 지정 컬럼에 매핑된 정보를 별도로 저장하고 있다. 보통 플랜 확인을 통해 효율적으로 인덱스를 추가하여 쿼리 최적화를 진행하게 된다. 오래되고 변경이 잦은 어플리케이션일수록 미사용 인덱스는 늘어나고, 인덱스가 사용되지 않는 경우를 매번 모니터링하여 삭제하는 것은 힘든 일이다. 하지만 불필요 인덱스는 디비 성능저하 및 vacuum 코스트를 증가시키기에, 최적화된 인덱스 생성만큼 최적화된 인덱스 삭제도 중요하다.\u003c/p\u003e","title":"[PostgreSQL] 미사용 인덱스(INDEX) 찾기 및 삭제, 성능향상"},{"content":" 1. 발생 해당 에러는 Postgresql에서 Full Text Search를 위해 tsvector 컬럼을 업데이트할 때 발생한다.\n-- 특정 컬럼을 ts_vector로 변경하여 업데이트 UPDATE TABLE SET tsvec_words = to_tsvector(\u0026#39;english\u0026#39;,COLUMN); 2. 원인 해당 컬럼 (혹은 다른 컬럼) 에 테이블 row 업데이트/인서트 시 ts_vector를 자동으로 업데이트하는 trigger가 걸려 있기 때문에 업데이트 간 충돌이 생겨 발생한다.\n3. 해결 트러거를 삭제 후 데이터 업데이트 후에 트리거를 재설정하면 해결된다.\n3-1. 트리거 삭제 drop trigger TABLE_TRGGER on TABLE; 3-2. 트리거 생성 CREATE TRIGGER TABLE_TRIGGER BEFORE INSERT OR UPDATE ON TABLE FOR EACH ROW EXECUTE PROCEDURE tsvector_update_trigger(tsvec_words, \u0026#39;english\u0026#39;,COLUMN); ","permalink":"http://localhost:50666/posts/13/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/13/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"발생\" ke-size=\"size26\"\u003e1. 발생\u003c/h2\u003e\n\u003cp\u003e해당 에러는 Postgresql에서 Full Text Search를 위해 tsvector 컬럼을 업데이트할 때 발생한다.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e-- 특정 컬럼을 ts_vector로 변경하여 업데이트\nUPDATE\n    TABLE\nSET\n    tsvec_words = to_tsvector(\u0026#39;english\u0026#39;,COLUMN);\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"원인\" ke-size=\"size26\"\u003e2. 원인\u003c/h2\u003e\n\u003cp\u003e해당 컬럼 (혹은 다른 컬럼) 에 테이블 row 업데이트/인서트 시 ts_vector를 자동으로 업데이트하는 trigger가 걸려 있기 때문에 업데이트 간 충돌이 생겨 발생한다.\u003c/p\u003e\n\u003ch2 id=\"해결\" ke-size=\"size26\"\u003e3. 해결\u003c/h2\u003e\n\u003cp\u003e트러거를 삭제 후 데이터 업데이트 후에 트리거를 재설정하면 해결된다.\u003c/p\u003e\n\u003ch4 id=\"트리거-삭제\" ke-size=\"size20\"\u003e       3-1. 트리거 삭제\u003c/h4\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edrop trigger TABLE_TRGGER on TABLE;\n\u003c/code\u003e\u003c/pre\u003e\u003ch4 id=\"트리거-생성\" ke-size=\"size20\"\u003e       3-2. 트리거 생성\u003c/h4\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE TRIGGER\n  TABLE_TRIGGER\nBEFORE INSERT OR UPDATE ON\n  TABLE\nFOR EACH ROW EXECUTE PROCEDURE\n  tsvector_update_trigger(tsvec_words, \u0026#39;english\u0026#39;,COLUMN);\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e \u003c/p\u003e","title":"ERROR: text search configuration name \\\"english\\\" must be schema-qualified"},{"content":" 1. 중복 공백 제거 특정 문자열에 대해서 중복 공백 제거를 하고 싶다면 postgresql 정규식을 사용해서 가능하다.\n(공백 외에 단일 문자에 대한 중복제거도 동일한 방법으로 가능하다.)\nselect regexp_replace(name, \u0026#39; +\u0026#39;, \u0026#39; \u0026#39;, \u0026#39;g\u0026#39;) from TABLE; -- \u0026#39;g\u0026#39; 옵션을 제거할 경우 최초 건에 대에서만 변경 2. 중복 단어 제거 컬럼 단위 중복제거는 distinct, group by를 통해 쉽게 가능하지만, 컬럼 내 문자열의 중복 단어 제거의 경우 다음과 같다.\n(쉼표 기준으로 컬럼을 분리, 중복을 제거한 후 다시 연결)\nselect id, array_to_string(array_agg(distinct token), \u0026#39; \u0026#39;) from ( SELECT unnest(string_to_array(COLUMN, \u0026#39; \u0026#39;)) as token, id FROM TABLE) as tmp group by id 3. 실습 3-1. 테이블 생성 -- 테스트 테이블 생성 create table duplicate_test ( id serial primary key, name varchar(255) not null ); 3-2. 테스트 데이터 insert -- 테스트 데이터 입력 insert into duplicate_test (name) values (\u0026#39;서울 서울 대구 서울 부산\u0026#39;), (\u0026#39;서울 서울 대구 서울\u0026#39;), (\u0026#39;부산 대구 대구 서울 서울서울 서울 광주\u0026#39;), (\u0026#39;서울 에서 대구 갔다가 부산 거쳐 다시 서울 로\u0026#39;), (\u0026#39;광주광주대구 대구 대 구 서울 \u0026#39;), (\u0026#39;서울 서울 서울 \u0026#39;), (\u0026#39;서울 대구 대 구 서울 \u0026#39;), (\u0026#39;서울 대구 대구 대 구 서울 \u0026#39;), (\u0026#39;서울 대구 대구 대구 부산부산 서울 부산 서울부 산\u0026#39;) ; 3-3. 중복 공백 제거 select regexp_replace(name, \u0026#39; +\u0026#39;, \u0026#39; \u0026#39;, \u0026#39;g\u0026#39;) from DUPLICATE_TEST; -- \u0026#39;g\u0026#39; 옵션을 제거할 경우 최초 건에 대에서만 변경 select regexp_replace(name, \u0026#39;단일문자열+\u0026#39;, \u0026#39; \u0026#39;, \u0026#39;g\u0026#39;) from DUPLICATE_TEST; -- 단일문자열에 대한 중복 제거도 동일한 방법으로 가능하다. 3-4. 중복 단어 제거 select id, array_to_string(array_agg(distinct token), \u0026#39; \u0026#39;) from ( SELECT unnest(string_to_array(name, \u0026#39; \u0026#39;)) as token, id FROM DUPLICATE_TEST) as tmp group by id ","permalink":"http://localhost:50666/posts/12/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/12/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"중복-공백-제거\" ke-size=\"size26\"\u003e1. 중복 공백 제거\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e특정 문자열에 대해서 중복 공백 제거를 하고 싶다면 postgresql 정규식을 사용해서 가능하다.\u003cbr\u003e\n(공백 외에 단일 문자에 대한 중복제거도 동일한 방법으로 가능하다.)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eselect regexp_replace(name, \u0026#39; +\u0026#39;, \u0026#39; \u0026#39;, \u0026#39;g\u0026#39;) from TABLE; -- \u0026#39;g\u0026#39; 옵션을 제거할 경우 최초 건에 대에서만 변경\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"중복-단어-제거\" style=\"color: #333333; text-align: start;\" ke-size=\"size26\"\u003e2. 중복 단어 제거\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e컬럼 단위 중복제거는 distinct, group by를 통해 쉽게 가능하지만, 컬럼 내 문자열의 중복 단어 제거의 경우 다음과 같다.\u003cbr\u003e\n(쉼표 기준으로 컬럼을 분리, 중복을 제거한 후 다시 연결)\u003c/p\u003e","title":"[PostgreSQL] 문자열내 중복 공백, 단어 제거"},{"content":" 1. BRIN 인덱스란? ▪ Block range index의 약자\n▪ Page 검색에 도움 되는 메타 데이터를 뽑아서 인덱스를 구성 (ex, 특정컬럼의 최대/최솟값)\n▪ 특정 컬럼이 물리 주소의 일정한 상관관계를 가지는 매우 큰 테이블을 다루기 위해 설계 (타임시쿼스한 대용량 데이터 조회에 유용)\nBlock range는 테이블 내에서 근접한 물리주소를 가진 page 그룹을 의미한다. 각 Block range 에 대해 일부 요약 정보가 인덱스로 저장된다. 예를 들어 상점의 판매 주문을 저장하는 테이블에는 각 주문이 배치된 날짜 열이 있을 수 있으며 대부분의 경우 이전 주문시점에 맞게 순차적으로 주문정보가 들어갈 것이고, ZIP 코드 열을 저장하는 테이블에는 도시에 대한 모든 코드가 자연스럽게 그룹화되어 있을 것이다.\nBRIN 인덱스는 정기적인 비트맵 인덱스 검색을 통해 쿼리를 결과를 확인하고, 인덱스에 의해 저장된 요약 정보가 쿼리 조건과 일치하면, 범위 내 모든 페이지의 모든 튜플을 반환한다. 쿼리 실행기는 반환된 튜플을 다시 검사하고, 쿼리 조건과 일치하지 않는 튜플을 폐기한다. (결과가 일치하지 않아 폐기된 인덱스는 손실된다.) BRIN 인덱스는 매우 작기 때문에 인덱스를 스캔하면 순차적 스캔에 비해 오버헤드가 거의 발생하지 않지만, 일치하는 튜플이 없는 것으로 알려진 테이블의 많은 부분을 스캔하는 것은 피할 수 있다.\nBRIN 인덱스가 저장할 특정 데이터는 인덱스의 각 열에 대해 선택된 연산자 유형에 따라서도 달라진다. 예를 들어 선형 정렬 순서를 갖는 데이터 유형은 각 블록 범위 내에서 최솟값과 최댓값을 저장할 수 있고, 기하학적 유형은 블록 범위의 모든 객체에 대한 경계 정보를 저장할 수도 있다.\n2. BRIN 인덱스 관리 Brin 인덱스가 생성될시, 모든 존재하는 heap page를 스캔하고, 각 block range마다 요약 인덱스 tuple을 생성하고 마지막으로 불완전한 block range를 생성한다. 새로운 page가 데이터로 가득 차면, 이미 요약된 block range가 새 튜플의 데이터로 요약 정보가 업데이트된다. 마지막 요약 범위에 속하지 않는 새 페이지가 생성되면 요약 튜플을 자동으로 획득하지 않고 나중에 요약 실행이 호출될 때까지 해당 튜플은 요약되지 않은 상태로 남아 해당 범위에 대한 초기 요약을 만든다. 이 과정을 직접 실행하는 몇 가지 방법이 있다. 테이블을 auto vacuum 하여 요약되지 않은 page ranges를 요약한다. 만약 auto summarize 파라미터가 on이라면(default 아님), autovacuum이 데이터베이스에 실행될 때마다 summarization이 실행된다.\n--요약 안된 전체 범위 요약 brin_summarize_new_values(regclass) --주어진 page만 요약 (요약안됐을 경우에만) brin_summarize_range(regclass, bigint) 을 통해 ranges에 summarization 실행 가능하다. 반대로 다음을 통해 요약을 해제 하는것도 가능하다.\nbrin_desummarize_range(regclass, bigint) tuple의 기존값이 변경되어 인덱스 tuple이 더 이상 좋은 결과를 나타내지 못할 때 유용하다. 3. BRIN VS B-TREE ▪ BRIN 인덱스는 B-TREE 인덱스보다 쿼리 성능이 좋다.\n▪ BRIN 인덱스는, B-TREE에서 사용하는 용량의 1%만 사용한다.\n▪ BRIN이 특정 블록 범위만 다루다 보니, 검색 범위를 이탈할 경우 해당하는 블록 범위 전체를 검사한다.\n▪ BRIN은 lossy index이므로, 데이터의 hash 값을 저장하는 컬럼에 BRIN을 써도 데이터가 포함된 블록을 정확히 반환하지 못한다.\n▪ 인덱스 생성 속도가 BRIN이 더 빠르다.\n4. 연산자 이름 인덱싱 된 데이터 유형 인덱싱 가능한 연산자 abstime_minmax_ops abstime \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; int8_minmax_ops bigint \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; bit_minmax_ops bit \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; varbit_minmax_ops bit varying \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; box_inclusion_ops box \u0026laquo; \u0026amp;\u0026lt; \u0026amp;\u0026amp; \u0026amp;\u0026gt; \u0026gt;\u0026gt; ~= @\u0026gt; \u0026lt;@ \u0026amp;\u0026lt;| \u0026laquo;| |\u0026raquo; |\u0026amp;\u0026gt; bytea_minmax_ops bytea \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; bpchar_minmax_ops character \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; char_minmax_ops \u0026ldquo;char\u0026rdquo; \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; date_minmax_ops date \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; float8_minmax_ops double precision \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; inet_minmax_ops inet \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; network_inclusion_ops inet \u0026amp;\u0026amp; \u0026gt;\u0026gt;= \u0026laquo;= = \u0026gt;\u0026gt; \u0026laquo; int4_minmax_ops integer \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; interval_minmax_ops interval \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; macaddr_minmax_ops macaddr \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; name_minmax_ops name \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; numeric_minmax_ops numeric \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; pg_lsn_minmax_ops pg_lsn \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; oid_minmax_ops oid \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; range_inclusion_ops any range type \u0026laquo; \u0026amp;\u0026lt; \u0026amp;\u0026amp; \u0026amp;\u0026gt; \u0026gt;\u0026gt; @\u0026gt; \u0026lt;@ -|- = \u0026lt; \u0026lt;= = \u0026gt; \u0026gt;= float4_minmax_ops real \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; reltime_minmax_ops reltime \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; int2_minmax_ops smallint \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; text_minmax_ops text \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; tid_minmax_ops tid \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; timestamp_minmax_ops timestamp without time zone \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; timestamptz_minmax_ops timestamp with time zone \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; time_minmax_ops time without time zone \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; timetz_minmax_ops time with time zone \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; uuid_minmax_ops uuid \u0026lt; \u0026lt;= = \u0026gt;= \u0026gt; 참고 :\nhttps://bajratech.github.io/2016/09/16/Postgres-BRIN-Index/\nhttps://www.postgresql.kr/docs/13/brin-intro.html\n","permalink":"http://localhost:50666/posts/11/","summary":"\u003chr\u003e\n\u003cp\u003e \u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/11/img.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"brin-인덱스란\" ke-size=\"size26\"\u003e\u003cstrong\u003e1. BRIN 인덱스란?\u003c/strong\u003e\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e▪ Block range index의 약자\u003cbr\u003e\n▪ Page 검색에 도움 되는 메타 데이터를 뽑아서 인덱스를 구성 (ex, 특정컬럼의 최대/최솟값)\u003cbr\u003e\n▪ 특정 컬럼이 물리 주소의 일정한 상관관계를 가지는 매우 큰 테이블을 다루기 위해 설계 (타임시쿼스한 대용량 데이터 조회에 유용)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eBlock range는 테이블 내에서 근접한 물리주소를 가진 page 그룹을 의미한다. 각 Block range 에 대해 일부 요약 정보가 인덱스로 저장된다. 예를 들어 상점의 판매 주문을 저장하는 테이블에는 각 주문이 배치된 날짜 열이 있을 수 있으며 대부분의 경우 이전 주문시점에 맞게 순차적으로 주문정보가 들어갈 것이고, ZIP 코드 열을 저장하는 테이블에는 도시에 대한 모든 코드가 자연스럽게 그룹화되어 있을 것이다.\u003c/p\u003e","title":"[PostgreSQL] BRIN 인덱스의 원리 및 특징"},{"content":" 1. GIN 인덱스란? Generalized Inverted Index의 약자이다. 이전 포스트인 full text search에서 사용하는 인덱스의 유형. 기본 구조는 B-tree와 유사하지만, 저장 형태가 다르다. 저장된 요소 자제에 대한 검색이 아닌 인덱스 컬럼의 값을 split 한 token인 lexeme 배열에 대해서 검색을 한다. array_ops, tsvector_ops, jsonb_ops, jsonb_path_ops 등 의 built-in operators를 통해 접근이 가능하다.\n2. full text search에서의 적용 2-1. 샘플 테이블 및 데이터 생성 create table ts(doc text, doc_tsv tsvector); insert into ts(doc) values (\u0026#39;Can a sheet slitter slit sheets?\u0026#39;), (\u0026#39;How many sheets could a sheet slitter slit?\u0026#39;), (\u0026#39;I slit a sheet, a sheet I slit.\u0026#39;), (\u0026#39;Upon a slitted sheet I sit.\u0026#39;), (\u0026#39;Whoever slit the sheets is a good sheet slitter.\u0026#39;), (\u0026#39;I am a sheet slitter.\u0026#39;), (\u0026#39;I slit sheets.\u0026#39;), (\u0026#39;I am the sleekest sheet slitter that ever slit sheets.\u0026#39;), (\u0026#39;She slits the sheet she sits on.\u0026#39;); update ts set doc_tsv = to_tsvector(doc); create index on ts using gin(doc_tsv); select doc from ts where doc_tsv @@ to_tsquery(\u0026#39;many \u0026amp; slitter\u0026#39;); 2-2. 조회 결과 및 플랜 확인 QUERY PLAN --------------------------------------------------------------------- Bitmap Heap Scan on ts Recheck Cond: (doc_tsv @@ to_tsquery(\u0026#39;many \u0026amp; slitter\u0026#39;::text)) -\u0026gt; Bitmap Index Scan on ts_doc_tsv_idx Index Cond: (doc_tsv @@ to_tsquery(\u0026#39;many \u0026amp; slitter\u0026#39;::text)) (4 rows) doc --------------------------------------------- How many sheets could a sheet slitter slit? (1 row) 2-3. 작동 방식 ▪ 먼저 쿼리에서 검색에 사용할 lexeme인 'many'와 'slitter'를 추출한다. ▪ lexeme B-tree에서 2개의 키를 동시에 찾는다.\nmani = (0,2)\\ slitter = (0,1),(0,2),(1,2),(1,3),(2,2) ▪ 마지막으로, 발견된 TID각각에 대해 검색 쿼리에 부합하는지 확인한다. (예제의 쿼리의 경우 and 조건이기에 (0,2)에 해당하는 TID만 리턴하게 된다.) 3. 특징 ▪ GIN의 업데이트는 매우 느리다. document는 보통 많은 lexeme을 포함하고, 1개의 document가 업데이트되거나 추가된다고 해도 인덱스 트리 내에서는 많은 업데이트가 진행된다.\n▪ 반면에, 몇몇의 document가 동시에 업데이트된다면, 중복되는 Lexeme들이 존재할 것이고, 총 인덱스 업데이트량은 개별 업데이트 시보다 줄어들 것이다.\n▪ GIN인덱스의 또 하나의 특징은 항상 결과를 bitmap으로 리턴한다는 것이다. (TID 자체로 리턴하지 않는다.) 그렇기 때문에 Limit을 통한 결괏값 제한은 그렇게 효율적이지 않다.\n▪ full text search, array, json 등의 타입 조회에 효율적이다. 참고 : https://postgrespro.com/blog/pgsql/4261647 ","permalink":"http://localhost:50666/posts/10/","summary":"\u003chr\u003e\n\u003ch2 id=\"1-gin-인덱스란\"\u003e1. GIN 인덱스란?\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eGeneralized Inverted Index의 약자이다. 이전 포스트인 full text search에서 사용하는 인덱스의 유형. 기본 구조는 B-tree와 유사하지만, 저장 형태가 다르다.  저장된 요소 자제에 대한 검색이 아닌 인덱스 컬럼의 값을 split 한 token인 lexeme 배열에 대해서 검색을 한다. array_ops, tsvector_ops, jsonb_ops, jsonb_path_ops 등 의 built-in operators를 통해 접근이 가능하다.\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch2 id=\"2-full-text-search에서의-적용\"\u003e2. full text search에서의 적용\u003c/h2\u003e\n\u003ch3 id=\"2-1-샘플-테이블-및-데이터-생성\"\u003e2-1. 샘플 테이블 및 데이터 생성\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ecreate table ts(doc text, doc_tsv tsvector);\n\ninsert into ts(doc) values\n  (\u0026#39;Can a sheet slitter slit sheets?\u0026#39;), \n  (\u0026#39;How many sheets could a sheet slitter slit?\u0026#39;),\n  (\u0026#39;I slit a sheet, a sheet I slit.\u0026#39;),\n  (\u0026#39;Upon a slitted sheet I sit.\u0026#39;), \n  (\u0026#39;Whoever slit the sheets is a good sheet slitter.\u0026#39;), \n  (\u0026#39;I am a sheet slitter.\u0026#39;),\n  (\u0026#39;I slit sheets.\u0026#39;),\n  (\u0026#39;I am the sleekest sheet slitter that ever slit sheets.\u0026#39;),\n  (\u0026#39;She slits the sheet she sits on.\u0026#39;);\n\nupdate ts set doc_tsv = to_tsvector(doc);\ncreate index on ts using gin(doc_tsv);\n\nselect doc from ts where doc_tsv @@ to_tsquery(\u0026#39;many \u0026amp; slitter\u0026#39;);\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"조회-결과-및-플랜-확인\" ke-size=\"size23\"\u003e2-2. 조회 결과 및 플랜 확인\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e                             QUERY PLAN                              \n---------------------------------------------------------------------\n Bitmap Heap Scan on ts\n   Recheck Cond: (doc_tsv @@ to_tsquery(\u0026#39;many \u0026amp; slitter\u0026#39;::text))\n   -\u0026gt;  Bitmap Index Scan on ts_doc_tsv_idx\n         Index Cond: (doc_tsv @@ to_tsquery(\u0026#39;many \u0026amp; slitter\u0026#39;::text))\n(4 rows)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e \u003c/p\u003e","title":"[PostgreSQL] GIN인덱스의 원리 및 특징"},{"content":" 1. SP-GiST 인덱스란? Space-Partitioned Generalized Search Tree의 약자이다. GiST인덱스와 같이 지리, 좌표, ip주소 데이터 등 복잡한 유형의 데이터를 처리하는 인덱스 유형이다. GiST가 B-tree 인덱스를 통해 보관 데이터를 세분화할 때, 위계적 순서를 따라야 하기에, 이를 보완하기 위해 만들어진 유형으로, GiST로 분리된 공간을 다시 한번 공간 단위로 나누어 관리하는 개념이다. SP-GiST는 겹치지 않는 영역으로 재귀적 분할을 할 수 있는 구조에 적합하다. 기본적으로 SP-GiST는 다양한 데이터 유형, 복잡한 쿼리를 지원하도록 설계되었다.\n1-1. SP-GiST 인덱스 생성 CREATE INDEX idx_spgist_example ON example_table USING spgist (column1); 1-2. 장점 다양한 종류의 데이터 타입에 사용 가능 : 기하학, IP, 다른 복잡한 데이터 타입\n복잡한 쿼리에 사용 가능 : 복잡한 데이터구조, 쿼리에 사용 적합하도록 설계 빠른 검색 효율\n1-3. 단점 복잡한 구현 방법 : btree/hash 에 비해 구현이 복잡하다.\n느린 업데이트 : SP-GiST index는 업데이트가 느리다, 복잡한 알고리즘인 만큼 특정 데이터 변경시 인덱스의 업데이트가 느리다. 한정된 쿼리 유형 : 복잡한 유형의 쿼리에 특화 되어있다보니 =, \u0026lt;등 간단한 타입의 비교에는 고려되지 않을 수 있다.\n1-4. 그렇다면 SP-GiST는 GiST인덱스와 어떻게 다를까? ▪ GiST에 비해 SP-GiST를 지원하는 operator가 적다. (SP-GiST 지원 Operator는 아래에서 확인가능) ▪ GiST에 비해 SP-GiST를 지원하는 operator가 적다. (SP-GiST 지원 Operator는 아래에서 확인가능)\n(GiST는 (k) NN searches를 포함한 모든 operator 지원을 받는다.)\n▪ 인덱스 생성 시간 ▪ GiST 인덱스의 생성시간은 데이터 증가에 따라 비선형적이지만 안정적으로 증가한다.\n▪ SP-GiST 인덱스는 적은 데이터일 경우 빠르지만, 몇 억 건이 넘어갈 경우 GiST에 비해 현저히 떨어지는 속도를 보인다.\n▪ 데이터 밀집도에 따른 효율성 ▪ GIST는 기하학적 구조의 공간 분포와 토폴로지에 크게 민감하지 않다.\n▪ SP_GIST는 공간 분할(Spatial Partitioning)로 인해 중첩되지 않는 지오메트리에 가장 효과적이며 공간적으로 균일한 분포에 대한 검색에 효율적이다.\n데이터 사이즈, 구조, 사용하는 쿼리 등에 따라 인덱스의 효율성이 달라질 수 있어, 실제 데이터로 GiST, SP-GiST의 성능테스트가 꼭 필요하다.\n2. 지도 / 좌표 형태의 데이터 인덱싱 다음과 같이 위도/경도 데이터로 조회를 시도할 시 효율적이다\nSELECT city_name FROM locations WHERE ST_DWithin(ST_MakePoint(:longitude, :latitude), ST_MakePoint(longitude, latitude), :distance); 지도의4분 할로 지속적으로 나눈다. 각각의 사각형이 index page 역할을 한다\n나눠진 부분을 좀 더 상세히 보면\n다음 좌표에서 (2,7) 위에 존재하는 좌표들을 찾고 싶다면 select * from points where p \u0026gt;^ point \u0026#39;(2,7)\u0026#39; (4,4)를 (2,7)과 비교하여 더 큰 좌표가 존재할 수 있는 영역을 확인한다.\n1 사분면의 중심좌표인 (6,6)으로 다시 비교하여 더 큰 좌표가 존재할 수 있는 영역을 확인한 후 다음과 같은 인덱스 구조를 생성한다.\n3. Built-in Operator Class Name Indexable Operators Ordering Operators box_ops \u0026laquo; (box,box)\n\u0026amp;\u0026lt; (box,box)\n\u0026amp;\u0026gt; (box,box)\n\u0026raquo; (box,box)\n\u0026lt;@ (box,box)\n@\u0026gt; (box,box)\n~= (box,box)\n\u0026amp;\u0026amp; (box,box)\n\u0026laquo;| (box,box)\n\u0026amp;\u0026lt;| (box,box)\n|\u0026amp;\u0026gt; (box,box)\n|\u0026raquo; (box,box) \u0026lt;-\u0026gt; (box,point) inet_ops \u0026laquo; (inet,inet)\n\u0026laquo;= (inet,inet)\n\u0026raquo; (inet,inet)\n\u0026raquo;= (inet,inet)\n= (inet,inet)\n\u0026lt;\u0026gt; (inet,inet)\n\u0026lt; (inet,inet)\n\u0026lt;= (inet,inet)\n\u0026gt; (inet,inet)\n\u0026gt;= (inet,inet)\n\u0026amp;\u0026amp; (inet,inet) kd_point_ops |\u0026raquo; (point,point)\n\u0026laquo; (point,point)\n\u0026raquo; (point,point)\n\u0026laquo;| (point,point)\n~= (point,point)\n\u0026lt;@ (point,box) \u0026lt;-\u0026gt; (point,point) poly_ops \u0026laquo; (polygon,polygon)\n\u0026amp;\u0026lt; (polygon,polygon)\n\u0026amp;\u0026gt; (polygon,polygon)\n\u0026raquo; (polygon,polygon)\n\u0026lt;@ (polygon,polygon)\n@\u0026gt; (polygon,polygon)\n~= (polygon,polygon)\n\u0026amp;\u0026amp; (polygon,polygon)\n\u0026laquo;| (polygon,polygon)\n\u0026amp;\u0026lt;| (polygon,polygon)\n|\u0026raquo; (polygon,polygon)\n|\u0026amp;\u0026gt; (polygon,polygon) \u0026lt;-\u0026gt; (polygon,point) quad_point_ops |\u0026raquo; (point,point)\n\u0026laquo; (point,point)\n\u0026raquo; (point,point)\n\u0026laquo;| (point,point)\n~= (point,point)\n\u0026lt;@ (point,box) \u0026lt;-\u0026gt; (point,point) range_ops = (anyrange,anyrange)\n\u0026amp;\u0026amp; (anyrange,anyrange)\n@\u0026gt; (anyrange,anyelement)\n@\u0026gt; (anyrange,anyrange)\n\u0026lt;@ (anyrange,anyrange)\n\u0026laquo; (anyrange,anyrange)\n\u0026raquo; (anyrange,anyrange)\n\u0026amp;\u0026lt; (anyrange,anyrange)\n\u0026amp;\u0026gt; (anyrange,anyrange)\n-|- (anyrange,anyrange) text_ops = (text,text)\n\u0026lt; (text,text)\n\u0026lt;= (text,text)\n\u0026gt; (text,text)\n\u0026gt;= (text,text)\n\u0026lt; (text,text)\n\u0026lt;= (text,text)\n\u0026gt;= (text,text)\n\u0026gt; (text,text)\n^@ (text,text) 참고\nhttps://www.postgresql.org/docs/current/spgist-builtin-opclasses.html\nhttps://gis.stackexchange.com/questions/374091/when-to-use-gist-and-when-to-use-sp-gist-index\n","permalink":"http://localhost:50666/posts/9/","summary":"\u003chr\u003e\n\u003ch2 id=\"sp-gist-인덱스란\" ke-size=\"size26\"\u003e1. SP-GiST 인덱스란?\u003c/h2\u003e\n\u003cp\u003eSpace-Partitioned Generalized Search Tree의 약자이다. GiST인덱스와 같이 지리, 좌표, ip주소 데이터 등 복잡한 유형의 데이터를 처리하는 인덱스 유형이다. GiST가 B-tree 인덱스를 통해 보관 데이터를 세분화할 때, 위계적 순서를 따라야 하기에, 이를 보완하기 위해 만들어진 유형으로, GiST로 분리된 공간을 다시 한번 공간 단위로 나누어 관리하는 개념이다. SP-GiST는 겹치지 않는 영역으로 재귀적 분할을 할 수 있는 구조에 적합하다. 기본적으로 SP-GiST는 다양한 데이터 유형, 복잡한 쿼리를 지원하도록 설계되었다.\u003c/p\u003e\n\u003ch3 id=\"sp-gist-인덱스-생성\" style=\"color: #000000; text-align: start;\" ke-size=\"size23\"\u003e1-1. SP-GiST 인덱스 생성\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE INDEX idx_spgist_example ON example_table USING spgist (column1);\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"장점\" ke-size=\"size23\"\u003e1-2. 장점\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e다양한 종류의 데이터 타입에 사용 가능\u003c/strong\u003e : 기하학, IP, 다른 복잡한 데이터 타입\u003cbr\u003e\n\u003cstrong\u003e복잡한 쿼리에 사용 가능\u003c/strong\u003e : 복잡한 데이터구조, 쿼리에 사용 적합하도록 설계\n\u003cstrong\u003e빠른 검색 효율\u003c/strong\u003e\u003c/p\u003e","title":"[PostgreSQL] SP-GiST인덱스의 원리 및 특징"},{"content":" 1. GiST 인덱스란? Generalized Search Tree의 약자이며 B-tree와 같은 balanced search tree의 형태이다. B-tree인덱스는 정렬된 채로 비교\u0026amp;일치의 연산에 최적화된 채로 연결되어있다. 하지만 현대의 다양한 데이터 종류 (기하학적, 텍스트문서, 이미지 등)를 연산하는 데는 적합하지 않다.\nGiST 인덱스는 이러한 데이터 타입의 인덱싱을 위해 설계되었다. GiST 인덱스는 각 유형의 데이터를 Balanced tree 형태로 구성하게하고, tree에 접근하는 연산자를 정의해 준다. 각각 leaf node는 table row(TID)와 boolean 형태의 predicate를 가지고 있고 인덱스 데이터(key)는 이 predicate와 부합한다. 그 후는 일반적인 tree search처럼, 루트노드에서 시작하여, 어떤 child node로 진입할지를 결정한다. 그러다가 leaf node를 발견하면, 그 결과들을 반환한다.\nSELECT city_name FROM locations WHERE ST_DWithin(ST_MakePoint(:longitude, :latitude), ST_MakePoint(longitude, latitude), :distance); 그렇기에 다음과 같은 위/경도를 통한 위치정보의 검색에 효율적인 인덱스이다.\n▪ 지도(좌표)형태의 데이터 인덱싱 지도, 좌표 형태의 데이터에 유용하며 지도의 특정 분포지점을 사각형으로 계속해서 분할한다. 각각의 사각형이 index page 역할을 한다.\nROOT page 는 몇몇 최대로 큰 사각형을 보유하고 있으며, child nodes는 큰 사각형들에 포함된 작은 사각형들을 포함하고 있어 모든 point들을 커버할 수 있도록 구성된다.\n좀 더 자세히 들여다보자\n다음 좌표를 보면 (2,1)-(7,4) 사각형은 (1-1)-(6,3)과 겹치지만 (5-5), (8-8) 사각형과는 겹치지 않는다, 그렇기 때문에 다음과 같은 방법으로 하위 leaf를 결정하게된다. 참고 : https://postgrespro.com/blog/pgsql/4175817 ","permalink":"http://localhost:50666/posts/8/","summary":"\u003chr\u003e\n\u003ch2 id=\"gist-인덱스란\" ke-size=\"size26\"\u003e1. GiST 인덱스란?\u003c/h2\u003e\n\u003cp\u003eGeneralized Search Tree의 약자이며 B-tree와 같은 balanced search tree의 형태이다. B-tree인덱스는 정렬된 채로 비교\u0026amp;일치의 연산에 최적화된 채로 연결되어있다. 하지만 현대의 다양한 데이터 종류 (기하학적, 텍스트문서, 이미지 등)를 연산하는 데는 적합하지 않다.\u003c/p\u003e\n\u003cp\u003eGiST 인덱스는 이러한 데이터 타입의 인덱싱을 위해 설계되었다. GiST 인덱스는 각 유형의 데이터를 Balanced tree 형태로 구성하게하고, tree에 접근하는 연산자를 정의해 준다. 각각 leaf node는 table row(TID)와 boolean 형태의 predicate를 가지고 있고 인덱스 데이터(key)는 이 predicate와 부합한다. 그 후는 일반적인 tree search처럼, 루트노드에서 시작하여, 어떤 child node로 진입할지를 결정한다. 그러다가 leaf node를 발견하면, 그 결과들을 반환한다.\u003c/p\u003e","title":"[PostgreSQL] GiST인덱스의 원리 및 특징"},{"content":" 1. Hash 인덱스란? 해쉬 인덱스의 기본 아이디어는, hash function을 통해 작은 숫자를 데이터와 조합하여 integer 형태의 해쉬값 (최대 2^32 = 4B)을 생성하고 해쉬값을 테이블 행 정보(TID)가 저장될 배열의 인덱스 값으로 사용하는 것이다. 이 배열의 각 요소를 해시 테이블 버킷(hash table bucket)이라고 한다. 데이터 조회 시, hash function을 통해 생성된 key가 포함된 bucket을 찾고, 그 bucket만 확인하면 실제 데이터의 위치를 바로 확인할 수 있다. 데이터의 크기에 상관없이 인덱스의 크기가 작고 검색이 빠르다. 1개의 데이터를 조회하는 시간은 O(1)로 빠르지만 해쉬 테이블 내의 값들은 정렬이 되어있지 않기 때문에 범위 비교나 부정형 비교가 포함된 조건에서는 인덱스를 사용할 수 없다. Hash function이 버킷 단위로 소스 값을 더 균일하게 분배할수록 효율이 좋다. ▪ Collision 그러나 아무리 좋은 해시 함수라도 다른 키 값에 대해 동일한 해쉬값을 리턴하는 경우가 있고 이런 경우를 \u0026ldquo;충돌(collision)\u0026ldquo;이라고 한다. 하나의 버킷은 서로 다른 TID를 저장할 수 있으므로 인덱스에서 얻은 실제 값을 재확인하여야 한다. 예를들어, 256개의 버킷이 있다면, 모든 문자열의 첫 글자로 해쉬값을 생성할 수 있다. 이 경우 동일한 문자로 시작하는 모든 문자열이 동일한 해쉬값을 가진 채로 한 버킷으로 들어갈 것이고, 실제 버킷에서 값이 찾아진 후에도 TID의 검증이 계속 필요할 것이며, 해싱의 의미가 없어질 것이다. 이를 방지하기위해 Hash function은 hashcode - TID 쌍을 순차적으로 저장하여 효율적으로 bucket 내 동일 hash codes들 중에 정확히 일치하는 TID만을 찾는다.\n2. Hash 인덱스 구조 Meta page - Root, 첫 번째 page, 인덱스 정보 포함 Bucket pages - 인덱스의 메인 page, \u0026ldquo;hash code - TID\u0026rdquo; 쌍으로 데이터를 저장 Overflow pages - bucket pages와 동일한 구성, 한 page가 버킷에 부족할 때 사용됨 Bitmap pages - overflow pages를 계속해서 추적하며 clear 한 지, 다른 버킷에서 재사용 가능한지 확인한다 해쉬 인덱스의 크기는 줄어들지 않는다. 인덱스 열을 삭제하여도 한번 할당된 bucket은 os로 반환되지 않는다. Vacuuming, reindexing을 통해 전체 삭제 후 처음부터 다시 인덱싱은 가능하다.\n▪ 결론 Hash 인덱스는 단일 값 검색에서는 효율적인 성능을 보인다. (B-Tree에 비해 작은 인덱스 size, O(1)) 범위, 부정형 조건에 대한 검색에서는 사용 불가하다. 한번 생성된 버킷은 인덱스를 제거하여도 초기화[ ]{style=\u0026ldquo;color: #333333; text-align: left;\u0026quot;}전에는 메모리에 반환되지 않는다. 참고 : https://postgrespro.com/blog/pgsql/4161321\n","permalink":"http://localhost:50666/posts/7/","summary":"\u003chr\u003e\n\u003cp\u003e \u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/7/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"hash-인덱스란\" ke-size=\"size26\"\u003e1. Hash 인덱스란?\u003c/h2\u003e\n\u003cp\u003e해쉬 인덱스의 기본 아이디어는, hash function을 통해 작은 숫자를 데이터와 조합하여 integer 형태의 해쉬값 (최대 2^32 = 4B)을 생성하고 해쉬값을 테이블 행 정보(TID)가 저장될 배열의 인덱스 값으로 사용하는 것이다. 이 배열의 각 요소를 해시 테이블 버킷(hash table bucket)이라고 한다. 데이터 조회 시, hash function을 통해 생성된 key가 포함된 bucket을 찾고, 그 bucket만 확인하면 실제 데이터의 위치를 바로 확인할 수 있다. 데이터의 크기에 상관없이 인덱스의 크기가 작고 검색이 빠르다. 1개의 데이터를 조회하는 시간은 O(1)로 빠르지만 해쉬 테이블 내의 값들은 정렬이 되어있지 않기 때문에 범위 비교나 부정형 비교가 포함된 조건에서는 인덱스를 사용할 수 없다. Hash function이 버킷 단위로 소스 값을 더 균일하게 분배할수록 효율이 좋다. \u003c/p\u003e","title":"[PostgreSQL] Hash 인덱스의 원리 및 특징"},{"content":" PostgreSQL에는 6가지의 인덱스 종류가 있다. 각각의 인덱스는 다양한 데이터 탐색을 위해 다른 알고리즘을 사용한다.\n그중 가장 일반적으로 사용되고, 가장 먼저 도입된 알고리즘인 B-tree 인덱스에 대해 알아보자.\n1. B-tree 인덱스란? ▪ 트리의 노드를 밸런스 있게 재정렬한 트리형태의 자료구조\n▪ B-tree는 Binary 가 아닌 Balanced의 약자\n▪ 컬럼의 기존 데이터를 변형하지 않음\n▪ 인덱스 구조체 내에서는 항상 정렬된 상태를 유지\n2. B-tree 인덱스의 원리 ▪ B-tree 인덱스의 자료구조 형태 최상위 Root를 Meta page, 최하위 노드를 Leaf page라고 한다. Root page에서 leaf page들 간의 내부 page 수가 항상 같기에, 어떤 value를 검색하여도 동일한 시간이 걸린다. 데이터는 non-decreasing order로 정렬되어 있고, 동일 레벨의 page들끼리는 양방향으로 연결되어 있기에 (ex. 그림의 25 \u0026lt;-\u0026gt;32) 순차적 데이터를 root를 확인할 필요 없이 한 번에 찾을 수 있다.\n▪ 정확히 일치하는 데이터를 찾기 위해 데이터를 탐색하는 순서 (\u0026quot;indexed-field = *expression\u0026quot;) 인덱스 컬럼에 동일 데이터가 너무 많이 분포되어있어 한 page에 넘치게 데이터가 들어있을 수 있기에, 내부 페이지에서 정확히 일치하는 page를 찾게 되면 왼쪽으로 한 page를 이동하여 left to right 방향으로 인덱스를 조회한다.\n▪ 불일치에 해당하는 데이터를 찾기 위해 데이터를 탐색하는 순서 (\u0026quot;indexed-field ≤ expression\u0026quot; (or \u0026quot;indexed-field ≥ expression\u0026quot;)), 다음의 경우, 먼저 일치하는 leaf page 를 찾은 후 leaf page 간의 이동으로 조회한다.\n▪ 범위로 조회할 경우 데이터를 탐색하는 순서 (\u0026quot;indexed-field ≤ expression\u0026quot; and \u0026quot;indexed-field ≥ expression\u0026quot;), A\u0026lt; indexed-field \u0026lt; B 일경우 A에 해당하는 leaf page를 찾은 후 B의 조건에 맞는 범주 내의 데이터까지 leaf page 간의 이동으로 조회한다. 혹은 반대 방향으로, B부터 조건에 맞는 데이터 범주까지 leaf page 간 이동하여 조회한다. 3. B-tree 인덱스의 특징 인덱스 자체의 특징 및 설계방법은 이전 포스트 (https://junhkang.tistory.com/5) 에서 다루고 있으니 참고, B-tree 인덱스만의 특징은 다음과 같다.\n▪ 데이터가 동일한지, 특정 범주에 있는 데이터인지 여부로 조회할 때 사용\n▪ Postgresql 옵티마이저는 \u0026ldquo;\u0026lt; \u0026lt;= = \u0026gt;= \u0026gt;\u0026ldquo;와 같은 수식을 사용할때 B-tree 인덱스를 고려 ▪ 수식의 조합인 IN\u0026amp;\u0026amp; Between도 고려\n▪ IS NULL, IS NOT NULL 조건도 인덱스 사용 가능\n▪ 패턴매칭 (like)도 문자열의 시작에 매칭을 사용하지 않으면 적용 가능 ( \u0026rsquo;test%\u0026rsquo;, \u0026rsquo;test^\u0026rsquo;)\n▪ Ilike, ~* 에는 패턴이 알파벳 외의 문자(대/소문자 영향을 안 받는 문자로 시작하는 경우)로 시작할 때만 사용 가능 참고\nhttps://postgrespro.com/blog/pgsql/4161516 ","permalink":"http://localhost:50666/posts/6/","summary":"\u003chr\u003e\n\u003cp\u003ePostgreSQL에는 6가지의 인덱스 종류가 있다. 각각의 인덱스는 다양한 데이터 탐색을 위해 다른 알고리즘을 사용한다.\u003c/p\u003e\n\u003cp\u003e그중 가장 일반적으로 사용되고, 가장 먼저 도입된 알고리즘인 B-tree 인덱스에 대해 알아보자.\u003c/p\u003e\n\u003ch2 id=\"b-tree-인덱스란\" ke-size=\"size26\"\u003e1. B-tree 인덱스란?\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e▪ 트리의 노드를 밸런스 있게 재정렬한 트리형태의 자료구조\u003cbr\u003e\n▪ B-tree는 Binary 가 아닌 Balanced의 약자\u003cbr\u003e\n▪ 컬럼의 기존 데이터를 변형하지 않음\u003cbr\u003e\n▪ 인덱스 구조체 내에서는 항상 정렬된 상태를 유지\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch2 id=\"b-tree-인덱스의-원리\" ke-size=\"size26\"\u003e2. B-tree 인덱스의 원리\u003c/h2\u003e\n\u003ch3 id=\"b-tree-인덱스의-자료구조-형태\" ke-size=\"size23\"\u003e▪ B-tree 인덱스의 자료구조 형태\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/6/img.png\"\u003e\u003c/p\u003e","title":"[PostgreSQL] B-tree 인덱스의 원리 및 특징"},{"content":" 1. 인덱스 컨트롤 1-1. 인덱스 조회 SELECT * FROM pg_indexes WHERE tablename = \u0026#39;{테이블명}\u0026#39;; -- 테이블명에 \u0026#39;\u0026#39; 필요 1-2. 인덱스 생성 -- 단일 인덱스 CREATE INDEX {인덱스명} ON {테이블명} USING btree({컬럼명}); -- 결합 인덱스 CREATE INDEX {인덱스명} ON {테이블명} USING btree({컬럼명1}, {컬럼명2}); -- 유니크 인덱스 CREATE UNIQUE INDEX {인덱스명} ON table_name ({컬럼명}); 1-3. 인덱스 삭제 DROP INDEX {인덱스명}; 1-4. 인덱스 사용 빈도 확인 SELECT schemaname, relname, indexrelname, idx_scan as idx_scan_cnt FROM pg_stat_user_indexes ORDER BY idx_scan; 1-5. 인덱스 손상 시 재인덱싱 REINDEX INDEX {인덱스명} REINDEX TABLE {테이블명} REINDEX DATABASE {데이터베이스명} 2. 인덱스 란? 데이터에 색인을 통해 데이터의 위치를 빠르게 찾아주는 역할을 한다. 인덱스 설정 없이는 Seq 스캔을 통해 테이블 전체를 조회하기에 검색 성능이 저하된다.\n2-1. 테이블 스캔 방식 Postgresql은 seq scan, index scan, bitmap index scan, index only scan, tid scan 5가지 스캔 방식을 사용한다. 그중 2가지를 확인해 보면,\n▪ Seq Scan 방식 - Seq Scan은 테이블을 Full Scan 하여 전체 데이터를 조회한다.\\\n인덱스가 존재하지 않거나, 인덱스가 존재하더라도 읽어야 할 범위가 넓은 경우에 선택된다. ▪ Index Scan 방식 - Index Scan은 인덱스 Leaf 블록에 저장된 키를 이용해서 테이블 레코드를 액세스 하는 방식이다.\\\n레코드 정렬 상태에 따라서 테이블 블록 액세스 횟수가 크게 차이 난다. 다음과 같이, 인덱스를 사용할 경우 테이블 레코드에 효과적인 접근이 가능하다. 하지만 select 성능은 올라가지만, update, insert, delete시 index 색인정보 갱신을 하기에 시간이 더 소모된다.\n2-2. 인덱스가 적용되지 않는 경우 ▪ order by {인덱스칼럼 1}, {칼럼 2} : 복수 키에 대해 order by 하는 경우 (order by col1, col2 자체를 인덱스 설정하면 적용가능)\n▪ where {칼럼 1} = \u0026lsquo;x\u0026rsquo; order by {인덱스칼럼 2} : 조건절에 의하여 연속적이지 않게 된 컬럼에 대한 order by\n▪ order by {인덱스컬럼1} desc, {인덱스컬럼2} asc : desc와 asc의 결합사용\n▪ group by {인덱스컬럼1} order by {인덱스컬럼2} : group by, order by 컬럼이 서로 다른 경우\n▪ order by abs({인덱스컬럼1}) : 형 변환 이후의 order by, group by 인 경우\n2-3. 인덱스 설계 방법 ▪ 명확한 이유를 가진 인덱스만 설계 (많을수록 좋은 게 아니다.)\n▪ 조회 쿼리들을 전체 확인 후 자주 사용하는 컬럼\n▪ 고유 값 위주의 설계\n▪ Cardinality가 높을수록 효율적 (=컬럼별 중복도가 낮을수록 좋다)\n▪ 인덱스 key의 크기는 되도록 작게 설계\n▪ PK, join의 대상이 되는 컬럼에 설계\n▪ 단일 인덱스 여러 개 \u0026lt; 복합인덱스 고려\n▪ Update, delete가 빈번하지 않은 컬럼\n▪ join의 대상이 자주 되는 컬럼\n▪ 인덱스를 생성할 때 가장 효율적인 자료형은 정수형 자료 (가변적 데이터는 비효율적)\n▪ 선택도가 낮을수록 효율적 (10~15%)\n▪ 선택도는 데이터에서 특정 값을 얼마나 잘 선택할 수 있는지에 대한 지표\n(특정 필드값을 지정했을 때 선택되는 레코드 수를 테이블 전체 레코드 수로 나눈 수치) = 컬럼의 특정 값의 row 수 / 테이블의 총 row 수 * 100= 컬럼의 값들의 평균 row 수 / 테이블의 총 row 수 * 100 2-4. 다중 컬럼 인덱스 다중 컬럼 인덱스는 두개 이상의 필드를 조합해서 생성한 인덱스이다. 다중 컬럼 인덱스는 단일 컬럼 인덱스 보다 더 비효율적으로 INDEX/UPDATE/DELETE를 수행하기 때문에 생성에 신중해야 한다. (가급적 UPDATE가 안되는 값을 선정해야 한다). 조건절에 여러개의 조건이 있을시, 선행되는 조건과 이를 만족하는 후행되는 조건들을 차례로 함께 INDEX해서 사용한다. 2-4-1. 다중 컬럼 인덱스 설계 방법 ▪ 다중 컬럼 인덱스 구성시 컬럼의 순서는, IO를 적게 발생시키는 순으로 구성하여야 한다.\n(선행 인덱스에서 더 많은 데이터가 필터링될수록 이후 인덱스의 I/O가 덜 소모된다.)\n▪ 인덱스 컬럼을 합쳐 색인하기에 선행 인덱스 컬럼이 조건에 있어야 한다.\n예를 들어\nCREATE INDEX idx_user_sample ON user USING btree(first_name, address ); user 테이블에 first_name, address 두컬럼을 대상으로 다중 컬럼 인덱스를 부여한 후,\nSELECT * from WHERE first_name = ‘Jun’ AND address = ‘Seoul’ 으로 테이블을 조회할 경우 \u0026lsquo;JunSeoul\u0026rsquo;에 대한 인덱스를 찾고,\nB-Tree 인덱스의 left to right 특성대로, address 만으로는 인덱스를 사용할 수 없다. (선행되는 조건절인 first_name에 대한 조건에 부합하는 데이터를 기준으로 인덱싱이 되어있다.) 또한 CREATE INDEX idx_user_sample2 ON user USING btree(first_name, last_name, address ); first name, last name, address로 다중 컬럼 인덱스를 설정할 경우 다음과 같이 인덱스가 사용된다.\n-- 인덱스 사용 SELECT * from WHERE first_name = ‘Jun’ AND last_name = ‘H’ AND address = ‘Seoul’ -- 인덱스 미사용 SELECT * from WHERE first_name = ‘Jun’ AND address = ‘Seoul’ 결론적으로 다중 컬럼 인덱스의 성능은 절대적인 것이 아닌, 어떤 데이터를 조회하는지, 쿼리를 어떻게 작성하는지에 따라 크게 달라질 수 있기에 확실한 쿼리 플랜 분석 및 설계가 필요하다. [PostgreSQL] B-tree 인덱스의 원리 및 특징 [PostgreSQL] Hash 인덱스의 원리 및 특징 [PostgreSQL] GiST인덱스의 원리 및 특징 [PostgreSQL] SP-GiST인덱스의 원리 및 특징 [PostgreSQL] GIN인덱스의 원리 및 특징 [PostgreSQL] BRIN 인덱스의 원리 및 특징 ","permalink":"http://localhost:50666/posts/5/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/5/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"인덱스-컨트롤\" ke-size=\"size26\"\u003e1. 인덱스 컨트롤\u003c/h2\u003e\n\u003ch3 id=\"인덱스-조회\" ke-size=\"size23\"\u003e1-1. 인덱스 조회 \u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eSELECT * FROM pg_indexes WHERE tablename = \u0026#39;{테이블명}\u0026#39;; -- 테이블명에 \u0026#39;\u0026#39; 필요\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"인덱스-생성\" ke-size=\"size23\"\u003e1-2. 인덱스 생성 \u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e-- 단일 인덱스\nCREATE INDEX {인덱스명} ON {테이블명} USING btree({컬럼명});\n\n-- 결합 인덱스\nCREATE INDEX {인덱스명} ON {테이블명} USING btree({컬럼명1}, {컬럼명2});\n\n-- 유니크 인덱스\nCREATE UNIQUE INDEX {인덱스명} ON table_name ({컬럼명});\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"인덱스-삭제\" ke-size=\"size23\"\u003e1-3. 인덱스 삭제\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eDROP INDEX {인덱스명};\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"인덱스-사용-빈도-확인\" ke-size=\"size23\"\u003e1-4. 인덱스 사용 빈도 확인\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eSELECT schemaname, relname, indexrelname, idx_scan as idx_scan_cnt FROM pg_stat_user_indexes ORDER BY idx_scan;\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"인덱스-손상-시-재인덱싱\" ke-size=\"size23\"\u003e1-5. 인덱스 손상 시 재인덱싱\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eREINDEX INDEX {인덱스명}\n\nREINDEX TABLE {테이블명}\n\nREINDEX DATABASE {데이터베이스명}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e \u003c/p\u003e","title":"[PostgreSQL]  인덱스(INDEX)개념 및 생성, 삭제, 분석, 설계 방법"},{"content":" 1. Lock 확인방법 SELECT PSAT.RELNAME, PL.LOCKTYPE, PL.PID, PL.MODE, PL.GRANTED FROM PG_LOCKS PL, PG_STAT_ALL_TABLES PSAT WHERE PL.RELATION = PSAT.RELID 2. Lock Kill 방법 SELECT PG_CANCEL_BACKEND([PID]) SELECT PG_TERMINATE_BACKEND([PID]) Lock 리스트에서 조회된 PID를 넣고 cancel, 혹은 terminate 시켜주면 된다. cancel은 해당 프로세스만을, terminate는 상위 프로세스들까지 종료시킨다.\n3. Lock 이란? (Postgresql) Postgresql은 다양한 종류의 lock 기능을 제공한다. 애플리케이션 단에서 제어도 가능하지만, 대부분 기본적인 SQL 실행 시 적절한 락을 자동실행시켜 관련 테이블의 무결성 유지한다.\n3-1. 테이블 단위 Lock 다음 락 들은 모두 테이블 단위의 락이며, 명칭과 상관없이 테이블 단위로 적용된다. 서로 다른 락이 충돌했을때의 상관관계에 의해 대기 상태로 돌입한다. (테이블 단위 락은 유형에 따라 서로 충돌여부가 다름) 한 테이블에는 2개의 트랜잭션이 동시에 락 적용 될 수 없다. (서로 충돌되지 않는 락은 여러 트랜잭션에 동시에 적용될 수 있다.) 특정 락은 self-conflicting 될 수 있다. (ex. access exclusive 락은 중첩불가 access share 락은 여러 트랜잭션에서 다중으로 적용될 수 있다.) 3-2. Row 단위 Lock 데이터 검색에는 영향을 주지 않는다. 해당 열의 writers and lockers에만 영향을 준다. Row Lock은 해당 트랜잭션의 종료 시에 풀리거나, save point rollback 시점에 풀린다. (테이블 락과 동일) 주로 for update 구문을 사용하여 select 동안 데이터의 무결성을 보장하기 위해 사용한다. 3-3. Page 단위 Lock 공유된 buffer pool 내의 페이지 (데이터 블록) 단위의 read/write를 조절하기 위해 페이지 내부에서 Lock을 수행한다. 3-4. Dead Lock 서로 다른 트랜잭션이 각각 서로 락을 대기하는 상태이다. 예를 들어, 트랜잭션 1이 테이블 A에 배타적 락을 획득하고, 테이블 B에도 배타적 락을 획득하려고 할 때, 동시에 트랜잭션 2가 이미 테이블 B에 배타적 락을 획득한 채로 테이블 A에 대한 배타적 락을 원한다면 양쪽 모두 진행할 수 없게 된다. 데드 락이 감지되면, 테이블 레벨 또는 레코드 레벨 락을 찾을 때까지 트랜잭션은 무기한으로 기다리게 된다. Postgresql에서는 데드락 상황을 자동으로 감지하고, 그중 한 개의 트랜잭션을 중단시켜 나머지를 완료되게 하지만, 어떤 트랜잭션이 중단될지 예측이 어렵고, 데드락을 즉시 발견하지 못하는 경우가 있기에 이에 의존하면 안 된다. 가장 좋은 해결책은, 일관된 순서로 객체에 대한 Lock을 획득하고, 트랜잭션에서 객체에 대한 첫 번째 Lock은 해당 객체에 대해 필요한 최소한의 lock 형태를 보장하는 것이다.\n3-5. Advisory lock 어플리케이션에서 정의된 락 유형이다. 시스템에서 정의되지 않은 락이기에, 애플리케이션에서 올바르게 사용해야 한다. Advisory lock은 일반 락들과 같은 공유 메모리 풀에 저장되며 max_locks_per_transaction와 max_connections파라미터 풀에 의해 사이즈가 정의된다. 3-5-1. Session level advisory lock Session level advisory lock은 세션이 끝나거나 명확한 해제가 있을 때까지 유지된다. 보통의 락 요청과는 다르게, 세션레벨의 advisory lock은 트랜잭션에 종속되지 않는다. 트랜잭션 중간에 획득된 락은 롤백 후에도 유지된다. 그리고 이어지는 트랜잭션이 실패가 되더라도, 락은 유지된다. 락은 한 프로세스 내에 여러 번 획득 가능하며, 완료된 각각의 락요청 이후에는 그에 대응하는 락 해제 요청이 뒤따라야 한다. 3-5-2. transaction level advisory lock 트랜잭션 레벨 락 요청은 반면에, 기존의 락요청과 비슷하게 적용된다. 트랜잭션의 종료시점에 자동으로 해제되며 실직적인 락 해제 작업이 없다. 3-6. Advisory Lock 조회 SELECT pg_advisory_lock(id) FROM foo WHERE id = 12345; -- ok SELECT pg_advisory_lock(id) FROM foo WHERE id \u0026gt; 12345 LIMIT 100; -- danger! SELECT pg_advisory_lock(q.id) FROM ( SELECT id FROM foo WHERE id \u0026gt; 12345 LIMIT 100 ) q; -- ok 다음 구문을 통해 Advisory Lock을 조회가능하다. 다만 2번째 쿼리처럼 Limit을 사용하게 되면 locking 함수가 실행되기 이전에 limit이 적용되는지를 보장할 수 없기에, 예상치 못한 lock을 발생시킬 수 있으며, 세션이 끝나기 전에 해제되지 않을 수 있으므로 주의해야 한다.\n","permalink":"http://localhost:50666/posts/4/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/4/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"1-lock-확인방법\"\u003e1. Lock 확인방법\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eSELECT PSAT.RELNAME,\n       PL.LOCKTYPE,\n       PL.PID,\n       PL.MODE,\n       PL.GRANTED\nFROM PG_LOCKS PL,\n     PG_STAT_ALL_TABLES PSAT\nWHERE PL.RELATION = PSAT.RELID\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"2-lock-kill-방법\"\u003e2. Lock Kill 방법\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eSELECT PG_CANCEL_BACKEND([PID])\n\nSELECT PG_TERMINATE_BACKEND([PID])\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eLock 리스트에서 조회된 PID를 넣고 cancel, 혹은 terminate 시켜주면 된다. cancel은 해당 프로세스만을, terminate는 상위 프로세스들까지 종료시킨다.\u003c/p\u003e\n\u003ch2 id=\"3-lock-이란-postgresql\"\u003e3. Lock 이란? (Postgresql)\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePostgresql은 다양한 종류의 lock 기능을 제공한다. 애플리케이션 단에서 제어도 가능하지만, 대부분 기본적인 SQL 실행 시 적절한 락을 자동실행시켜 관련 테이블의 무결성 유지한다.\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"테이블-단위-lock\" ke-size=\"size23\"\u003e\u003cstrong\u003e3-1.  테이블 단위 Lock\u003c/strong\u003e\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cul\u003e\n\u003cli\u003e다음 락 들은 모두 테이블 단위의 락이며, 명칭과 상관없이 테이블 단위로 적용된다.\u003c/li\u003e\n\u003cli\u003e서로 다른 락이 충돌했을때의 상관관계에 의해 대기 상태로 돌입한다. (테이블 단위 락은 유형에 따라 서로 충돌여부가 다름)\u003c/li\u003e\n\u003cli\u003e한 테이블에는 2개의 트랜잭션이 동시에 락 적용 될 수 없다. (서로 충돌되지 않는 락은 여러 트랜잭션에 동시에 적용될 수 있다.)\u003c/li\u003e\n\u003cli\u003e특정 락은 self-conflicting 될 수 있다.\n(ex. access exclusive 락은 중첩불가 access share 락은 여러 트랜잭션에서 다중으로 적용될 수 있다.)\u003c/li\u003e\n\u003c/ul\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/4/img_1.png\"\u003e\u003c/p\u003e","title":"Postgresql Lock이란? (조회 및 kill, Dead lock)"},{"content":" * 가장 보편적으로 쓰이는 간단한 history 저장 트리거 생성 예제 특정 테이블에 insert, update가 수행될 경우 무조건 내역에 “insert”를 하는 간단한 트리거 생성 예제이다.\n1-1. 함수를 실행할 트리거 생성 create trigger trigger_save_history after insert or update on A for each row execute procedure trigger_insert(); 1-2. 실제 insert문이 실행되는 함수 CREATE OR REPLACE FUNCTION trigger_insert() returns trigger AS $$ DECLARE BEGIN insert into B (id, values, date) values (new.id, new.values, current_timestamp()); return NULL; END; $$ LANGUAGE \u0026#39;plpgsql\u0026#39;; 하지만 특정 table에 insert, delete, update에 따라 서로 다른 테이블에 이력을 보관하거나, 기존 이력을 업데이트하는 등\n서로 다른 행위에 대한 트리거가 필요한 경우가 있다.\n그럴경우 다음 예제처럼 TG_OP을 통해 데이터베이스에 UPDATE, INSERT, DELETE를 분기하는 것이 가능하다. 또한 old, new를 통해 delete의 삭제 전 값, update의 업데이트 전, 후 값을 각각 사용할 수 있다.\n* 수행되는 SQL에 따라 별도 history 저장 트리거 생성 예제 CREATE OR REPLACE FUNCTION TRIGGER_INSERT() RETURNS TRIGGER LANGUAGE PLPGSQL AS $function$ BEGIN IF (TG_OP = \u0026#39;UPDATE\u0026#39;) THEN insert into update_history (id, values, date) values (new.id, new.values, current_timestamp()); return NULL; ELSIF (TG_OP = \u0026#39;INSERT\u0026#39;) THEN insert into insert_history (id, values, date) values (new.id, new.values, current_timestamp()); return NULL; ELSIF (TG_OP = \u0026#39;DELETE\u0026#39;) THEN insert into delete_history (id, values, date) values (new.id, old.values, current_timestamp()); return NULL; END IF; RETURN NULL; END $function$ ; 1. 트리거(Trigger) 란 무엇일까? 특정 SQL이 실행될 때 자동으로 실행되는 객체이다. 테이블의 변경 감지 및 로깅에 많이 사용되며, 데이터만 전달 후 연산 자체를 DB에 넘기기에 부하 및 확장성을 고려하여 적용하여야한다. 단일 함수를 생각할 경우 서버 로직보다 트리거 함수가 빠르고 쉽게 적용되는 경우가 있지만, 트리거가 너무 많은 경우 문제발생 원인파악과 유지보수가 힘들다. 또한 코드가 복잡하여 작성자 외 트리거 내용을 분석하기 힘들다. 또한, 트리거 함수가 동작할 때, 트리거의 영향을 받는 모든 개체들은 트랜잭션이 열린 상태로 유지된다. 즉, 트리거 연산 시간만큼 트랜잭션 lock 타임도 길어진다. 부적합한 트리거는 성능을 크게 저하시킬 수 있다. 그래서 복잡한 로직을 처리하기보다는, 간단한 실행에 사용하고, 트리거 작성 시 정확한 목적과 동작방식을 문서화하는 것이 중요하다.\n- 특정 SQL이 실행될 때 자동으로 실행되는 객체이다\n- 트리거 내에서 COMMIT / ROLLBACK 사용불가하다.\n- 쿼리문장별, ROW별로 실행가능하다.\n- OLD / NEW 를 통해 실행된 쿼리의 이전, 이후값 접근 가능하다.\n- 특정 PROCEDURE / FUNCTION을 실행시킬 수 있다.\n2. 프로시저(Procedure)와 함수(Function)는 무엇일까? 자주 사용되는 특정기능을 모듈화 시켜놓은 것을 함수(function) 또는 프로시저(procedure)라고 하는 것을 알고 있다. PostgreSQL에서 정확히 어떻게 사용되며 차이점은 무엇일까?\n2-1. FUNCTION - 주로 클라이언트에서 실행 (어플리케이션에서 호출)\n- 리턴값 필수\n- 저장해서 쓰는 프로시져 ( 인자만 변경하여 자유롭게 재사용 가능 )- 반복작업을 줄여주며 여러개의 쿼리문을 묶어서 실행 가능\n- 특정 계산을 수행\n2-2. PROCEDURE - 리턴값은 필요에 따라 반환\n- DB서버에서 실행(처리속도 빠름)- 미리 컴파일된 SQL 명령어의 집합\n- 특정 작업을 수행\n2-3. 차이점 비교 함수(Function) 프로시저(Procedure) 특정 계산 수행 특정 작업 수행 리턴값 필수 O 리턴값 필수 X 리턴값이 1개여야만 함 리턴값이 여러개일 수 있음 Client에서 실행 (어플리케이션에서 호출) Server에서 실행(DB) 단독 문장 구성 불가 단독 문장 구성 가능 수식내에서만 사용 가능 수식 내에서 사용 불가 ","permalink":"http://localhost:50666/posts/3/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/3/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"-가장보편적으로쓰이는간단한history-저장트리거생성예제\"\u003e* 가장 보편적으로 쓰이는 간단한 history 저장 트리거 생성 예제\u003c/h2\u003e\n\u003cp\u003e특정 테이블에 insert, update가 수행될 경우 무조건 내역에 “insert”를 하는 간단한 트리거 생성 예제이다.\u003c/p\u003e\n\u003ch4 id=\"1-1-함수를-실행할-트리거-생성\"\u003e1-1. 함수를 실행할 트리거 생성\u003c/h4\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ecreate trigger trigger_save_history\nafter insert or update on A\nfor each row\nexecute procedure trigger_insert();\n\u003c/code\u003e\u003c/pre\u003e\u003ch4 id=\"1-2-실제-insert문이-실행되는-함수\"\u003e1-2. 실제 insert문이 실행되는 함수 \u003c/h4\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE OR REPLACE FUNCTION trigger_insert()\nreturns trigger\nAS $$\nDECLARE\nBEGIN\n    insert into B\n        (id, values, date)\n    values\n        (new.id, new.values, current_timestamp());\n    return NULL;\nEND; $$\nLANGUAGE \u0026#39;plpgsql\u0026#39;;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e하지만 특정 table에 insert, delete, update에 따라 서로 다른 테이블에 이력을 보관하거나, 기존 이력을 업데이트하는 등\u003c/p\u003e","title":"[PostgreSQL] Trigger, Procedure, Function (history 관리하기)"},{"content":" 1. 문제상황 긴 텍스트에서 단순 like 조합 외 방법으로 유사 문자열 검색\n(Ex. Susan loves hiking 을 \u0026ldquo;love hike\u0026rdquo; 이라는 키워드로 검색하고자 함) RDBMS에서 수천만 건의 데이터 처리 시 긴 문자열 검색 속도 향상 2. Full Text Search(전문검색)란? 게시물의 내용/제목 등 문장, 문서 전체에서 키워드를 검색하는 기능이다. 단순한 like, 비교연산자와 달리 각 단어의 Token화 및 정규화를 통해 긴 문장내에서의 유사 검색을 가능하게 한다. Postgresql 기본 인덱스인 b-tree인덱스로는 Like 와 같은 패턴 매칭 검색시 양쪽에 %%를 거는 경우는 인덱스를 타지 않지만, gin 인덱스를 사용하여 빠른 검색이 가능하다.\n2-1. Gin Index GIN stands for \u0026quot;Generalized Inverted index\u0026quot;. \u0026quot;Inverted\u0026quot; refers to the way that the index structure is set up, building a table-encompassing tree of all column values, where a single row can be represented in many places within the tree. By comparison, a B-tree index generally has one location where an index entry points to a specific row.\n인덱스를 적용하는 column의 값을 일정한 규칙에 따라 split 후 사용한다. 데이터 내의 각 token들을 indexing하여 등장 위치와 상관없이 해당 찾고자 하는 값이 데이터의 중간에 등장하더라도 인덱스가 가능하다.\n3. 원리 document를 형태소 단위로 해체하여 각 토큰들을 숫자, 단어, 이메일 등으로 분류한다. 분류된 토큰들을 정규화(대문자를 소문자로 바꾸거나, 복수형들을 단수형으로 바꾸는 등) 를 통해 lexem 형태로 분류한다. 이 과정을 통해 전처리 된 document를 lexem 배열 형태로 저장 후 검색에 사용한다. Postgresql 의 경우 to_tsvector 함수를 통해 document 를 tsvector 타입으로 변환이 가능하다. 변환 후에는 tsquery를 사용하여 매칭 여부를 확인가능하다\nSELECT \u0026#39;a fat cat sat on a mat and ate a fat rat\u0026#39;::tsvector @@ \u0026#39;cat \u0026amp; rat\u0026#39;::tsquery; 3-1. 참고 (한국어 처리) 각 lexem이 정규화 되어있기에 문장 내에 과거형, 복수형 등 유사언어에 대한 검색이 가능하지만 기본적으로 영단어만 제공되며, 한글의 경우 hunspell과 같은 한글 언어팩을 별도로 설치해야 사용 가능하다.\nRDS 현재 설치된 extension 확인\nSHOW rds.extensions 해당 언어팩을 사용하려면 plpythonu3u 와 같은 extension을 설치해야하나, AWS RDS의 경우 보안상의 이유로 특정 extension들의 별도 설치를 제한하고 있어 한글팩 설치가 불가하다. 다만 한글 버전의 유사도 검색이 꼭 필요하다면 AWS Lamda 함수를 통해 우회하여 한글 단어팩을 사용하는 방법은 가능하다.\n4.데이터 타입 4-1. Tsvector Document를 token으로 분리한 후 정형화(복수형, 과거형,대소문자 등의 데이터를 정리 후 ) 중복제거된 lexemes list 유형\nSELECT \u0026#39;a:1A fat:2B,3C cat:4D\u0026#39;::tsvector; **tsvector ----------------------------** \u0026#39;a\u0026#39;:1A \u0026#39;cat\u0026#39;:4 \u0026#39;fat\u0026#39;:2B,3C 4-2. Tsquery lexemes 내에 조건에 맞는 결과가 있는지 여부를 나타내는 유형\nSELECT \u0026#39;fat \u0026amp; rat \u0026amp; ! cat\u0026#39;::tsquery; **tsquery ------------------------** \u0026#39;fat\u0026#39; \u0026amp; \u0026#39;rat\u0026#39; \u0026amp; !\u0026#39;cat\u0026#39; 4-3. 테이블에 적용 - Tsvector 컬럼 추가 ALTER TABLE table ADD COLUMN tsvec_words tsvector - Tsvector 데이터 추가 UPDATE table SET tsvec_words = to_tsvector(column); - Tsvector 인덱스 추가 CREATE INDEX example_idx ON table USING gin(tsvec_words); 5. 연산자 문법의 경우 https://www.postgresql.org/docs/current/textsearch-controls.html공식문서를 참고하는 것이 가장 정확하지만, 직접 적용해본 후 가장 기본적이고 유용한 문법을 정리해보자면\n1. \u0026amp; : and 2. | : or 3. ! : not 4. :* : like (sql의 %%와 동일) 5. \u0026lt;-\u0026gt; : 문자 사이의 간격 확인 예를들어 cat과 dog의 lexem을 모두 포함하고 있는 document를 검색하고 싶다면,\nSELECT * FROM Table WHERE tsvec_words @@ to_tsquery(‘cat:*\u0026amp;dog:*’) 6. 구문 분석 쿼리 PostgreSQL은 쿼리를 tsquery 데이터 형식으로 변환하기 위해 to_tsquery , plainto_tsquery , phraseto_tsquery 및 websearch_to_tsquery 함수를 제공한다.\n6-1. plainto_tsquery plainto_tsquery는 and, a, or 등의 단어를 제외한 형식화되지 않은 텍스트만을 tsquery로 변환 후 단어 사이에 \u0026amp; 연산자를 포함하여 리턴한다. (연산자는 인식하지 않음)\nSELECT plainto_tsquery(\u0026#39;english\u0026#39;, \u0026#39;the Fat Rats eat ttt a and young bean\u0026#39;); \u0026#39;fat\u0026#39; \u0026amp; \u0026#39;rat\u0026#39; \u0026amp; \u0026#39;eat\u0026#39; \u0026amp; \u0026#39;ttt\u0026#39; \u0026amp; \u0026#39;young\u0026#39; \u0026amp; \u0026#39;bean\u0026#39; 6-2. phraseto_tsquery phraseto_tsquery는 plainto_tsquery와 유사하지만, \u0026amp; 대신 \u0026lt;-\u0026gt;(followed by) 연산자로 구분된다. 사이에 삭제되는단어가 있다면 으로 표현되기에 lexeme의 시퀀스를 확인할때 유용하다.\n(연산자는 인식하지 않음)\nSELECT phraseto_tsquery(\u0026#39;english\u0026#39;, \u0026#39;the Fat Rats eat ttt a and young a bean\u0026#39;); \u0026#39;fat\u0026#39; \u0026lt;-\u0026gt; \u0026#39;rat\u0026#39; \u0026lt;-\u0026gt; \u0026#39;eat\u0026#39; \u0026lt;-\u0026gt; \u0026#39;ttt\u0026#39; \u0026lt;-\u0026gt; \u0026#39;young\u0026#39; \u0026lt;2\u0026gt; \u0026#39;bean\u0026#39; 6-3. websearch_to_tsquery websearch_to_tsquery는 plainto_tsquery와 동일 (연산자 인식)\nSELECT websearch_to_tsquery(\u0026#39;\u0026#34;\u0026#34;\u0026#34; )( dummy \\ query \u0026lt;-\u0026gt;\u0026#39;); websearch_to_tsquery ---------------------- \u0026#39;dummy\u0026#39; \u0026lt;-\u0026gt; \u0026#39;query\u0026#39; 7. 검색결과 순위 보통 유사 검색을 실행할 시 가장 유사도가 높은( 검색결과에 가장 부합하는) 결과를 최우선적으로 노출시켜야한다.\nFull Text Search를 통해 조회할 경우 우선 순위를 검색 결과에 가장 부합한다는 지표를 어떻게 확인할 수 있을까?\nts_rank_cd 함수는 쿼리와 매칭 결과 간의 유사도를 점수화하여 보여준다.\nSELECT title, ts_rank_cd(textsearch, query) AS rank FROM apod, to_tsquery(‘test|white\u0026amp;’blue’) query WHERE query @@ textsearch ORDER BY rank DESC 해당 쿼리를 사용하면 \u0026rsquo;test|white\u0026amp;\u0026lsquo;blue\u0026rsquo; 조건에 부합하면서, 관련도를 추출할수 있어\n관련성 높은 document를 최우선적으로 리스트업 할 수 있다.\n마무리 full text search를 사용시 like문의 결합으로 텍스트를 검색할 시보다 효율 높음\na. full Text Search(gin index) 사용시 b. 기본 like절 사용시 150만건 데이터 기준 실행시간 9초 -\u0026gt; 1초 향상.\n데이터 수량이 더 많을 경우 향상될 것으로 보임\n정교한 유사단어 검색 가능 (시제, 단복수, 줄임말 등이 고려된 유사어에 대한 검색이 가능)\n\u0026ldquo;키워드%\u0026rdquo;, 짧은 단어 내 검색, 숫자형태 검색 등 b-tree 인덱스의 효율을 살릴 수 있는 검색의 경우 충분한 검토 후 도입이 필요\n참고\n이미지 출처: https://pganalyze.com/blog/gin-index ","permalink":"http://localhost:50666/posts/2/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/posts/2/img.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"문제상황\" ke-size=\"size26\"\u003e1. 문제상황\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e긴 텍스트에서 단순 like 조합 외 방법으로 유사 문자열 검색\u003cbr\u003e\n(Ex. Susan loves hiking 을 \u0026ldquo;love hike\u0026rdquo; 이라는 키워드로 검색하고자 함)\u003c/li\u003e\n\u003cli\u003eRDBMS에서 수천만 건의 데이터 처리 시 긴 문자열 검색 속도 향상\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"full-text-search전문검색란\" ke-size=\"size26\"\u003e2. Full Text Search(전문검색)란?\u003c/h2\u003e\n\u003cp\u003e게시물의 내용/제목 등 문장, 문서 전체에서 키워드를 검색하는 기능이다. 단순한 like, 비교연산자와 달리 각 단어의 Token화 및 정규화를 통해 긴 문장내에서의 유사 검색을 가능하게 한다. Postgresql 기본 인덱스인 b-tree인덱스로는 Like 와 같은 패턴 매칭 검색시 양쪽에 %%를 거는 경우는 인덱스를 타지 않지만, gin 인덱스를 사용하여 빠른 검색이 가능하다.\u003c/p\u003e","title":"Full Text Search를 활용한 데이터베이스 성능 향상"}]